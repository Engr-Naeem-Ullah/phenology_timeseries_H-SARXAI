{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c84e282",
   "metadata": {},
   "source": [
    "CNN with a Gated Attention mechanism (Conv-GAtt) Code for Horizon 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daef08c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Optimizing Hyperparameters...\n",
      "|   iter    |  target   | batch_... | dropou... |  epochs   | num_units |\n",
      "-------------------------------------------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m33.98    \u001b[39m | \u001b[39m0.4803   \u001b[39m | \u001b[39m86.6     \u001b[39m | \u001b[39m89.47    \u001b[39m |\n",
      "| \u001b[39m2        \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m23.49    \u001b[39m | \u001b[39m0.1624   \u001b[39m | \u001b[39m52.9     \u001b[39m | \u001b[39m115.2    \u001b[39m |\n",
      "| \u001b[39m3        \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m44.85    \u001b[39m | \u001b[39m0.3832   \u001b[39m | \u001b[39m51.03    \u001b[39m | \u001b[39m125.1    \u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m55.96    \u001b[39m | \u001b[39m0.1849   \u001b[39m | \u001b[39m59.09    \u001b[39m | \u001b[39m49.61    \u001b[39m |\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m30.6     \u001b[39m | \u001b[39m0.3099   \u001b[39m | \u001b[39m71.6     \u001b[39m | \u001b[39m59.96    \u001b[39m |\n",
      "| \u001b[39m6        \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m58.87    \u001b[39m | \u001b[39m0.237    \u001b[39m | \u001b[39m99.91    \u001b[39m | \u001b[39m32.34    \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m63.83    \u001b[39m | \u001b[39m0.1911   \u001b[39m | \u001b[39m99.11    \u001b[39m | \u001b[39m127.8    \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m0.9919   \u001b[39m | \u001b[39m17.24    \u001b[39m | \u001b[39m0.4906   \u001b[39m | \u001b[39m50.56    \u001b[39m | \u001b[39m33.63    \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m21.82    \u001b[39m | \u001b[39m0.3011   \u001b[39m | \u001b[39m51.99    \u001b[39m | \u001b[39m115.7    \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m63.61    \u001b[39m | \u001b[39m0.2064   \u001b[39m | \u001b[39m87.45    \u001b[39m | \u001b[39m68.96    \u001b[39m |\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m62.61    \u001b[39m | \u001b[39m0.4329   \u001b[39m | \u001b[39m50.58    \u001b[39m | \u001b[39m87.37    \u001b[39m |\n",
      "| \u001b[39m12       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m16.46    \u001b[39m | \u001b[39m0.2994   \u001b[39m | \u001b[39m99.72    \u001b[39m | \u001b[39m124.8    \u001b[39m |\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m19.34    \u001b[39m | \u001b[39m0.3874   \u001b[39m | \u001b[39m99.84    \u001b[39m | \u001b[39m62.01    \u001b[39m |\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m63.48    \u001b[39m | \u001b[39m0.2778   \u001b[39m | \u001b[39m73.74    \u001b[39m | \u001b[39m111.0    \u001b[39m |\n",
      "| \u001b[39m15       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m16.15    \u001b[39m | \u001b[39m0.3729   \u001b[39m | \u001b[39m59.28    \u001b[39m | \u001b[39m86.44    \u001b[39m |\n",
      "| \u001b[39m16       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m16.56    \u001b[39m | \u001b[39m0.2081   \u001b[39m | \u001b[39m73.99    \u001b[39m | \u001b[39m127.9    \u001b[39m |\n",
      "| \u001b[39m17       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m41.49    \u001b[39m | \u001b[39m0.1105   \u001b[39m | \u001b[39m99.73    \u001b[39m | \u001b[39m55.84    \u001b[39m |\n",
      "| \u001b[39m18       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m45.44    \u001b[39m | \u001b[39m0.3414   \u001b[39m | \u001b[39m65.32    \u001b[39m | \u001b[39m74.43    \u001b[39m |\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m0.9919   \u001b[39m | \u001b[39m63.83    \u001b[39m | \u001b[39m0.4958   \u001b[39m | \u001b[39m74.15    \u001b[39m | \u001b[39m32.55    \u001b[39m |\n",
      "| \u001b[39m20       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m39.68    \u001b[39m | \u001b[39m0.3511   \u001b[39m | \u001b[39m84.51    \u001b[39m | \u001b[39m119.8    \u001b[39m |\n",
      "=========================================================================\n",
      "\n",
      " Best Params: {'batch_size': 33.977925704673396, 'dropout_rate': 0.4802857225639665, 'epochs': 86.59969709057026, 'num_units': 89.47121448291551}\n",
      "\n",
      " Training Final Gate Attention CNN Model...\n",
      "Epoch 1/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.4388 - loss: 1.2416 - val_accuracy: 0.6463 - val_loss: 0.9060\n",
      "Epoch 2/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6090 - loss: 0.9525 - val_accuracy: 0.8496 - val_loss: 0.6064\n",
      "Epoch 3/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7394 - loss: 0.7075 - val_accuracy: 0.9024 - val_loss: 0.4076\n",
      "Epoch 4/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7949 - loss: 0.5472 - val_accuracy: 0.9065 - val_loss: 0.3246\n",
      "Epoch 5/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8415 - loss: 0.4308 - val_accuracy: 0.9268 - val_loss: 0.2617\n",
      "Epoch 6/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8717 - loss: 0.3515 - val_accuracy: 0.9228 - val_loss: 0.2241\n",
      "Epoch 7/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8895 - loss: 0.3226 - val_accuracy: 0.9309 - val_loss: 0.1965\n",
      "Epoch 8/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8882 - loss: 0.2856 - val_accuracy: 0.9309 - val_loss: 0.1794\n",
      "Epoch 9/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8931 - loss: 0.2620 - val_accuracy: 0.9350 - val_loss: 0.1623\n",
      "Epoch 10/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8956 - loss: 0.2469 - val_accuracy: 0.9390 - val_loss: 0.1506\n",
      "Epoch 11/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9148 - loss: 0.2235 - val_accuracy: 0.9512 - val_loss: 0.1335\n",
      "Epoch 12/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9197 - loss: 0.2091 - val_accuracy: 0.9431 - val_loss: 0.1229\n",
      "Epoch 13/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9269 - loss: 0.1955 - val_accuracy: 0.9553 - val_loss: 0.1151\n",
      "Epoch 14/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9347 - loss: 0.1750 - val_accuracy: 0.9593 - val_loss: 0.0984\n",
      "Epoch 15/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9339 - loss: 0.1771 - val_accuracy: 0.9593 - val_loss: 0.0962\n",
      "Epoch 16/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9400 - loss: 0.1527 - val_accuracy: 0.9634 - val_loss: 0.0819\n",
      "Epoch 17/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9404 - loss: 0.1381 - val_accuracy: 0.9756 - val_loss: 0.0766\n",
      "Epoch 18/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9413 - loss: 0.1478 - val_accuracy: 0.9797 - val_loss: 0.0699\n",
      "Epoch 19/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9418 - loss: 0.1484 - val_accuracy: 0.9797 - val_loss: 0.0674\n",
      "Epoch 20/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9491 - loss: 0.1455 - val_accuracy: 0.9919 - val_loss: 0.0638\n",
      "Epoch 21/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9501 - loss: 0.1153 - val_accuracy: 0.9878 - val_loss: 0.0617\n",
      "Epoch 22/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9631 - loss: 0.1222 - val_accuracy: 0.9797 - val_loss: 0.0627\n",
      "Epoch 23/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9736 - loss: 0.1054 - val_accuracy: 0.9675 - val_loss: 0.0633\n",
      "Epoch 24/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9547 - loss: 0.1209 - val_accuracy: 0.9919 - val_loss: 0.0495\n",
      "Epoch 25/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9686 - loss: 0.1117 - val_accuracy: 0.9959 - val_loss: 0.0462\n",
      "Epoch 26/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9527 - loss: 0.1052 - val_accuracy: 0.9959 - val_loss: 0.0407\n",
      "Epoch 27/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9650 - loss: 0.1045 - val_accuracy: 0.9959 - val_loss: 0.0403\n",
      "Epoch 28/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9646 - loss: 0.0998 - val_accuracy: 0.9756 - val_loss: 0.0451\n",
      "Epoch 29/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9819 - loss: 0.0697 - val_accuracy: 0.9959 - val_loss: 0.0355\n",
      "Epoch 30/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9684 - loss: 0.0838 - val_accuracy: 0.9919 - val_loss: 0.0371\n",
      "Epoch 31/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9569 - loss: 0.1026 - val_accuracy: 1.0000 - val_loss: 0.0356\n",
      "Epoch 32/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9600 - loss: 0.0983 - val_accuracy: 0.9837 - val_loss: 0.0405\n",
      "Epoch 33/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9586 - loss: 0.0902 - val_accuracy: 0.9959 - val_loss: 0.0348\n",
      "Epoch 34/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9764 - loss: 0.0660 - val_accuracy: 0.9959 - val_loss: 0.0280\n",
      "Epoch 35/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9854 - loss: 0.0682 - val_accuracy: 1.0000 - val_loss: 0.0262\n",
      "Epoch 36/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9704 - loss: 0.0822 - val_accuracy: 0.9797 - val_loss: 0.0384\n",
      "Epoch 37/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9638 - loss: 0.0960 - val_accuracy: 1.0000 - val_loss: 0.0257\n",
      "Epoch 38/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9809 - loss: 0.0618 - val_accuracy: 1.0000 - val_loss: 0.0251\n",
      "Epoch 39/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9679 - loss: 0.0738 - val_accuracy: 1.0000 - val_loss: 0.0223\n",
      "Epoch 40/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9772 - loss: 0.0791 - val_accuracy: 1.0000 - val_loss: 0.0224\n",
      "Epoch 41/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9806 - loss: 0.0597 - val_accuracy: 0.9959 - val_loss: 0.0183\n",
      "Epoch 42/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9795 - loss: 0.0712 - val_accuracy: 0.9959 - val_loss: 0.0204\n",
      "Epoch 43/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9772 - loss: 0.0612 - val_accuracy: 0.9959 - val_loss: 0.0222\n",
      "Epoch 44/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9788 - loss: 0.0576 - val_accuracy: 0.9959 - val_loss: 0.0172\n",
      "Epoch 45/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9747 - loss: 0.0719 - val_accuracy: 0.9959 - val_loss: 0.0198\n",
      "Epoch 46/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9727 - loss: 0.0647 - val_accuracy: 1.0000 - val_loss: 0.0180\n",
      "Epoch 47/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9802 - loss: 0.0483 - val_accuracy: 0.9878 - val_loss: 0.0234\n",
      "Epoch 48/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9842 - loss: 0.0469 - val_accuracy: 0.9959 - val_loss: 0.0158\n",
      "Epoch 49/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9824 - loss: 0.0465 - val_accuracy: 1.0000 - val_loss: 0.0143\n",
      "Epoch 50/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9835 - loss: 0.0537 - val_accuracy: 1.0000 - val_loss: 0.0120\n",
      "Epoch 51/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9830 - loss: 0.0473 - val_accuracy: 1.0000 - val_loss: 0.0114\n",
      "Epoch 52/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9812 - loss: 0.0500 - val_accuracy: 0.9959 - val_loss: 0.0132\n",
      "Epoch 53/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9794 - loss: 0.0510 - val_accuracy: 0.9959 - val_loss: 0.0129\n",
      "Epoch 54/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9840 - loss: 0.0536 - val_accuracy: 1.0000 - val_loss: 0.0093\n",
      "Epoch 55/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9810 - loss: 0.0418 - val_accuracy: 1.0000 - val_loss: 0.0098\n",
      "Epoch 56/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9826 - loss: 0.0449 - val_accuracy: 1.0000 - val_loss: 0.0088\n",
      "Epoch 57/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9816 - loss: 0.0498 - val_accuracy: 1.0000 - val_loss: 0.0083\n",
      "Epoch 58/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9799 - loss: 0.0363 - val_accuracy: 0.9959 - val_loss: 0.0114\n",
      "Epoch 59/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9819 - loss: 0.0508 - val_accuracy: 1.0000 - val_loss: 0.0075\n",
      "Epoch 60/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9898 - loss: 0.0326 - val_accuracy: 0.9959 - val_loss: 0.0091\n",
      "Epoch 61/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9874 - loss: 0.0360 - val_accuracy: 0.9959 - val_loss: 0.0093\n",
      "Epoch 62/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9857 - loss: 0.0399 - val_accuracy: 0.9959 - val_loss: 0.0109\n",
      "Epoch 63/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9898 - loss: 0.0455 - val_accuracy: 0.9959 - val_loss: 0.0086\n",
      "Epoch 64/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9921 - loss: 0.0325 - val_accuracy: 0.9959 - val_loss: 0.0079\n",
      "Epoch 65/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9876 - loss: 0.0367 - val_accuracy: 0.9959 - val_loss: 0.0100\n",
      "Epoch 66/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9877 - loss: 0.0370 - val_accuracy: 0.9959 - val_loss: 0.0097\n",
      "Epoch 67/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9891 - loss: 0.0357 - val_accuracy: 1.0000 - val_loss: 0.0064\n",
      "Epoch 68/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9832 - loss: 0.0330 - val_accuracy: 0.9959 - val_loss: 0.0098\n",
      "Epoch 69/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9879 - loss: 0.0307 - val_accuracy: 1.0000 - val_loss: 0.0053\n",
      "Epoch 70/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9879 - loss: 0.0327 - val_accuracy: 1.0000 - val_loss: 0.0053\n",
      "Epoch 71/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9910 - loss: 0.0260 - val_accuracy: 1.0000 - val_loss: 0.0050\n",
      "Epoch 72/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9895 - loss: 0.0349 - val_accuracy: 1.0000 - val_loss: 0.0041\n",
      "Epoch 73/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9928 - loss: 0.0245 - val_accuracy: 1.0000 - val_loss: 0.0045\n",
      "Epoch 74/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9873 - loss: 0.0292 - val_accuracy: 1.0000 - val_loss: 0.0046\n",
      "Epoch 75/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9953 - loss: 0.0199 - val_accuracy: 0.9959 - val_loss: 0.0070\n",
      "Epoch 76/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9928 - loss: 0.0235 - val_accuracy: 1.0000 - val_loss: 0.0034\n",
      "Epoch 77/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9858 - loss: 0.0341 - val_accuracy: 0.9959 - val_loss: 0.0119\n",
      "Epoch 78/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9870 - loss: 0.0419 - val_accuracy: 0.9959 - val_loss: 0.0061\n",
      "Epoch 79/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9903 - loss: 0.0314 - val_accuracy: 1.0000 - val_loss: 0.0051\n",
      "Epoch 80/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9880 - loss: 0.0270 - val_accuracy: 1.0000 - val_loss: 0.0030\n",
      "Epoch 81/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9991 - loss: 0.0175 - val_accuracy: 1.0000 - val_loss: 0.0046\n",
      "Epoch 82/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9880 - loss: 0.0432 - val_accuracy: 0.9959 - val_loss: 0.0067\n",
      "Epoch 83/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9932 - loss: 0.0256 - val_accuracy: 1.0000 - val_loss: 0.0026\n",
      "Epoch 84/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9868 - loss: 0.0263 - val_accuracy: 1.0000 - val_loss: 0.0038\n",
      "Epoch 85/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9943 - loss: 0.0194 - val_accuracy: 1.0000 - val_loss: 0.0058\n",
      "Epoch 86/86\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9898 - loss: 0.0197 - val_accuracy: 1.0000 - val_loss: 0.0025\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Evaluation =====\n",
      "Accuracy: 0.9606\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.5714    0.7273        56\n",
      "           1     0.9087    0.9958    0.9503       240\n",
      "           2     0.9959    0.9877    0.9918       487\n",
      "           3     0.8286    0.9667    0.8923        30\n",
      "\n",
      "    accuracy                         0.9606       813\n",
      "   macro avg     0.9333    0.8804    0.8904       813\n",
      "weighted avg     0.9643    0.9606    0.9576       813\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 32  24   0   0]\n",
      " [  0 239   1   0]\n",
      " [  0   0 481   6]\n",
      " [  0   0   1  29]]\n",
      "\n",
      "===== Model Metrics =====\n",
      "Training Time: 19.52s\n",
      "Inference/sample: 0.000364s\n",
      "Testing Time: 0.30s\n",
      "Model Size: 0.08 MB\n",
      "Trainable Params: 14,731\n",
      "RAM Usage: 856.61 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import psutil\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from bayes_opt import BayesianOptimization\n",
    "from scipy.fftpack import fft\n",
    "import warnings\n",
    "\n",
    "# Set seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================\n",
    "# 1. Data Loading and Preprocessing\n",
    "# ============================================\n",
    "def load_and_preprocess_data(filepath):\n",
    "    df = pd.read_csv(filepath, delimiter=\",\", low_memory=False)\n",
    "    df.columns = df.columns.str.replace('\"', '').str.strip()\n",
    "\n",
    "    if 'TIME' not in df.columns:\n",
    "        raise KeyError(\"The column 'TIME' does not exist in the dataset.\")\n",
    "\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'], errors='coerce')\n",
    "    df.dropna(subset=['TIME'], inplace=True)\n",
    "    df.set_index('TIME', inplace=True)\n",
    "\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "    target_col = 'fenologia_h1'\n",
    "    if target_col not in df.columns:\n",
    "        raise KeyError(f\"The target column '{target_col}' does not exist.\")\n",
    "\n",
    "    df[target_col].interpolate(method='linear', inplace=True)\n",
    "    df[target_col] = df[target_col] - 1\n",
    "\n",
    "    for i in range(1, 4):\n",
    "        df[f'{target_col}_lag{i}'] = df[target_col].shift(i)\n",
    "\n",
    "    for window in [3, 6]:\n",
    "        df[f'{target_col}_roll_mean_{window}'] = df[target_col].rolling(window).mean()\n",
    "        df[f'{target_col}_roll_std_{window}'] = df[target_col].rolling(window).std()\n",
    "\n",
    "    df['month'] = df.index.month\n",
    "    df['weekofyear'] = df.index.isocalendar().week\n",
    "    df['year'] = df.index.isocalendar().year\n",
    "    df['sin_week'] = np.sin(2 * np.pi * df['weekofyear'] / 52)\n",
    "    df['cos_week'] = np.cos(2 * np.pi * df['weekofyear'] / 52)\n",
    "    df['EMA_3'] = df[target_col].ewm(span=3, adjust=False).mean()\n",
    "    df['EMA_6'] = df[target_col].ewm(span=6, adjust=False).mean()\n",
    "    df['correlation_target_month'] = df[target_col].rolling(window=6).corr(df['month'])\n",
    "    df['correlation_target_week'] = df[target_col].rolling(window=6).corr(df['weekofyear'])\n",
    "\n",
    "    fft_values = fft(df[target_col].dropna().values)\n",
    "    fft_real = np.real(fft_values)[:len(df[target_col])]\n",
    "    fft_imag = np.imag(fft_values)[:len(df[target_col])]\n",
    "    df['fft_real'] = np.concatenate([fft_real, np.nan * np.ones(len(df) - len(fft_real))])\n",
    "    df['fft_imag'] = np.concatenate([fft_imag, np.nan * np.ones(len(df) - len(fft_imag))])\n",
    "\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "    return df, target_col\n",
    "\n",
    "# ============================================\n",
    "# 2. Feature Scaling\n",
    "# ============================================\n",
    "def feature_scaling(df, target_col):\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))  # 3D\n",
    "    return X_scaled, y, scaler\n",
    "\n",
    "# ============================================\n",
    "# 3. CNN + Gate Attention Model\n",
    "# ============================================\n",
    "def gated_attention_block(inputs, units):\n",
    "    conv_output = layers.Conv1D(units, kernel_size=1, activation='relu')(inputs)\n",
    "    attention_scores = layers.Dense(units, activation='sigmoid')(conv_output)\n",
    "    gated_output = layers.Multiply()([conv_output, attention_scores])\n",
    "    return gated_output\n",
    "\n",
    "def build_cnn_gatt_model(input_shape, num_classes=4, num_units=64, dropout_rate=0.2):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = gated_attention_block(inputs, units=num_units)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    x = layers.Dense(num_units // 2, activation='relu')(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# ============================================\n",
    "# 4. Bayesian Optimization Objective\n",
    "# ============================================\n",
    "def objective_function(num_units, dropout_rate, epochs, batch_size, X_train, y_train):\n",
    "    model = build_cnn_gatt_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        num_classes=4,\n",
    "        num_units=int(num_units),\n",
    "        dropout_rate=dropout_rate\n",
    "    )\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=int(epochs),\n",
    "        batch_size=int(batch_size),\n",
    "        validation_split=0.2,\n",
    "        verbose=0\n",
    "    )\n",
    "    return np.max(history.history['val_accuracy'])\n",
    "\n",
    "# ============================================\n",
    "# 5. Optimize Hyperparameters\n",
    "# ============================================\n",
    "def optimize_hyperparameters(X_train, y_train):\n",
    "    pbounds = {\n",
    "        'num_units': (32, 128),\n",
    "        'dropout_rate': (0.1, 0.5),\n",
    "        'epochs': (50, 100),\n",
    "        'batch_size': (16, 64)\n",
    "    }\n",
    "\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=lambda num_units, dropout_rate, epochs, batch_size: objective_function(\n",
    "            num_units, dropout_rate, epochs, batch_size, X_train, y_train\n",
    "        ),\n",
    "        pbounds=pbounds,\n",
    "        random_state=42,\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    optimizer.maximize(init_points=5, n_iter=15)\n",
    "    return optimizer.max['params']\n",
    "\n",
    "# ============================================\n",
    "# 6. Train & Evaluate Model\n",
    "# ============================================\n",
    "def train_and_evaluate_model(X_train, X_test, y_train, y_test, best_params):\n",
    "    model = build_cnn_gatt_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        num_classes=4,\n",
    "        num_units=int(best_params['num_units']),\n",
    "        dropout_rate=float(best_params['dropout_rate'])\n",
    "    )\n",
    "\n",
    "    start_train = time.time()\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=int(best_params['epochs']),\n",
    "        batch_size=int(best_params['batch_size']),\n",
    "        validation_split=0.2,\n",
    "        verbose=1\n",
    "    )\n",
    "    training_time = time.time() - start_train\n",
    "\n",
    "    start_infer = time.time()\n",
    "    y_pred = model.predict(X_test).argmax(axis=1)\n",
    "    inference_time = (time.time() - start_infer) / len(X_test)\n",
    "    testing_time = time.time() - start_infer\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(\"\\n===== Evaluation =====\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(\"Report:\\n\", classification_report(y_test, y_pred, digits=4))\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "    model.save(\"temp_model.h5\", include_optimizer=False)\n",
    "    model_size = os.path.getsize(\"temp_model.h5\") / (1024 ** 2)\n",
    "    os.remove(\"temp_model.h5\")\n",
    "\n",
    "    total_params = np.sum([np.prod(v.shape) for v in model.trainable_weights])\n",
    "    ram_usage = psutil.Process(os.getpid()).memory_info().rss / (1024 ** 2)\n",
    "\n",
    "    print(\"\\n===== Model Metrics =====\")\n",
    "    print(f\"Training Time: {training_time:.2f}s\")\n",
    "    print(f\"Inference/sample: {inference_time:.6f}s\")\n",
    "    print(f\"Testing Time: {testing_time:.2f}s\")\n",
    "    print(f\"Model Size: {model_size:.2f} MB\")\n",
    "    print(f\"Trainable Params: {total_params:,}\")\n",
    "    print(f\"RAM Usage: {ram_usage:.2f} MB\")\n",
    "\n",
    "    return model, history\n",
    "\n",
    "# ============================================\n",
    "# 7. Main\n",
    "# ============================================\n",
    "def main():\n",
    "    train_path = r\"E:\\Abroad period research\\Phenology datasets\\PHENOLOGY_H1_train.csv\"\n",
    "    test_path = r\"E:\\Abroad period research\\Phenology datasets\\PHENOLOGY_H1_test.csv\"\n",
    "\n",
    "    df_train, target_col = load_and_preprocess_data(train_path)\n",
    "    X_train, y_train, _ = feature_scaling(df_train, target_col)\n",
    "\n",
    "    df_test, _ = load_and_preprocess_data(test_path)\n",
    "    X_test, y_test, _ = feature_scaling(df_test, target_col)\n",
    "\n",
    "    print(\"\\n Optimizing Hyperparameters...\")\n",
    "    best_params = optimize_hyperparameters(X_train, y_train)\n",
    "    print(\"\\n Best Params:\", best_params)\n",
    "\n",
    "    print(\"\\n Training Final Gate Attention CNN Model...\")\n",
    "    model, history = train_and_evaluate_model(X_train, X_test, y_train, y_test, best_params)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b5a07b",
   "metadata": {},
   "source": [
    "CNN with a Gated Attention mechanism (Conv-GAtt) Code for Horizon 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba49f50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Optimizing Hyperparameters...\n",
      "|   iter    |  target   | batch_... | dropou... |  epochs   | num_units |\n",
      "-------------------------------------------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.9959   \u001b[39m | \u001b[39m33.98    \u001b[39m | \u001b[39m0.4803   \u001b[39m | \u001b[39m86.6     \u001b[39m | \u001b[39m89.47    \u001b[39m |\n",
      "| \u001b[35m2        \u001b[39m | \u001b[35m1.0      \u001b[39m | \u001b[35m23.49    \u001b[39m | \u001b[35m0.1624   \u001b[39m | \u001b[35m52.9     \u001b[39m | \u001b[35m115.2    \u001b[39m |\n",
      "| \u001b[39m3        \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m44.85    \u001b[39m | \u001b[39m0.3832   \u001b[39m | \u001b[39m51.03    \u001b[39m | \u001b[39m125.1    \u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m0.9959   \u001b[39m | \u001b[39m55.96    \u001b[39m | \u001b[39m0.1849   \u001b[39m | \u001b[39m59.09    \u001b[39m | \u001b[39m49.61    \u001b[39m |\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m30.6     \u001b[39m | \u001b[39m0.3099   \u001b[39m | \u001b[39m71.6     \u001b[39m | \u001b[39m59.96    \u001b[39m |\n",
      "| \u001b[39m6        \u001b[39m | \u001b[39m0.9959   \u001b[39m | \u001b[39m31.24    \u001b[39m | \u001b[39m0.1594   \u001b[39m | \u001b[39m50.22    \u001b[39m | \u001b[39m125.1    \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m18.36    \u001b[39m | \u001b[39m0.1074   \u001b[39m | \u001b[39m80.03    \u001b[39m | \u001b[39m101.5    \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m16.21    \u001b[39m | \u001b[39m0.1161   \u001b[39m | \u001b[39m73.32    \u001b[39m | \u001b[39m65.8     \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m62.6     \u001b[39m | \u001b[39m0.3838   \u001b[39m | \u001b[39m99.69    \u001b[39m | \u001b[39m67.9     \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m38.63    \u001b[39m | \u001b[39m0.1642   \u001b[39m | \u001b[39m90.1     \u001b[39m | \u001b[39m41.26    \u001b[39m |\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m61.04    \u001b[39m | \u001b[39m0.3825   \u001b[39m | \u001b[39m72.51    \u001b[39m | \u001b[39m127.9    \u001b[39m |\n",
      "| \u001b[39m12       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m30.67    \u001b[39m | \u001b[39m0.1671   \u001b[39m | \u001b[39m72.5     \u001b[39m | \u001b[39m59.03    \u001b[39m |\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m23.86    \u001b[39m | \u001b[39m0.3465   \u001b[39m | \u001b[39m79.9     \u001b[39m | \u001b[39m63.82    \u001b[39m |\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m53.88    \u001b[39m | \u001b[39m0.1904   \u001b[39m | \u001b[39m57.99    \u001b[39m | \u001b[39m124.7    \u001b[39m |\n",
      "| \u001b[39m15       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m17.72    \u001b[39m | \u001b[39m0.4992   \u001b[39m | \u001b[39m60.17    \u001b[39m | \u001b[39m107.9    \u001b[39m |\n",
      "| \u001b[39m16       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m20.26    \u001b[39m | \u001b[39m0.1164   \u001b[39m | \u001b[39m50.14    \u001b[39m | \u001b[39m102.7    \u001b[39m |\n",
      "| \u001b[39m17       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m53.08    \u001b[39m | \u001b[39m0.3014   \u001b[39m | \u001b[39m51.18    \u001b[39m | \u001b[39m111.5    \u001b[39m |\n",
      "| \u001b[39m18       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m17.46    \u001b[39m | \u001b[39m0.1636   \u001b[39m | \u001b[39m75.0     \u001b[39m | \u001b[39m51.92    \u001b[39m |\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m23.69    \u001b[39m | \u001b[39m0.3419   \u001b[39m | \u001b[39m89.34    \u001b[39m | \u001b[39m49.27    \u001b[39m |\n",
      "| \u001b[39m20       \u001b[39m | \u001b[39m0.9959   \u001b[39m | \u001b[39m25.95    \u001b[39m | \u001b[39m0.4192   \u001b[39m | \u001b[39m78.86    \u001b[39m | \u001b[39m37.58    \u001b[39m |\n",
      "=========================================================================\n",
      "\n",
      " Best Params: {'batch_size': 23.488894741236955, 'dropout_rate': 0.16239780813448107, 'epochs': 52.90418060840997, 'num_units': 115.15290999439378}\n",
      "\n",
      " Training Final Gate Attention CNN Model...\n",
      "Epoch 1/52\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.4849 - loss: 1.2566 - val_accuracy: 0.6545 - val_loss: 0.9245\n",
      "Epoch 2/52\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6778 - loss: 0.8891 - val_accuracy: 0.8333 - val_loss: 0.5900\n",
      "Epoch 3/52\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8060 - loss: 0.6069 - val_accuracy: 0.9350 - val_loss: 0.3987\n",
      "Epoch 4/52\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8641 - loss: 0.4520 - val_accuracy: 0.9675 - val_loss: 0.2791\n",
      "Epoch 5/52\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8937 - loss: 0.3615 - val_accuracy: 0.9756 - val_loss: 0.2133\n",
      "Epoch 6/52\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9267 - loss: 0.2860 - val_accuracy: 0.9837 - val_loss: 0.1618\n",
      "Epoch 7/52\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9365 - loss: 0.2443 - val_accuracy: 0.9837 - val_loss: 0.1355\n",
      "Epoch 8/52\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9480 - loss: 0.1988 - val_accuracy: 0.9837 - val_loss: 0.1142\n",
      "Epoch 9/52\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9546 - loss: 0.1754 - val_accuracy: 0.9837 - val_loss: 0.0967\n",
      "Epoch 10/52\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9515 - loss: 0.1431 - val_accuracy: 0.9878 - val_loss: 0.0808\n",
      "Epoch 11/52\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9512 - loss: 0.1344 - val_accuracy: 0.9837 - val_loss: 0.0692\n",
      "Epoch 12/52\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9554 - loss: 0.1217 - val_accuracy: 0.9837 - val_loss: 0.0611\n",
      "Epoch 13/52\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9599 - loss: 0.1042 - val_accuracy: 0.9878 - val_loss: 0.0488\n",
      "Epoch 14/52\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9838 - loss: 0.0777 - val_accuracy: 0.9919 - val_loss: 0.0391\n",
      "Epoch 15/52\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9752 - loss: 0.0728 - val_accuracy: 0.9837 - val_loss: 0.0384\n",
      "Epoch 16/52\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9791 - loss: 0.0731 - val_accuracy: 0.9878 - val_loss: 0.0315\n",
      "Epoch 17/52\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9776 - loss: 0.0724 - val_accuracy: 0.9878 - val_loss: 0.0283\n",
      "Epoch 18/52\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9872 - loss: 0.0498 - val_accuracy: 0.9878 - val_loss: 0.0275\n",
      "Epoch 19/52\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9817 - loss: 0.0557 - val_accuracy: 0.9878 - val_loss: 0.0267\n",
      "Epoch 20/52\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9872 - loss: 0.0485 - val_accuracy: 0.9878 - val_loss: 0.0226\n",
      "Epoch 21/52\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9849 - loss: 0.0471 - val_accuracy: 0.9959 - val_loss: 0.0210\n",
      "Epoch 22/52\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9880 - loss: 0.0418 - val_accuracy: 0.9959 - val_loss: 0.0175\n",
      "Epoch 23/52\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9852 - loss: 0.0413 - val_accuracy: 0.9959 - val_loss: 0.0151\n",
      "Epoch 24/52\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9950 - loss: 0.0319 - val_accuracy: 0.9959 - val_loss: 0.0158\n",
      "Epoch 25/52\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9885 - loss: 0.0344 - val_accuracy: 0.9878 - val_loss: 0.0206\n",
      "Epoch 26/52\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9884 - loss: 0.0319 - val_accuracy: 0.9959 - val_loss: 0.0150\n",
      "Epoch 27/52\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9925 - loss: 0.0285 - val_accuracy: 0.9959 - val_loss: 0.0117\n",
      "Epoch 28/52\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9894 - loss: 0.0268 - val_accuracy: 0.9959 - val_loss: 0.0137\n",
      "Epoch 29/52\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9863 - loss: 0.0368 - val_accuracy: 0.9959 - val_loss: 0.0143\n",
      "Epoch 30/52\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9856 - loss: 0.0286 - val_accuracy: 0.9878 - val_loss: 0.0151\n",
      "Epoch 31/52\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9836 - loss: 0.0330 - val_accuracy: 0.9959 - val_loss: 0.0134\n",
      "Epoch 32/52\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9922 - loss: 0.0310 - val_accuracy: 0.9959 - val_loss: 0.0110\n",
      "Epoch 33/52\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9840 - loss: 0.0342 - val_accuracy: 0.9959 - val_loss: 0.0099\n",
      "Epoch 34/52\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9900 - loss: 0.0293 - val_accuracy: 0.9959 - val_loss: 0.0099\n",
      "Epoch 35/52\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9911 - loss: 0.0306 - val_accuracy: 0.9959 - val_loss: 0.0114\n",
      "Epoch 36/52\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9875 - loss: 0.0262 - val_accuracy: 0.9878 - val_loss: 0.0211\n",
      "Epoch 37/52\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9883 - loss: 0.0355 - val_accuracy: 0.9959 - val_loss: 0.0095\n",
      "Epoch 38/52\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9913 - loss: 0.0233 - val_accuracy: 0.9878 - val_loss: 0.0126\n",
      "Epoch 39/52\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9889 - loss: 0.0248 - val_accuracy: 0.9959 - val_loss: 0.0098\n",
      "Epoch 40/52\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9929 - loss: 0.0168 - val_accuracy: 0.9878 - val_loss: 0.0187\n",
      "Epoch 41/52\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9863 - loss: 0.0335 - val_accuracy: 0.9919 - val_loss: 0.0122\n",
      "Epoch 42/52\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9875 - loss: 0.0274 - val_accuracy: 0.9959 - val_loss: 0.0077\n",
      "Epoch 43/52\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9907 - loss: 0.0251 - val_accuracy: 0.9878 - val_loss: 0.0171\n",
      "Epoch 44/52\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9918 - loss: 0.0222 - val_accuracy: 0.9959 - val_loss: 0.0063\n",
      "Epoch 45/52\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9883 - loss: 0.0203 - val_accuracy: 1.0000 - val_loss: 0.0056\n",
      "Epoch 46/52\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9952 - loss: 0.0149 - val_accuracy: 0.9959 - val_loss: 0.0116\n",
      "Epoch 47/52\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9885 - loss: 0.0213 - val_accuracy: 0.9919 - val_loss: 0.0124\n",
      "Epoch 48/52\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9922 - loss: 0.0174 - val_accuracy: 0.9919 - val_loss: 0.0138\n",
      "Epoch 49/52\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9897 - loss: 0.0204 - val_accuracy: 0.9959 - val_loss: 0.0099\n",
      "Epoch 50/52\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9920 - loss: 0.0160 - val_accuracy: 0.9959 - val_loss: 0.0064\n",
      "Epoch 51/52\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9965 - loss: 0.0138 - val_accuracy: 0.9959 - val_loss: 0.0098\n",
      "Epoch 52/52\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9932 - loss: 0.0154 - val_accuracy: 0.9959 - val_loss: 0.0098\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Evaluation =====\n",
      "Accuracy: 0.9680\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9091    0.4762    0.6250        42\n",
      "           1     0.9102    0.9867    0.9469       226\n",
      "           2     0.9980    0.9980    0.9980       509\n",
      "           3     0.9730    1.0000    0.9863        36\n",
      "\n",
      "    accuracy                         0.9680       813\n",
      "   macro avg     0.9476    0.8652    0.8891       813\n",
      "weighted avg     0.9679    0.9680    0.9640       813\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 20  22   0   0]\n",
      " [  2 223   1   0]\n",
      " [  0   0 508   1]\n",
      " [  0   0   0  36]]\n",
      "\n",
      "===== Model Metrics =====\n",
      "Training Time: 11.60s\n",
      "Inference/sample: 0.000287s\n",
      "Testing Time: 0.23s\n",
      "Model Size: 0.11 MB\n",
      "Trainable Params: 21,104\n",
      "RAM Usage: 1148.95 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import psutil\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from bayes_opt import BayesianOptimization\n",
    "from scipy.fftpack import fft\n",
    "import warnings\n",
    "\n",
    "# Set seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================\n",
    "# 1. Data Loading and Preprocessing\n",
    "# ============================================\n",
    "def load_and_preprocess_data(filepath):\n",
    "    df = pd.read_csv(filepath, delimiter=\",\", low_memory=False)\n",
    "    df.columns = df.columns.str.replace('\"', '').str.strip()\n",
    "\n",
    "    if 'TIME' not in df.columns:\n",
    "        raise KeyError(\"The column 'TIME' does not exist in the dataset.\")\n",
    "\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'], errors='coerce')\n",
    "    df.dropna(subset=['TIME'], inplace=True)\n",
    "    df.set_index('TIME', inplace=True)\n",
    "\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "    target_col = 'fenologia_h2'\n",
    "    if target_col not in df.columns:\n",
    "        raise KeyError(f\"The target column '{target_col}' does not exist.\")\n",
    "\n",
    "    df[target_col].interpolate(method='linear', inplace=True)\n",
    "    df[target_col] = df[target_col] - 1\n",
    "\n",
    "\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "    return df, target_col\n",
    "\n",
    "# ============================================\n",
    "# 2. Feature Scaling\n",
    "# ============================================\n",
    "def feature_scaling(df, target_col):\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))  # 3D\n",
    "    return X_scaled, y, scaler\n",
    "\n",
    "# ============================================\n",
    "# 3. CNN + Gate Attention Model\n",
    "# ============================================\n",
    "def gated_attention_block(inputs, units):\n",
    "    conv_output = layers.Conv1D(units, kernel_size=1, activation='relu')(inputs)\n",
    "    attention_scores = layers.Dense(units, activation='sigmoid')(conv_output)\n",
    "    gated_output = layers.Multiply()([conv_output, attention_scores])\n",
    "    return gated_output\n",
    "\n",
    "def build_cnn_gatt_model(input_shape, num_classes=4, num_units=64, dropout_rate=0.2):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = gated_attention_block(inputs, units=num_units)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    x = layers.Dense(num_units // 2, activation='relu')(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# ============================================\n",
    "# 4. Bayesian Optimization Objective\n",
    "# ============================================\n",
    "def objective_function(num_units, dropout_rate, epochs, batch_size, X_train, y_train):\n",
    "    model = build_cnn_gatt_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        num_classes=4,\n",
    "        num_units=int(num_units),\n",
    "        dropout_rate=dropout_rate\n",
    "    )\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=int(epochs),\n",
    "        batch_size=int(batch_size),\n",
    "        validation_split=0.2,\n",
    "        verbose=0\n",
    "    )\n",
    "    return np.max(history.history['val_accuracy'])\n",
    "\n",
    "# ============================================\n",
    "# 5. Optimize Hyperparameters\n",
    "# ============================================\n",
    "def optimize_hyperparameters(X_train, y_train):\n",
    "    pbounds = {\n",
    "        'num_units': (32, 128),\n",
    "        'dropout_rate': (0.1, 0.5),\n",
    "        'epochs': (50, 100),\n",
    "        'batch_size': (16, 64)\n",
    "    }\n",
    "\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=lambda num_units, dropout_rate, epochs, batch_size: objective_function(\n",
    "            num_units, dropout_rate, epochs, batch_size, X_train, y_train\n",
    "        ),\n",
    "        pbounds=pbounds,\n",
    "        random_state=42,\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    optimizer.maximize(init_points=5, n_iter=15)\n",
    "    return optimizer.max['params']\n",
    "\n",
    "# ============================================\n",
    "# 6. Train & Evaluate Model\n",
    "# ============================================\n",
    "def train_and_evaluate_model(X_train, X_test, y_train, y_test, best_params):\n",
    "    model = build_cnn_gatt_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        num_classes=4,\n",
    "        num_units=int(best_params['num_units']),\n",
    "        dropout_rate=float(best_params['dropout_rate'])\n",
    "    )\n",
    "\n",
    "    start_train = time.time()\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=int(best_params['epochs']),\n",
    "        batch_size=int(best_params['batch_size']),\n",
    "        validation_split=0.2,\n",
    "        verbose=1\n",
    "    )\n",
    "    training_time = time.time() - start_train\n",
    "\n",
    "    start_infer = time.time()\n",
    "    y_pred = model.predict(X_test).argmax(axis=1)\n",
    "    inference_time = (time.time() - start_infer) / len(X_test)\n",
    "    testing_time = time.time() - start_infer\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(\"\\n===== Evaluation =====\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(\"Report:\\n\", classification_report(y_test, y_pred, digits=4))\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "    model.save(\"temp_model.h5\", include_optimizer=False)\n",
    "    model_size = os.path.getsize(\"temp_model.h5\") / (1024 ** 2)\n",
    "    os.remove(\"temp_model.h5\")\n",
    "\n",
    "    total_params = np.sum([np.prod(v.shape) for v in model.trainable_weights])\n",
    "    ram_usage = psutil.Process(os.getpid()).memory_info().rss / (1024 ** 2)\n",
    "\n",
    "    print(\"\\n===== Model Metrics =====\")\n",
    "    print(f\"Training Time: {training_time:.2f}s\")\n",
    "    print(f\"Inference/sample: {inference_time:.6f}s\")\n",
    "    print(f\"Testing Time: {testing_time:.2f}s\")\n",
    "    print(f\"Model Size: {model_size:.2f} MB\")\n",
    "    print(f\"Trainable Params: {total_params:,}\")\n",
    "    print(f\"RAM Usage: {ram_usage:.2f} MB\")\n",
    "\n",
    "    return model, history\n",
    "\n",
    "# ============================================\n",
    "# 7. Main\n",
    "# ============================================\n",
    "def main():\n",
    "    train_path = r\"E:\\Abroad period research\\Phenology datasets\\Lasso_Selected_Features_H2\\train_lasso_selected.csv\"\n",
    "    test_path = r\"E:\\Abroad period research\\Phenology datasets\\Lasso_Selected_Features_H2\\test_lasso_selected.csv\"\n",
    "\n",
    "    df_train, target_col = load_and_preprocess_data(train_path)\n",
    "    X_train, y_train, _ = feature_scaling(df_train, target_col)\n",
    "\n",
    "    df_test, _ = load_and_preprocess_data(test_path)\n",
    "    X_test, y_test, _ = feature_scaling(df_test, target_col)\n",
    "\n",
    "    print(\"\\n Optimizing Hyperparameters...\")\n",
    "    best_params = optimize_hyperparameters(X_train, y_train)\n",
    "    print(\"\\n Best Params:\", best_params)\n",
    "\n",
    "    print(\"\\n Training Final Gate Attention CNN Model...\")\n",
    "    model, history = train_and_evaluate_model(X_train, X_test, y_train, y_test, best_params)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b1b473",
   "metadata": {},
   "source": [
    "CNN with a Gated Attention mechanism (Conv-GAtt) Code for Horizon 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4d47bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Optimizing Hyperparameters...\n",
      "|   iter    |  target   | batch_... | dropou... |  epochs   | num_units |\n",
      "-------------------------------------------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.9919   \u001b[39m | \u001b[39m33.98    \u001b[39m | \u001b[39m0.4803   \u001b[39m | \u001b[39m86.6     \u001b[39m | \u001b[39m89.47    \u001b[39m |\n",
      "| \u001b[35m2        \u001b[39m | \u001b[35m0.9959   \u001b[39m | \u001b[35m23.49    \u001b[39m | \u001b[35m0.1624   \u001b[39m | \u001b[35m52.9     \u001b[39m | \u001b[35m115.2    \u001b[39m |\n",
      "| \u001b[39m3        \u001b[39m | \u001b[39m0.9919   \u001b[39m | \u001b[39m44.85    \u001b[39m | \u001b[39m0.3832   \u001b[39m | \u001b[39m51.03    \u001b[39m | \u001b[39m125.1    \u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m0.9919   \u001b[39m | \u001b[39m55.96    \u001b[39m | \u001b[39m0.1849   \u001b[39m | \u001b[39m59.09    \u001b[39m | \u001b[39m49.61    \u001b[39m |\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m0.9959   \u001b[39m | \u001b[39m30.6     \u001b[39m | \u001b[39m0.3099   \u001b[39m | \u001b[39m71.6     \u001b[39m | \u001b[39m59.96    \u001b[39m |\n",
      "| \u001b[39m6        \u001b[39m | \u001b[39m0.9878   \u001b[39m | \u001b[39m30.28    \u001b[39m | \u001b[39m0.3338   \u001b[39m | \u001b[39m71.98    \u001b[39m | \u001b[39m61.01    \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m0.9878   \u001b[39m | \u001b[39m30.43    \u001b[39m | \u001b[39m0.4113   \u001b[39m | \u001b[39m71.94    \u001b[39m | \u001b[39m59.91    \u001b[39m |\n",
      "| \u001b[35m8        \u001b[39m | \u001b[35m1.0      \u001b[39m | \u001b[35m16.21    \u001b[39m | \u001b[35m0.1161   \u001b[39m | \u001b[35m73.32    \u001b[39m | \u001b[35m65.8     \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m0.9919   \u001b[39m | \u001b[39m62.6     \u001b[39m | \u001b[39m0.3838   \u001b[39m | \u001b[39m99.69    \u001b[39m | \u001b[39m67.9     \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m0.9837   \u001b[39m | \u001b[39m38.63    \u001b[39m | \u001b[39m0.1642   \u001b[39m | \u001b[39m90.1     \u001b[39m | \u001b[39m41.26    \u001b[39m |\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m0.9919   \u001b[39m | \u001b[39m61.04    \u001b[39m | \u001b[39m0.3825   \u001b[39m | \u001b[39m72.51    \u001b[39m | \u001b[39m127.9    \u001b[39m |\n",
      "| \u001b[39m12       \u001b[39m | \u001b[39m0.9837   \u001b[39m | \u001b[39m53.21    \u001b[39m | \u001b[39m0.4529   \u001b[39m | \u001b[39m61.36    \u001b[39m | \u001b[39m67.75    \u001b[39m |\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m0.9959   \u001b[39m | \u001b[39m25.29    \u001b[39m | \u001b[39m0.3485   \u001b[39m | \u001b[39m62.41    \u001b[39m | \u001b[39m105.7    \u001b[39m |\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m16.3     \u001b[39m | \u001b[39m0.2837   \u001b[39m | \u001b[39m98.83    \u001b[39m | \u001b[39m82.57    \u001b[39m |\n",
      "| \u001b[39m15       \u001b[39m | \u001b[39m0.9919   \u001b[39m | \u001b[39m51.64    \u001b[39m | \u001b[39m0.31     \u001b[39m | \u001b[39m65.02    \u001b[39m | \u001b[39m33.89    \u001b[39m |\n",
      "| \u001b[39m16       \u001b[39m | \u001b[39m0.9837   \u001b[39m | \u001b[39m58.58    \u001b[39m | \u001b[39m0.3124   \u001b[39m | \u001b[39m62.84    \u001b[39m | \u001b[39m82.99    \u001b[39m |\n",
      "| \u001b[39m17       \u001b[39m | \u001b[39m0.9797   \u001b[39m | \u001b[39m25.9     \u001b[39m | \u001b[39m0.488    \u001b[39m | \u001b[39m50.72    \u001b[39m | \u001b[39m38.67    \u001b[39m |\n",
      "| \u001b[39m18       \u001b[39m | \u001b[39m0.9919   \u001b[39m | \u001b[39m37.52    \u001b[39m | \u001b[39m0.1352   \u001b[39m | \u001b[39m52.59    \u001b[39m | \u001b[39m112.3    \u001b[39m |\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m0.9959   \u001b[39m | \u001b[39m19.99    \u001b[39m | \u001b[39m0.4194   \u001b[39m | \u001b[39m96.1     \u001b[39m | \u001b[39m125.3    \u001b[39m |\n",
      "| \u001b[39m20       \u001b[39m | \u001b[39m0.9959   \u001b[39m | \u001b[39m33.5     \u001b[39m | \u001b[39m0.2147   \u001b[39m | \u001b[39m84.61    \u001b[39m | \u001b[39m96.06    \u001b[39m |\n",
      "=========================================================================\n",
      "\n",
      " Best Params: {'batch_size': 16.214948111954957, 'dropout_rate': 0.11614649803945523, 'epochs': 73.32263026686323, 'num_units': 65.80234720602593}\n",
      "\n",
      " Training Final Gate Attention CNN Model...\n",
      "Epoch 1/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.3395 - loss: 1.2993 - val_accuracy: 0.7480 - val_loss: 0.8828\n",
      "Epoch 2/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7015 - loss: 0.8456 - val_accuracy: 0.8537 - val_loss: 0.5278\n",
      "Epoch 3/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8621 - loss: 0.5220 - val_accuracy: 0.8780 - val_loss: 0.3724\n",
      "Epoch 4/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8815 - loss: 0.3974 - val_accuracy: 0.8984 - val_loss: 0.2944\n",
      "Epoch 5/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8993 - loss: 0.3152 - val_accuracy: 0.9146 - val_loss: 0.2485\n",
      "Epoch 6/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9013 - loss: 0.2923 - val_accuracy: 0.9187 - val_loss: 0.2239\n",
      "Epoch 7/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9163 - loss: 0.2665 - val_accuracy: 0.9187 - val_loss: 0.2010\n",
      "Epoch 8/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9218 - loss: 0.2457 - val_accuracy: 0.9268 - val_loss: 0.1708\n",
      "Epoch 9/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9378 - loss: 0.2115 - val_accuracy: 0.9350 - val_loss: 0.1625\n",
      "Epoch 10/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9346 - loss: 0.2028 - val_accuracy: 0.9472 - val_loss: 0.1464\n",
      "Epoch 11/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9388 - loss: 0.1898 - val_accuracy: 0.9512 - val_loss: 0.1316\n",
      "Epoch 12/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9411 - loss: 0.1805 - val_accuracy: 0.9512 - val_loss: 0.1223\n",
      "Epoch 13/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9492 - loss: 0.1595 - val_accuracy: 0.9593 - val_loss: 0.1183\n",
      "Epoch 14/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9481 - loss: 0.1519 - val_accuracy: 0.9715 - val_loss: 0.1080\n",
      "Epoch 15/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9498 - loss: 0.1345 - val_accuracy: 0.9675 - val_loss: 0.1020\n",
      "Epoch 16/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9536 - loss: 0.1355 - val_accuracy: 0.9715 - val_loss: 0.0983\n",
      "Epoch 17/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9601 - loss: 0.1206 - val_accuracy: 0.9715 - val_loss: 0.0889\n",
      "Epoch 18/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9595 - loss: 0.1253 - val_accuracy: 0.9756 - val_loss: 0.0813\n",
      "Epoch 19/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9601 - loss: 0.1211 - val_accuracy: 0.9797 - val_loss: 0.0816\n",
      "Epoch 20/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9676 - loss: 0.1065 - val_accuracy: 0.9756 - val_loss: 0.0752\n",
      "Epoch 21/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9607 - loss: 0.1092 - val_accuracy: 0.9837 - val_loss: 0.0724\n",
      "Epoch 22/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9707 - loss: 0.1001 - val_accuracy: 0.9837 - val_loss: 0.0707\n",
      "Epoch 23/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9724 - loss: 0.0983 - val_accuracy: 0.9837 - val_loss: 0.0666\n",
      "Epoch 24/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9767 - loss: 0.0962 - val_accuracy: 0.9837 - val_loss: 0.0638\n",
      "Epoch 25/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9784 - loss: 0.0836 - val_accuracy: 0.9878 - val_loss: 0.0604\n",
      "Epoch 26/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9744 - loss: 0.0888 - val_accuracy: 0.9878 - val_loss: 0.0579\n",
      "Epoch 27/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9754 - loss: 0.0785 - val_accuracy: 0.9878 - val_loss: 0.0553\n",
      "Epoch 28/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9800 - loss: 0.0714 - val_accuracy: 0.9837 - val_loss: 0.0528\n",
      "Epoch 29/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9782 - loss: 0.0745 - val_accuracy: 0.9878 - val_loss: 0.0524\n",
      "Epoch 30/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9719 - loss: 0.0841 - val_accuracy: 0.9837 - val_loss: 0.0504\n",
      "Epoch 31/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9801 - loss: 0.0705 - val_accuracy: 0.9837 - val_loss: 0.0484\n",
      "Epoch 32/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9783 - loss: 0.0754 - val_accuracy: 0.9837 - val_loss: 0.0478\n",
      "Epoch 33/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9786 - loss: 0.0683 - val_accuracy: 0.9878 - val_loss: 0.0458\n",
      "Epoch 34/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9816 - loss: 0.0624 - val_accuracy: 0.9878 - val_loss: 0.0442\n",
      "Epoch 35/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9800 - loss: 0.0651 - val_accuracy: 0.9878 - val_loss: 0.0434\n",
      "Epoch 36/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9835 - loss: 0.0615 - val_accuracy: 0.9878 - val_loss: 0.0430\n",
      "Epoch 37/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9780 - loss: 0.0635 - val_accuracy: 0.9878 - val_loss: 0.0419\n",
      "Epoch 38/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9808 - loss: 0.0589 - val_accuracy: 0.9837 - val_loss: 0.0388\n",
      "Epoch 39/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9800 - loss: 0.0569 - val_accuracy: 0.9878 - val_loss: 0.0366\n",
      "Epoch 40/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9837 - loss: 0.0555 - val_accuracy: 0.9878 - val_loss: 0.0357\n",
      "Epoch 41/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9806 - loss: 0.0538 - val_accuracy: 0.9837 - val_loss: 0.0349\n",
      "Epoch 42/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9808 - loss: 0.0621 - val_accuracy: 0.9878 - val_loss: 0.0338\n",
      "Epoch 43/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9854 - loss: 0.0498 - val_accuracy: 0.9878 - val_loss: 0.0337\n",
      "Epoch 44/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9807 - loss: 0.0513 - val_accuracy: 0.9878 - val_loss: 0.0312\n",
      "Epoch 45/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9864 - loss: 0.0407 - val_accuracy: 0.9878 - val_loss: 0.0308\n",
      "Epoch 46/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9873 - loss: 0.0408 - val_accuracy: 0.9878 - val_loss: 0.0305\n",
      "Epoch 47/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9919 - loss: 0.0355 - val_accuracy: 0.9919 - val_loss: 0.0291\n",
      "Epoch 48/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9888 - loss: 0.0385 - val_accuracy: 0.9878 - val_loss: 0.0289\n",
      "Epoch 49/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9837 - loss: 0.0427 - val_accuracy: 0.9919 - val_loss: 0.0280\n",
      "Epoch 50/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9875 - loss: 0.0378 - val_accuracy: 0.9919 - val_loss: 0.0271\n",
      "Epoch 51/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9874 - loss: 0.0401 - val_accuracy: 0.9919 - val_loss: 0.0251\n",
      "Epoch 52/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9926 - loss: 0.0286 - val_accuracy: 0.9919 - val_loss: 0.0231\n",
      "Epoch 53/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9914 - loss: 0.0354 - val_accuracy: 0.9919 - val_loss: 0.0277\n",
      "Epoch 54/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9840 - loss: 0.0355 - val_accuracy: 0.9919 - val_loss: 0.0234\n",
      "Epoch 55/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9884 - loss: 0.0360 - val_accuracy: 0.9919 - val_loss: 0.0199\n",
      "Epoch 56/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9900 - loss: 0.0302 - val_accuracy: 0.9919 - val_loss: 0.0211\n",
      "Epoch 57/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9905 - loss: 0.0280 - val_accuracy: 0.9919 - val_loss: 0.0200\n",
      "Epoch 58/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9875 - loss: 0.0283 - val_accuracy: 0.9919 - val_loss: 0.0230\n",
      "Epoch 59/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9843 - loss: 0.0404 - val_accuracy: 0.9919 - val_loss: 0.0185\n",
      "Epoch 60/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9924 - loss: 0.0215 - val_accuracy: 0.9919 - val_loss: 0.0182\n",
      "Epoch 61/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9916 - loss: 0.0288 - val_accuracy: 0.9919 - val_loss: 0.0180\n",
      "Epoch 62/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9868 - loss: 0.0324 - val_accuracy: 0.9919 - val_loss: 0.0178\n",
      "Epoch 63/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9895 - loss: 0.0263 - val_accuracy: 0.9919 - val_loss: 0.0171\n",
      "Epoch 64/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9913 - loss: 0.0262 - val_accuracy: 0.9959 - val_loss: 0.0173\n",
      "Epoch 65/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9888 - loss: 0.0315 - val_accuracy: 0.9919 - val_loss: 0.0164\n",
      "Epoch 66/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9916 - loss: 0.0210 - val_accuracy: 0.9959 - val_loss: 0.0178\n",
      "Epoch 67/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9894 - loss: 0.0256 - val_accuracy: 0.9919 - val_loss: 0.0154\n",
      "Epoch 68/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9918 - loss: 0.0219 - val_accuracy: 0.9959 - val_loss: 0.0174\n",
      "Epoch 69/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9923 - loss: 0.0226 - val_accuracy: 0.9959 - val_loss: 0.0198\n",
      "Epoch 70/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9941 - loss: 0.0225 - val_accuracy: 0.9959 - val_loss: 0.0171\n",
      "Epoch 71/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9856 - loss: 0.0293 - val_accuracy: 0.9959 - val_loss: 0.0145\n",
      "Epoch 72/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9896 - loss: 0.0204 - val_accuracy: 0.9959 - val_loss: 0.0154\n",
      "Epoch 73/73\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9946 - loss: 0.0187 - val_accuracy: 0.9919 - val_loss: 0.0163\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Evaluation =====\n",
      "Accuracy: 0.9508\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.4074    0.5789        27\n",
      "           1     0.9256    0.9343    0.9299       213\n",
      "           2     0.9866    0.9809    0.9837       524\n",
      "           3     0.7424    1.0000    0.8522        49\n",
      "\n",
      "    accuracy                         0.9508       813\n",
      "   macro avg     0.9136    0.8306    0.8362       813\n",
      "weighted avg     0.9563    0.9508    0.9483       813\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 11  16   0   0]\n",
      " [  0 199   7   7]\n",
      " [  0   0 514  10]\n",
      " [  0   0   0  49]]\n",
      "\n",
      "===== Model Metrics =====\n",
      "Training Time: 22.99s\n",
      "Inference/sample: 0.000347s\n",
      "Testing Time: 0.28s\n",
      "Model Size: 0.06 MB\n",
      "Trainable Params: 7,704\n",
      "RAM Usage: 1404.92 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import psutil\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from bayes_opt import BayesianOptimization\n",
    "from scipy.fftpack import fft\n",
    "import warnings\n",
    "\n",
    "# Set seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================\n",
    "# 1. Data Loading and Preprocessing\n",
    "# ============================================\n",
    "def load_and_preprocess_data(filepath):\n",
    "    df = pd.read_csv(filepath, delimiter=\",\", low_memory=False)\n",
    "    df.columns = df.columns.str.replace('\"', '').str.strip()\n",
    "\n",
    "    if 'TIME' not in df.columns:\n",
    "        raise KeyError(\"The column 'TIME' does not exist in the dataset.\")\n",
    "\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'], errors='coerce')\n",
    "    df.dropna(subset=['TIME'], inplace=True)\n",
    "    df.set_index('TIME', inplace=True)\n",
    "\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "    target_col = 'fenologia_h3'\n",
    "    if target_col not in df.columns:\n",
    "        raise KeyError(f\"The target column '{target_col}' does not exist.\")\n",
    "\n",
    "    df[target_col].interpolate(method='linear', inplace=True)\n",
    "    df[target_col] = df[target_col] - 1\n",
    "\n",
    "    for i in range(1, 4):\n",
    "        df[f'{target_col}_lag{i}'] = df[target_col].shift(i)\n",
    "\n",
    "    for window in [3, 6]:\n",
    "        df[f'{target_col}_roll_mean_{window}'] = df[target_col].rolling(window).mean()\n",
    "        df[f'{target_col}_roll_std_{window}'] = df[target_col].rolling(window).std()\n",
    "\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "    return df, target_col\n",
    "\n",
    "# ============================================\n",
    "# 2. Feature Scaling\n",
    "# ============================================\n",
    "def feature_scaling(df, target_col):\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))  # 3D\n",
    "    return X_scaled, y, scaler\n",
    "\n",
    "# ============================================\n",
    "# 3. CNN + Gate Attention Model\n",
    "# ============================================\n",
    "def gated_attention_block(inputs, units):\n",
    "    conv_output = layers.Conv1D(units, kernel_size=1, activation='relu')(inputs)\n",
    "    attention_scores = layers.Dense(units, activation='sigmoid')(conv_output)\n",
    "    gated_output = layers.Multiply()([conv_output, attention_scores])\n",
    "    return gated_output\n",
    "\n",
    "def build_cnn_gatt_model(input_shape, num_classes=4, num_units=64, dropout_rate=0.2):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = gated_attention_block(inputs, units=num_units)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    x = layers.Dense(num_units // 2, activation='relu')(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# ============================================\n",
    "# 4. Bayesian Optimization Objective\n",
    "# ============================================\n",
    "def objective_function(num_units, dropout_rate, epochs, batch_size, X_train, y_train):\n",
    "    model = build_cnn_gatt_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        num_classes=4,\n",
    "        num_units=int(num_units),\n",
    "        dropout_rate=dropout_rate\n",
    "    )\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=int(epochs),\n",
    "        batch_size=int(batch_size),\n",
    "        validation_split=0.2,\n",
    "        verbose=0\n",
    "    )\n",
    "    return np.max(history.history['val_accuracy'])\n",
    "\n",
    "# ============================================\n",
    "# 5. Optimize Hyperparameters\n",
    "# ============================================\n",
    "def optimize_hyperparameters(X_train, y_train):\n",
    "    pbounds = {\n",
    "        'num_units': (32, 128),\n",
    "        'dropout_rate': (0.1, 0.5),\n",
    "        'epochs': (50, 100),\n",
    "        'batch_size': (16, 64)\n",
    "    }\n",
    "\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=lambda num_units, dropout_rate, epochs, batch_size: objective_function(\n",
    "            num_units, dropout_rate, epochs, batch_size, X_train, y_train\n",
    "        ),\n",
    "        pbounds=pbounds,\n",
    "        random_state=42,\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    optimizer.maximize(init_points=5, n_iter=15)\n",
    "    return optimizer.max['params']\n",
    "\n",
    "# ============================================\n",
    "# 6. Train & Evaluate Model\n",
    "# ============================================\n",
    "def train_and_evaluate_model(X_train, X_test, y_train, y_test, best_params):\n",
    "    model = build_cnn_gatt_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        num_classes=4,\n",
    "        num_units=int(best_params['num_units']),\n",
    "        dropout_rate=float(best_params['dropout_rate'])\n",
    "    )\n",
    "\n",
    "    start_train = time.time()\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=int(best_params['epochs']),\n",
    "        batch_size=int(best_params['batch_size']),\n",
    "        validation_split=0.2,\n",
    "        verbose=1\n",
    "    )\n",
    "    training_time = time.time() - start_train\n",
    "\n",
    "    start_infer = time.time()\n",
    "    y_pred = model.predict(X_test).argmax(axis=1)\n",
    "    inference_time = (time.time() - start_infer) / len(X_test)\n",
    "    testing_time = time.time() - start_infer\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(\"\\n===== Evaluation =====\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(\"Report:\\n\", classification_report(y_test, y_pred, digits=4))\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "    model.save(\"temp_model.h5\", include_optimizer=False)\n",
    "    model_size = os.path.getsize(\"temp_model.h5\") / (1024 ** 2)\n",
    "    os.remove(\"temp_model.h5\")\n",
    "\n",
    "    total_params = np.sum([np.prod(v.shape) for v in model.trainable_weights])\n",
    "    ram_usage = psutil.Process(os.getpid()).memory_info().rss / (1024 ** 2)\n",
    "\n",
    "    print(\"\\n===== Model Metrics =====\")\n",
    "    print(f\"Training Time: {training_time:.2f}s\")\n",
    "    print(f\"Inference/sample: {inference_time:.6f}s\")\n",
    "    print(f\"Testing Time: {testing_time:.2f}s\")\n",
    "    print(f\"Model Size: {model_size:.2f} MB\")\n",
    "    print(f\"Trainable Params: {total_params:,}\")\n",
    "    print(f\"RAM Usage: {ram_usage:.2f} MB\")\n",
    "\n",
    "    return model, history\n",
    "\n",
    "# ============================================\n",
    "# 7. Main\n",
    "# ============================================\n",
    "def main():\n",
    "    train_path = r\"E:\\Abroad period research\\Phenology datasets\\PHENOLOGY_H3_train.csv\"\n",
    "    test_path = r\"E:\\Abroad period research\\Phenology datasets\\PHENOLOGY_H3_test.csv\"\n",
    "\n",
    "    df_train, target_col = load_and_preprocess_data(train_path)\n",
    "    X_train, y_train, _ = feature_scaling(df_train, target_col)\n",
    "\n",
    "    df_test, _ = load_and_preprocess_data(test_path)\n",
    "    X_test, y_test, _ = feature_scaling(df_test, target_col)\n",
    "\n",
    "    print(\"\\n Optimizing Hyperparameters...\")\n",
    "    best_params = optimize_hyperparameters(X_train, y_train)\n",
    "    print(\"\\n Best Params:\", best_params)\n",
    "\n",
    "    print(\"\\n Training Final Gate Attention CNN Model...\")\n",
    "    model, history = train_and_evaluate_model(X_train, X_test, y_train, y_test, best_params)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5da5d00",
   "metadata": {},
   "source": [
    "CNN with a Gated Attention mechanism (Conv-GAtt) Code for Horizon 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2064b09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Optimizing Hyperparameters...\n",
      "|   iter    |  target   | batch_... | dropou... |  epochs   | num_units |\n",
      "-------------------------------------------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.9919   \u001b[39m | \u001b[39m33.98    \u001b[39m | \u001b[39m0.4803   \u001b[39m | \u001b[39m86.6     \u001b[39m | \u001b[39m89.47    \u001b[39m |\n",
      "| \u001b[39m2        \u001b[39m | \u001b[39m0.9919   \u001b[39m | \u001b[39m23.49    \u001b[39m | \u001b[39m0.1624   \u001b[39m | \u001b[39m52.9     \u001b[39m | \u001b[39m115.2    \u001b[39m |\n",
      "| \u001b[39m3        \u001b[39m | \u001b[39m0.9919   \u001b[39m | \u001b[39m44.85    \u001b[39m | \u001b[39m0.3832   \u001b[39m | \u001b[39m51.03    \u001b[39m | \u001b[39m125.1    \u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m0.9837   \u001b[39m | \u001b[39m55.96    \u001b[39m | \u001b[39m0.1849   \u001b[39m | \u001b[39m59.09    \u001b[39m | \u001b[39m49.61    \u001b[39m |\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m0.9837   \u001b[39m | \u001b[39m30.6     \u001b[39m | \u001b[39m0.3099   \u001b[39m | \u001b[39m71.6     \u001b[39m | \u001b[39m59.96    \u001b[39m |\n",
      "| \u001b[35m6        \u001b[39m | \u001b[35m0.9959   \u001b[39m | \u001b[35m33.45    \u001b[39m | \u001b[35m0.2494   \u001b[39m | \u001b[35m99.66    \u001b[39m | \u001b[35m127.5    \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m0.9919   \u001b[39m | \u001b[39m63.83    \u001b[39m | \u001b[39m0.1911   \u001b[39m | \u001b[39m99.11    \u001b[39m | \u001b[39m127.8    \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m0.9959   \u001b[39m | \u001b[39m34.57    \u001b[39m | \u001b[39m0.107    \u001b[39m | \u001b[39m98.66    \u001b[39m | \u001b[39m124.8    \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m0.9959   \u001b[39m | \u001b[39m16.43    \u001b[39m | \u001b[39m0.3942   \u001b[39m | \u001b[39m99.17    \u001b[39m | \u001b[39m113.4    \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m0.9959   \u001b[39m | \u001b[39m16.05    \u001b[39m | \u001b[39m0.4561   \u001b[39m | \u001b[39m80.68    \u001b[39m | \u001b[39m125.0    \u001b[39m |\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m0.9959   \u001b[39m | \u001b[39m18.27    \u001b[39m | \u001b[39m0.3674   \u001b[39m | \u001b[39m99.38    \u001b[39m | \u001b[39m128.0    \u001b[39m |\n",
      "| \u001b[39m12       \u001b[39m | \u001b[39m0.9959   \u001b[39m | \u001b[39m30.97    \u001b[39m | \u001b[39m0.4573   \u001b[39m | \u001b[39m84.88    \u001b[39m | \u001b[39m126.8    \u001b[39m |\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m0.9959   \u001b[39m | \u001b[39m22.72    \u001b[39m | \u001b[39m0.1984   \u001b[39m | \u001b[39m90.14    \u001b[39m | \u001b[39m118.6    \u001b[39m |\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m0.9919   \u001b[39m | \u001b[39m30.15    \u001b[39m | \u001b[39m0.4678   \u001b[39m | \u001b[39m99.89    \u001b[39m | \u001b[39m109.1    \u001b[39m |\n",
      "| \u001b[39m15       \u001b[39m | \u001b[39m0.9878   \u001b[39m | \u001b[39m63.78    \u001b[39m | \u001b[39m0.4501   \u001b[39m | \u001b[39m68.19    \u001b[39m | \u001b[39m107.6    \u001b[39m |\n",
      "| \u001b[39m16       \u001b[39m | \u001b[39m0.9919   \u001b[39m | \u001b[39m16.72    \u001b[39m | \u001b[39m0.1954   \u001b[39m | \u001b[39m99.07    \u001b[39m | \u001b[39m34.35    \u001b[39m |\n",
      "| \u001b[39m17       \u001b[39m | \u001b[39m0.9837   \u001b[39m | \u001b[39m63.24    \u001b[39m | \u001b[39m0.4801   \u001b[39m | \u001b[39m99.09    \u001b[39m | \u001b[39m32.68    \u001b[39m |\n",
      "| \u001b[39m18       \u001b[39m | \u001b[39m0.9959   \u001b[39m | \u001b[39m16.32    \u001b[39m | \u001b[39m0.3715   \u001b[39m | \u001b[39m77.74    \u001b[39m | \u001b[39m100.3    \u001b[39m |\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m0.9959   \u001b[39m | \u001b[39m16.3     \u001b[39m | \u001b[39m0.2756   \u001b[39m | \u001b[39m96.42    \u001b[39m | \u001b[39m86.32    \u001b[39m |\n",
      "| \u001b[35m20       \u001b[39m | \u001b[35m1.0      \u001b[39m | \u001b[35m16.45    \u001b[39m | \u001b[35m0.2366   \u001b[39m | \u001b[35m99.17    \u001b[39m | \u001b[35m63.21    \u001b[39m |\n",
      "=========================================================================\n",
      "\n",
      " Best Params: {'batch_size': 16.449633785117143, 'dropout_rate': 0.2366202637362964, 'epochs': 99.17111465520624, 'num_units': 63.209864175008775}\n",
      "\n",
      " Training Final Gate Attention CNN Model...\n",
      "Epoch 1/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.5079 - loss: 1.1740 - val_accuracy: 0.7276 - val_loss: 0.8459\n",
      "Epoch 2/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6857 - loss: 0.8481 - val_accuracy: 0.8008 - val_loss: 0.5512\n",
      "Epoch 3/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8065 - loss: 0.5789 - val_accuracy: 0.8496 - val_loss: 0.4031\n",
      "Epoch 4/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8305 - loss: 0.4485 - val_accuracy: 0.8537 - val_loss: 0.3286\n",
      "Epoch 5/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8511 - loss: 0.3836 - val_accuracy: 0.8862 - val_loss: 0.2774\n",
      "Epoch 6/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8788 - loss: 0.3518 - val_accuracy: 0.8943 - val_loss: 0.2471\n",
      "Epoch 7/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8859 - loss: 0.3028 - val_accuracy: 0.9024 - val_loss: 0.2179\n",
      "Epoch 8/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8925 - loss: 0.2940 - val_accuracy: 0.9228 - val_loss: 0.1938\n",
      "Epoch 9/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9188 - loss: 0.2459 - val_accuracy: 0.9268 - val_loss: 0.1750\n",
      "Epoch 10/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9157 - loss: 0.2334 - val_accuracy: 0.9390 - val_loss: 0.1628\n",
      "Epoch 11/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9208 - loss: 0.2278 - val_accuracy: 0.9675 - val_loss: 0.1455\n",
      "Epoch 12/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9325 - loss: 0.1907 - val_accuracy: 0.9715 - val_loss: 0.1301\n",
      "Epoch 13/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9435 - loss: 0.1854 - val_accuracy: 0.9756 - val_loss: 0.1232\n",
      "Epoch 14/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9473 - loss: 0.1791 - val_accuracy: 0.9797 - val_loss: 0.1134\n",
      "Epoch 15/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9384 - loss: 0.1799 - val_accuracy: 0.9837 - val_loss: 0.1072\n",
      "Epoch 16/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9515 - loss: 0.1543 - val_accuracy: 0.9878 - val_loss: 0.0982\n",
      "Epoch 17/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9524 - loss: 0.1649 - val_accuracy: 0.9878 - val_loss: 0.1000\n",
      "Epoch 18/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9565 - loss: 0.1495 - val_accuracy: 0.9878 - val_loss: 0.0909\n",
      "Epoch 19/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9602 - loss: 0.1316 - val_accuracy: 0.9878 - val_loss: 0.0860\n",
      "Epoch 20/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9609 - loss: 0.1286 - val_accuracy: 0.9878 - val_loss: 0.0824\n",
      "Epoch 21/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9615 - loss: 0.1230 - val_accuracy: 0.9878 - val_loss: 0.0780\n",
      "Epoch 22/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9637 - loss: 0.1190 - val_accuracy: 0.9878 - val_loss: 0.0768\n",
      "Epoch 23/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9693 - loss: 0.1145 - val_accuracy: 0.9878 - val_loss: 0.0723\n",
      "Epoch 24/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9709 - loss: 0.1041 - val_accuracy: 0.9878 - val_loss: 0.0725\n",
      "Epoch 25/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9662 - loss: 0.1026 - val_accuracy: 0.9878 - val_loss: 0.0725\n",
      "Epoch 26/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9746 - loss: 0.0927 - val_accuracy: 0.9878 - val_loss: 0.0654\n",
      "Epoch 27/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9746 - loss: 0.1077 - val_accuracy: 0.9878 - val_loss: 0.0617\n",
      "Epoch 28/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9702 - loss: 0.1060 - val_accuracy: 0.9878 - val_loss: 0.0621\n",
      "Epoch 29/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9687 - loss: 0.0945 - val_accuracy: 0.9878 - val_loss: 0.0559\n",
      "Epoch 30/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9749 - loss: 0.0855 - val_accuracy: 0.9878 - val_loss: 0.0597\n",
      "Epoch 31/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9751 - loss: 0.0819 - val_accuracy: 0.9878 - val_loss: 0.0492\n",
      "Epoch 32/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9751 - loss: 0.0826 - val_accuracy: 0.9878 - val_loss: 0.0506\n",
      "Epoch 33/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9709 - loss: 0.0723 - val_accuracy: 0.9878 - val_loss: 0.0507\n",
      "Epoch 34/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9739 - loss: 0.0797 - val_accuracy: 0.9878 - val_loss: 0.0455\n",
      "Epoch 35/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9790 - loss: 0.0630 - val_accuracy: 0.9878 - val_loss: 0.0513\n",
      "Epoch 36/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9804 - loss: 0.0626 - val_accuracy: 0.9878 - val_loss: 0.0468\n",
      "Epoch 37/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9814 - loss: 0.0602 - val_accuracy: 0.9878 - val_loss: 0.0441\n",
      "Epoch 38/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9805 - loss: 0.0634 - val_accuracy: 0.9878 - val_loss: 0.0389\n",
      "Epoch 39/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9786 - loss: 0.0635 - val_accuracy: 0.9878 - val_loss: 0.0413\n",
      "Epoch 40/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9759 - loss: 0.0632 - val_accuracy: 0.9878 - val_loss: 0.0381\n",
      "Epoch 41/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9845 - loss: 0.0541 - val_accuracy: 0.9878 - val_loss: 0.0338\n",
      "Epoch 42/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9811 - loss: 0.0548 - val_accuracy: 0.9878 - val_loss: 0.0420\n",
      "Epoch 43/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9843 - loss: 0.0452 - val_accuracy: 0.9878 - val_loss: 0.0319\n",
      "Epoch 44/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9836 - loss: 0.0530 - val_accuracy: 0.9878 - val_loss: 0.0335\n",
      "Epoch 45/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9814 - loss: 0.0545 - val_accuracy: 0.9878 - val_loss: 0.0326\n",
      "Epoch 46/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9862 - loss: 0.0460 - val_accuracy: 0.9878 - val_loss: 0.0268\n",
      "Epoch 47/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9798 - loss: 0.0540 - val_accuracy: 0.9919 - val_loss: 0.0286\n",
      "Epoch 48/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9843 - loss: 0.0448 - val_accuracy: 0.9878 - val_loss: 0.0302\n",
      "Epoch 49/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9842 - loss: 0.0543 - val_accuracy: 0.9878 - val_loss: 0.0293\n",
      "Epoch 50/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9854 - loss: 0.0375 - val_accuracy: 0.9919 - val_loss: 0.0244\n",
      "Epoch 51/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9904 - loss: 0.0344 - val_accuracy: 0.9919 - val_loss: 0.0287\n",
      "Epoch 52/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9876 - loss: 0.0342 - val_accuracy: 0.9919 - val_loss: 0.0202\n",
      "Epoch 53/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9917 - loss: 0.0374 - val_accuracy: 0.9919 - val_loss: 0.0265\n",
      "Epoch 54/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9913 - loss: 0.0288 - val_accuracy: 0.9919 - val_loss: 0.0290\n",
      "Epoch 55/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9837 - loss: 0.0438 - val_accuracy: 0.9919 - val_loss: 0.0230\n",
      "Epoch 56/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9793 - loss: 0.0453 - val_accuracy: 0.9919 - val_loss: 0.0205\n",
      "Epoch 57/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9895 - loss: 0.0395 - val_accuracy: 0.9919 - val_loss: 0.0248\n",
      "Epoch 58/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9832 - loss: 0.0352 - val_accuracy: 0.9919 - val_loss: 0.0207\n",
      "Epoch 59/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9926 - loss: 0.0282 - val_accuracy: 0.9919 - val_loss: 0.0248\n",
      "Epoch 60/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9950 - loss: 0.0247 - val_accuracy: 0.9919 - val_loss: 0.0218\n",
      "Epoch 61/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9927 - loss: 0.0268 - val_accuracy: 0.9959 - val_loss: 0.0207\n",
      "Epoch 62/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9899 - loss: 0.0364 - val_accuracy: 0.9919 - val_loss: 0.0287\n",
      "Epoch 63/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9906 - loss: 0.0234 - val_accuracy: 0.9919 - val_loss: 0.0193\n",
      "Epoch 64/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9909 - loss: 0.0268 - val_accuracy: 0.9959 - val_loss: 0.0135\n",
      "Epoch 65/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9960 - loss: 0.0201 - val_accuracy: 0.9919 - val_loss: 0.0196\n",
      "Epoch 66/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9922 - loss: 0.0178 - val_accuracy: 0.9919 - val_loss: 0.0184\n",
      "Epoch 67/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9959 - loss: 0.0209 - val_accuracy: 0.9919 - val_loss: 0.0176\n",
      "Epoch 68/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9906 - loss: 0.0266 - val_accuracy: 0.9919 - val_loss: 0.0176\n",
      "Epoch 69/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9918 - loss: 0.0231 - val_accuracy: 0.9959 - val_loss: 0.0162\n",
      "Epoch 70/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9968 - loss: 0.0202 - val_accuracy: 0.9959 - val_loss: 0.0179\n",
      "Epoch 71/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9953 - loss: 0.0191 - val_accuracy: 0.9919 - val_loss: 0.0208\n",
      "Epoch 72/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9932 - loss: 0.0305 - val_accuracy: 0.9919 - val_loss: 0.0189\n",
      "Epoch 73/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9939 - loss: 0.0209 - val_accuracy: 0.9919 - val_loss: 0.0170\n",
      "Epoch 74/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9927 - loss: 0.0182 - val_accuracy: 0.9919 - val_loss: 0.0173\n",
      "Epoch 75/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9945 - loss: 0.0168 - val_accuracy: 0.9919 - val_loss: 0.0155\n",
      "Epoch 76/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9947 - loss: 0.0190 - val_accuracy: 0.9919 - val_loss: 0.0216\n",
      "Epoch 77/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9930 - loss: 0.0183 - val_accuracy: 0.9919 - val_loss: 0.0168\n",
      "Epoch 78/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9955 - loss: 0.0152 - val_accuracy: 0.9959 - val_loss: 0.0165\n",
      "Epoch 79/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9979 - loss: 0.0136 - val_accuracy: 0.9919 - val_loss: 0.0177\n",
      "Epoch 80/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9932 - loss: 0.0187 - val_accuracy: 0.9919 - val_loss: 0.0233\n",
      "Epoch 81/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9919 - loss: 0.0165 - val_accuracy: 0.9959 - val_loss: 0.0163\n",
      "Epoch 82/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9957 - loss: 0.0140 - val_accuracy: 0.9959 - val_loss: 0.0105\n",
      "Epoch 83/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9953 - loss: 0.0161 - val_accuracy: 0.9959 - val_loss: 0.0093\n",
      "Epoch 84/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9925 - loss: 0.0156 - val_accuracy: 0.9959 - val_loss: 0.0110\n",
      "Epoch 85/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9963 - loss: 0.0142 - val_accuracy: 0.9919 - val_loss: 0.0177\n",
      "Epoch 86/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9962 - loss: 0.0186 - val_accuracy: 0.9959 - val_loss: 0.0092\n",
      "Epoch 87/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9923 - loss: 0.0176 - val_accuracy: 0.9959 - val_loss: 0.0201\n",
      "Epoch 88/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9972 - loss: 0.0106 - val_accuracy: 0.9959 - val_loss: 0.0167\n",
      "Epoch 89/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9953 - loss: 0.0161 - val_accuracy: 0.9959 - val_loss: 0.0188\n",
      "Epoch 90/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9931 - loss: 0.0185 - val_accuracy: 0.9919 - val_loss: 0.0243\n",
      "Epoch 91/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9960 - loss: 0.0130 - val_accuracy: 0.9959 - val_loss: 0.0172\n",
      "Epoch 92/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9973 - loss: 0.0142 - val_accuracy: 0.9959 - val_loss: 0.0231\n",
      "Epoch 93/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9994 - loss: 0.0073 - val_accuracy: 0.9959 - val_loss: 0.0178\n",
      "Epoch 94/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9975 - loss: 0.0088 - val_accuracy: 0.9959 - val_loss: 0.0181\n",
      "Epoch 95/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9956 - loss: 0.0140 - val_accuracy: 0.9919 - val_loss: 0.0120\n",
      "Epoch 96/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9994 - loss: 0.0087 - val_accuracy: 0.9959 - val_loss: 0.0129\n",
      "Epoch 97/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9967 - loss: 0.0125 - val_accuracy: 0.9959 - val_loss: 0.0163\n",
      "Epoch 98/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9995 - loss: 0.0060 - val_accuracy: 0.9959 - val_loss: 0.0219\n",
      "Epoch 99/99\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9965 - loss: 0.0092 - val_accuracy: 0.9919 - val_loss: 0.0253\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Evaluation =====\n",
      "Accuracy: 0.9557\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.3077    0.4706        13\n",
      "           1     0.9579    0.9146    0.9357       199\n",
      "           2     0.9852    0.9815    0.9833       541\n",
      "           3     0.7500    1.0000    0.8571        60\n",
      "\n",
      "    accuracy                         0.9557       813\n",
      "   macro avg     0.9233    0.8009    0.8117       813\n",
      "weighted avg     0.9614    0.9557    0.9542       813\n",
      "\n",
      "Confusion Matrix:\n",
      " [[  4   8   0   1]\n",
      " [  0 182   8   9]\n",
      " [  0   0 531  10]\n",
      " [  0   0   0  60]]\n",
      "\n",
      "===== Model Metrics =====\n",
      "Training Time: 33.03s\n",
      "Inference/sample: 0.000425s\n",
      "Testing Time: 0.35s\n",
      "Model Size: 0.05 MB\n",
      "Trainable Params: 7,278\n",
      "RAM Usage: 1657.35 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import psutil\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from bayes_opt import BayesianOptimization\n",
    "from scipy.fftpack import fft\n",
    "import warnings\n",
    "\n",
    "# Set seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================\n",
    "# 1. Data Loading and Preprocessing\n",
    "# ============================================\n",
    "def load_and_preprocess_data(filepath):\n",
    "    df = pd.read_csv(filepath, delimiter=\",\", low_memory=False)\n",
    "    df.columns = df.columns.str.replace('\"', '').str.strip()\n",
    "\n",
    "    if 'TIME' not in df.columns:\n",
    "        raise KeyError(\"The column 'TIME' does not exist in the dataset.\")\n",
    "\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'], errors='coerce')\n",
    "    df.dropna(subset=['TIME'], inplace=True)\n",
    "    df.set_index('TIME', inplace=True)\n",
    "\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "    target_col = 'fenologia_h4'\n",
    "    if target_col not in df.columns:\n",
    "        raise KeyError(f\"The target column '{target_col}' does not exist.\")\n",
    "\n",
    "    df[target_col].interpolate(method='linear', inplace=True)\n",
    "    df[target_col] = df[target_col] - 1\n",
    "\n",
    "    for i in range(1, 4):\n",
    "        df[f'{target_col}_lag{i}'] = df[target_col].shift(i)\n",
    "\n",
    "    for window in [3, 6]:\n",
    "        df[f'{target_col}_roll_mean_{window}'] = df[target_col].rolling(window).mean()\n",
    "        df[f'{target_col}_roll_std_{window}'] = df[target_col].rolling(window).std()\n",
    "\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "    return df, target_col\n",
    "\n",
    "# ============================================\n",
    "# 2. Feature Scaling\n",
    "# ============================================\n",
    "def feature_scaling(df, target_col):\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))  # 3D\n",
    "    return X_scaled, y, scaler\n",
    "\n",
    "# ============================================\n",
    "# 3. CNN + Gate Attention Model\n",
    "# ============================================\n",
    "def gated_attention_block(inputs, units):\n",
    "    conv_output = layers.Conv1D(units, kernel_size=1, activation='relu')(inputs)\n",
    "    attention_scores = layers.Dense(units, activation='sigmoid')(conv_output)\n",
    "    gated_output = layers.Multiply()([conv_output, attention_scores])\n",
    "    return gated_output\n",
    "\n",
    "def build_cnn_gatt_model(input_shape, num_classes=4, num_units=64, dropout_rate=0.2):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = gated_attention_block(inputs, units=num_units)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    x = layers.Dense(num_units // 2, activation='relu')(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# ============================================\n",
    "# 4. Bayesian Optimization Objective\n",
    "# ============================================\n",
    "def objective_function(num_units, dropout_rate, epochs, batch_size, X_train, y_train):\n",
    "    model = build_cnn_gatt_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        num_classes=4,\n",
    "        num_units=int(num_units),\n",
    "        dropout_rate=dropout_rate\n",
    "    )\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=int(epochs),\n",
    "        batch_size=int(batch_size),\n",
    "        validation_split=0.2,\n",
    "        verbose=0\n",
    "    )\n",
    "    return np.max(history.history['val_accuracy'])\n",
    "\n",
    "# ============================================\n",
    "# 5. Optimize Hyperparameters\n",
    "# ============================================\n",
    "def optimize_hyperparameters(X_train, y_train):\n",
    "    pbounds = {\n",
    "        'num_units': (32, 128),\n",
    "        'dropout_rate': (0.1, 0.5),\n",
    "        'epochs': (50, 100),\n",
    "        'batch_size': (16, 64)\n",
    "    }\n",
    "\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=lambda num_units, dropout_rate, epochs, batch_size: objective_function(\n",
    "            num_units, dropout_rate, epochs, batch_size, X_train, y_train\n",
    "        ),\n",
    "        pbounds=pbounds,\n",
    "        random_state=42,\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    optimizer.maximize(init_points=5, n_iter=15)\n",
    "    return optimizer.max['params']\n",
    "\n",
    "# ============================================\n",
    "# 6. Train & Evaluate Model\n",
    "# ============================================\n",
    "def train_and_evaluate_model(X_train, X_test, y_train, y_test, best_params):\n",
    "    model = build_cnn_gatt_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        num_classes=4,\n",
    "        num_units=int(best_params['num_units']),\n",
    "        dropout_rate=float(best_params['dropout_rate'])\n",
    "    )\n",
    "\n",
    "    start_train = time.time()\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=int(best_params['epochs']),\n",
    "        batch_size=int(best_params['batch_size']),\n",
    "        validation_split=0.2,\n",
    "        verbose=1\n",
    "    )\n",
    "    training_time = time.time() - start_train\n",
    "\n",
    "    start_infer = time.time()\n",
    "    y_pred = model.predict(X_test).argmax(axis=1)\n",
    "    inference_time = (time.time() - start_infer) / len(X_test)\n",
    "    testing_time = time.time() - start_infer\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(\"\\n===== Evaluation =====\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(\"Report:\\n\", classification_report(y_test, y_pred, digits=4))\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "    model.save(\"temp_model.h5\", include_optimizer=False)\n",
    "    model_size = os.path.getsize(\"temp_model.h5\") / (1024 ** 2)\n",
    "    os.remove(\"temp_model.h5\")\n",
    "\n",
    "    total_params = np.sum([np.prod(v.shape) for v in model.trainable_weights])\n",
    "    ram_usage = psutil.Process(os.getpid()).memory_info().rss / (1024 ** 2)\n",
    "\n",
    "    print(\"\\n===== Model Metrics =====\")\n",
    "    print(f\"Training Time: {training_time:.2f}s\")\n",
    "    print(f\"Inference/sample: {inference_time:.6f}s\")\n",
    "    print(f\"Testing Time: {testing_time:.2f}s\")\n",
    "    print(f\"Model Size: {model_size:.2f} MB\")\n",
    "    print(f\"Trainable Params: {total_params:,}\")\n",
    "    print(f\"RAM Usage: {ram_usage:.2f} MB\")\n",
    "\n",
    "    return model, history\n",
    "\n",
    "# ============================================\n",
    "# 7. Main\n",
    "# ============================================\n",
    "def main():\n",
    "    train_path = r\"E:\\Abroad period research\\Phenology datasets\\PHENOLOGY_H4_train.csv\"\n",
    "    test_path = r\"E:\\Abroad period research\\Phenology datasets\\PHENOLOGY_H4_test.csv\"\n",
    "\n",
    "    df_train, target_col = load_and_preprocess_data(train_path)\n",
    "    X_train, y_train, _ = feature_scaling(df_train, target_col)\n",
    "\n",
    "    df_test, _ = load_and_preprocess_data(test_path)\n",
    "    X_test, y_test, _ = feature_scaling(df_test, target_col)\n",
    "\n",
    "    print(\"\\n Optimizing Hyperparameters...\")\n",
    "    best_params = optimize_hyperparameters(X_train, y_train)\n",
    "    print(\"\\n Best Params:\", best_params)\n",
    "\n",
    "    print(\"\\n Training Final Gate Attention CNN Model...\")\n",
    "    model, history = train_and_evaluate_model(X_train, X_test, y_train, y_test, best_params)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
