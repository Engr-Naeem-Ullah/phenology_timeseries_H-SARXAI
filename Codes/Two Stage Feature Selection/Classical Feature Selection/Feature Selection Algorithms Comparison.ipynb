{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991697e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "#  IMPORTS\n",
    "# ===========================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_selection import mutual_info_classif, RFE, VarianceThreshold\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# ===========================================\n",
    "#  DATA LOADING + PREPROCESSING\n",
    "# ===========================================\n",
    "def load_and_preprocess_data(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df.columns = df.columns.str.replace('\"', '').str.strip()\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'], errors='coerce')\n",
    "    df.dropna(subset=['TIME'], inplace=True)\n",
    "    df.set_index('TIME', inplace=True)\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "    target_col = 'fenologia_h1'\n",
    "    df[target_col].interpolate(method='linear', inplace=True)\n",
    "    df[target_col] = df[target_col] - 1\n",
    "\n",
    "    for i in range(1, 4):\n",
    "        df[f'{target_col}_lag{i}'] = df[target_col].shift(i)\n",
    "\n",
    "    for window in [3, 6]:\n",
    "        df[f'{target_col}_roll_mean_{window}'] = df[target_col].rolling(window).mean()\n",
    "        df[f'{target_col}_roll_std_{window}'] = df[target_col].rolling(window).std()\n",
    "\n",
    "    df['month'] = df.index.month\n",
    "    df['weekofyear'] = df.index.isocalendar().week\n",
    "    df['year'] = df.index.isocalendar().year\n",
    "    df['sin_week'] = np.sin(2 * np.pi * df['weekofyear'] / 52)\n",
    "    df['cos_week'] = np.cos(2 * np.pi * df['weekofyear'] / 52)\n",
    "\n",
    "    df['EMA_3'] = df[target_col].ewm(span=3, adjust=False).mean()\n",
    "    df['EMA_6'] = df[target_col].ewm(span=6, adjust=False).mean()\n",
    "\n",
    "    df['correlation_target_month'] = df[target_col].rolling(window=6).corr(df['month'])\n",
    "    df['correlation_target_week'] = df[target_col].rolling(window=6).corr(df['weekofyear'])\n",
    "\n",
    "    fft_values = np.fft.fft(df[target_col].dropna().values)\n",
    "    fft_real = np.real(fft_values)[:len(df[target_col])]\n",
    "    fft_imag = np.imag(fft_values)[:len(df[target_col])]\n",
    "    df['fft_real'] = np.concatenate([fft_real, np.nan * np.ones(len(df) - len(fft_real))])\n",
    "    df['fft_imag'] = np.concatenate([fft_imag, np.nan * np.ones(len(df) - len(fft_imag))])\n",
    "\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "    return df, target_col\n",
    "\n",
    "# ===========================================\n",
    "#  SCALING\n",
    "# ===========================================\n",
    "def feature_scaling(df, target_col):\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    return pd.DataFrame(X_scaled, columns=X.columns), y, scaler\n",
    "\n",
    "# ===========================================\n",
    "#  MODEL\n",
    "# ===========================================\n",
    "def build_mlp_model(input_dim, num_classes=4, num_units=64, dropout_rate=0.2, activation='relu'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_units, activation=activation, input_dim=input_dim))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(num_units // 2, activation=activation))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# ===========================================\n",
    "#  FEATURE SELECTION\n",
    "# ===========================================\n",
    "def mutual_information_feature_selection(X, y):\n",
    "    scores = mutual_info_classif(X, y)\n",
    "    return scores > 0.01\n",
    "\n",
    "def pearson_correlation_feature_selection(X, y):\n",
    "    scores = np.array([abs(pearsonr(X[col], y)[0]) for col in X.columns])\n",
    "    return scores > 0.1\n",
    "\n",
    "def variance_threshold_feature_selection(X, threshold=0.01):\n",
    "    selector = VarianceThreshold(threshold)\n",
    "    selector.fit(X)\n",
    "    return selector.get_support()\n",
    "\n",
    "def rfe_feature_selection(X, y, n_features_to_select=10):\n",
    "    rfe = RFE(SVC(kernel=\"linear\"), n_features_to_select=n_features_to_select)\n",
    "    rfe.fit(X, y)\n",
    "    return rfe.support_\n",
    "\n",
    "def pso_feature_selection(X, y):\n",
    "    def fitness_function(features):\n",
    "        mask = features > 0.5\n",
    "        if mask.sum() == 0:\n",
    "            return 1.0\n",
    "        clf = RandomForestClassifier()\n",
    "        clf.fit(X.iloc[:, mask], y)\n",
    "        acc = clf.score(X.iloc[:, mask], y)\n",
    "        return 1 - acc\n",
    "\n",
    "    lb = np.zeros(X.shape[1])\n",
    "    ub = np.ones(X.shape[1])\n",
    "    best_pos, _ = pso(fitness_function, lb, ub, swarmsize=10, maxiter=3)\n",
    "    return best_pos > 0.5\n",
    "\n",
    "def lasso_feature_selection(X, y):\n",
    "    model = Lasso(alpha=0.01)\n",
    "    model.fit(X, y)\n",
    "    return np.abs(model.coef_) > 0.001\n",
    "\n",
    "def tree_based_feature_selection(X, y):\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(X, y)\n",
    "    return model.feature_importances_ > np.mean(model.feature_importances_)\n",
    "\n",
    "# ===========================================\n",
    "#  TRAINING AND EVALUATION\n",
    "# ===========================================\n",
    "def train_and_evaluate_model(X_train, X_test, y_train, y_test, selected_features):\n",
    "    selected_columns = X_train.columns[selected_features]\n",
    "    X_train_sel = X_train[selected_columns]\n",
    "    X_test_sel = X_test[selected_columns]\n",
    "\n",
    "    model = build_mlp_model(input_dim=X_train_sel.shape[1])\n",
    "    model.fit(X_train_sel, y_train, epochs=30, batch_size=32, validation_split=0.2, verbose=0)\n",
    "    y_pred = model.predict(X_test_sel).argmax(axis=1)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"\\n Accuracy: {acc:.4f}\")\n",
    "    print(\" Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    print(\" Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    return acc, model\n",
    "\n",
    "# ===========================================\n",
    "#  MAIN\n",
    "# ===========================================\n",
    "def main():\n",
    "    train_fp = r\"E:\\\\Abroad period research\\\\Phenology datasets\\\\PHENOLOGY_H1\\\\Cadiz_train.csv\"\n",
    "    test_fp  = r\"E:\\\\Abroad period research\\\\Phenology datasets\\\\PHENOLOGY_H1\\\\Cadiz_test.csv\"\n",
    "\n",
    "    df_train, target_col = load_and_preprocess_data(train_fp)\n",
    "    X_train, y_train, scaler = feature_scaling(df_train, target_col)\n",
    "\n",
    "    df_test, _ = load_and_preprocess_data(test_fp)\n",
    "    X_test, y_test, _ = feature_scaling(df_test, target_col)\n",
    "\n",
    "    print(\" Mutual Info...\")\n",
    "    acc1, _ = train_and_evaluate_model(X_train, X_test, y_train, y_test, mutual_information_feature_selection(X_train, y_train))\n",
    "\n",
    "    print(\" Pearson Corr...\")\n",
    "    acc2, _ = train_and_evaluate_model(X_train, X_test, y_train, y_test, pearson_correlation_feature_selection(X_train, y_train))\n",
    "\n",
    "    print(\" Variance Threshold...\")\n",
    "    acc3, _ = train_and_evaluate_model(X_train, X_test, y_train, y_test, variance_threshold_feature_selection(X_train))\n",
    "\n",
    "    print(\" RFE...\")\n",
    "    acc4, _ = train_and_evaluate_model(X_train, X_test, y_train, y_test, rfe_feature_selection(X_train, y_train))\n",
    "\n",
    "    print(\" PSO...\")\n",
    "    acc5, _ = train_and_evaluate_model(X_train, X_test, y_train, y_test, pso_feature_selection(X_train, y_train))\n",
    "\n",
    "    print(\" Lasso...\")\n",
    "    acc6, _ = train_and_evaluate_model(X_train, X_test, y_train, y_test, lasso_feature_selection(X_train, y_train))\n",
    "\n",
    "    print(\" Tree-Based...\")\n",
    "    acc7, _ = train_and_evaluate_model(X_train, X_test, y_train, y_test, tree_based_feature_selection(X_train, y_train))\n",
    "\n",
    "    print(\"\\n FINAL COMPARISON:\")\n",
    "    for name, acc in zip([\"MI\", \"Pearson\", \"Variance\", \"RFE\", \"PSO\", \"Lasso\", \"Tree\"],\n",
    "                         [acc1, acc2, acc3, acc4, acc5, acc6, acc7]):\n",
    "        print(f\"{name:10s}: {acc:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54319753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "#  IMPORTS\n",
    "# ===========================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_selection import mutual_info_classif, RFE, VarianceThreshold, SelectKBest, f_classif\n",
    "from sklearn.linear_model import Lasso, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# ===========================================\n",
    "#  DATA LOADING + PREPROCESSING\n",
    "# ===========================================\n",
    "def load_and_preprocess_data(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df.columns = df.columns.str.replace('\"', '').str.strip()\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'], errors='coerce')\n",
    "    df.dropna(subset=['TIME'], inplace=True)\n",
    "    df.set_index('TIME', inplace=True)\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "    target_col = 'fenologia_h1'\n",
    "    df[target_col].interpolate(method='linear', inplace=True)\n",
    "    df[target_col] = df[target_col] - 1\n",
    "\n",
    "    for i in range(1, 4):\n",
    "        df[f'{target_col}_lag{i}'] = df[target_col].shift(i)\n",
    "\n",
    "    for window in [3, 6]:\n",
    "        df[f'{target_col}_roll_mean_{window}'] = df[target_col].rolling(window).mean()\n",
    "        df[f'{target_col}_roll_std_{window}'] = df[target_col].rolling(window).std()\n",
    "\n",
    "    df['month'] = df.index.month\n",
    "    df['weekofyear'] = df.index.isocalendar().week\n",
    "    df['year'] = df.index.isocalendar().year\n",
    "    df['sin_week'] = np.sin(2 * np.pi * df['weekofyear'] / 52)\n",
    "    df['cos_week'] = np.cos(2 * np.pi * df['weekofyear'] / 52)\n",
    "\n",
    "    df['EMA_3'] = df[target_col].ewm(span=3, adjust=False).mean()\n",
    "    df['EMA_6'] = df[target_col].ewm(span=6, adjust=False).mean()\n",
    "\n",
    "    df['correlation_target_month'] = df[target_col].rolling(window=6).corr(df['month'])\n",
    "    df['correlation_target_week'] = df[target_col].rolling(window=6).corr(df['weekofyear'])\n",
    "\n",
    "    fft_values = np.fft.fft(df[target_col].dropna().values)\n",
    "    fft_real = np.real(fft_values)[:len(df[target_col])]\n",
    "    fft_imag = np.imag(fft_values)[:len(df[target_col])]\n",
    "    df['fft_real'] = np.concatenate([fft_real, np.nan * np.ones(len(df) - len(fft_real))])\n",
    "    df['fft_imag'] = np.concatenate([fft_imag, np.nan * np.ones(len(df) - len(fft_imag))])\n",
    "\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "    return df, target_col\n",
    "\n",
    "# ===========================================\n",
    "#  SCALING\n",
    "# ===========================================\n",
    "def feature_scaling(df, target_col):\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    return pd.DataFrame(X_scaled, columns=X.columns), y, scaler\n",
    "\n",
    "# ===========================================\n",
    "#  MODEL\n",
    "# ===========================================\n",
    "def build_mlp_model(input_dim, num_classes=4, num_units=64, dropout_rate=0.2, activation='relu'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_units, activation=activation, input_dim=input_dim))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(num_units // 2, activation=activation))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# ===========================================\n",
    "#  FEATURE SELECTION\n",
    "# ===========================================\n",
    "def mutual_information_feature_selection(X, y):\n",
    "    scores = mutual_info_classif(X, y)\n",
    "    return scores > 0.01\n",
    "\n",
    "def pearson_correlation_feature_selection(X, y):\n",
    "    scores = np.array([abs(pearsonr(X[col], y)[0]) for col in X.columns])\n",
    "    return scores > 0.1\n",
    "\n",
    "def variance_threshold_feature_selection(X, threshold=0.01):\n",
    "    selector = VarianceThreshold(threshold)\n",
    "    selector.fit(X)\n",
    "    return selector.get_support()\n",
    "\n",
    "def rfe_feature_selection(X, y, n_features_to_select=10):\n",
    "    rfe = RFE(SVC(kernel=\"linear\"), n_features_to_select=n_features_to_select)\n",
    "    rfe.fit(X, y)\n",
    "    return rfe.support_\n",
    "\n",
    "def logistic_regression_wrapper_selection(X, y):\n",
    "    model = LogisticRegression(solver='liblinear')\n",
    "    rfe = RFE(model, n_features_to_select=10)\n",
    "    rfe.fit(X, y)\n",
    "    return rfe.support_\n",
    "\n",
    "def f_classif_filter_selection(X, y):\n",
    "    selector = SelectKBest(score_func=f_classif, k=10)\n",
    "    selector.fit(X, y)\n",
    "    return selector.get_support()\n",
    "\n",
    "def lasso_feature_selection(X, y):\n",
    "    model = Lasso(alpha=0.01)\n",
    "    model.fit(X, y)\n",
    "    return np.abs(model.coef_) > 0.001\n",
    "\n",
    "def tree_based_feature_selection(X, y):\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(X, y)\n",
    "    return model.feature_importances_ > np.mean(model.feature_importances_)\n",
    "\n",
    "# ===========================================\n",
    "#  TRAINING AND EVALUATION\n",
    "# ===========================================\n",
    "def train_and_evaluate_model(X_train, X_test, y_train, y_test, selected_features):\n",
    "    selected_columns = X_train.columns[selected_features]\n",
    "    X_train_sel = X_train[selected_columns]\n",
    "    X_test_sel = X_test[selected_columns]\n",
    "\n",
    "    model = build_mlp_model(input_dim=X_train_sel.shape[1])\n",
    "    model.fit(X_train_sel, y_train, epochs=30, batch_size=32, validation_split=0.2, verbose=0)\n",
    "    y_pred = model.predict(X_test_sel).argmax(axis=1)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"\\n Accuracy: {acc:.4f}\")\n",
    "    print(\" Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    print(\" Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    return acc, model\n",
    "\n",
    "# ===========================================\n",
    "#  MAIN\n",
    "# ===========================================\n",
    "def main():\n",
    "    train_fp = r\"E:\\\\Abroad period research\\\\Phenology datasets\\\\PHENOLOGY_H1\\\\Cadiz_train.csv\"\n",
    "    test_fp  = r\"E:\\\\Abroad period research\\\\Phenology datasets\\\\PHENOLOGY_H1\\\\Cadiz_test.csv\"\n",
    "\n",
    "    df_train, target_col = load_and_preprocess_data(train_fp)\n",
    "    X_train, y_train, scaler = feature_scaling(df_train, target_col)\n",
    "\n",
    "    df_test, _ = load_and_preprocess_data(test_fp)\n",
    "    X_test, y_test, _ = feature_scaling(df_test, target_col)\n",
    "\n",
    "    print(\" Mutual Info (Filter)...\")\n",
    "    acc1, _ = train_and_evaluate_model(X_train, X_test, y_train, y_test, mutual_information_feature_selection(X_train, y_train))\n",
    "\n",
    "    print(\" F-ANOVA (Filter)...\")\n",
    "    acc2, _ = train_and_evaluate_model(X_train, X_test, y_train, y_test, f_classif_filter_selection(X_train, y_train))\n",
    "\n",
    "    print(\" RFE SVC (Wrapper)...\")\n",
    "    acc3, _ = train_and_evaluate_model(X_train, X_test, y_train, y_test, rfe_feature_selection(X_train, y_train))\n",
    "\n",
    "    print(\" Logistic Regression Wrapper...\")\n",
    "    acc4, _ = train_and_evaluate_model(X_train, X_test, y_train, y_test, logistic_regression_wrapper_selection(X_train, y_train))\n",
    "\n",
    "    print(\" Lasso (Embedded)...\")\n",
    "    acc5, _ = train_and_evaluate_model(X_train, X_test, y_train, y_test, lasso_feature_selection(X_train, y_train))\n",
    "\n",
    "    print(\" Tree-Based (Embedded)...\")\n",
    "    acc6, _ = train_and_evaluate_model(X_train, X_test, y_train, y_test, tree_based_feature_selection(X_train, y_train))\n",
    "\n",
    "    print(\"\\n FINAL COMPARISON:\")\n",
    "    for name, acc in zip([\"MI\", \"F-Classif\", \"RFE\", \"LogRegWrapper\", \"Lasso\", \"Tree\"],\n",
    "                         [acc1, acc2, acc3, acc4, acc5, acc6]):\n",
    "        print(f\"{name:15s}: {acc:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b293173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "#  IMPORTS\n",
    "# ===========================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_selection import mutual_info_classif, RFE, VarianceThreshold, SelectKBest, f_classif\n",
    "from sklearn.linear_model import Lasso, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_selection import (mutual_info_classif, RFE, VarianceThreshold,\n",
    "                                       SelectFromModel, f_classif)\n",
    "from sklearn.base import clone\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# ===========================================\n",
    "#  DATA LOADING + PREPROCESSING\n",
    "# ===========================================\n",
    "def load_and_preprocess_data(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df.columns = df.columns.str.replace('\"', '').str.strip()\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'], errors='coerce')\n",
    "    df.dropna(subset=['TIME'], inplace=True)\n",
    "    df.set_index('TIME', inplace=True)\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "    target_col = 'fenologia_h1'\n",
    "    df[target_col].interpolate(method='linear', inplace=True)\n",
    "    df[target_col] = df[target_col] - 1\n",
    "\n",
    "    for i in range(1, 4):\n",
    "        df[f'{target_col}_lag{i}'] = df[target_col].shift(i)\n",
    "\n",
    "    for window in [3, 6]:\n",
    "        df[f'{target_col}_roll_mean_{window}'] = df[target_col].rolling(window).mean()\n",
    "        df[f'{target_col}_roll_std_{window}'] = df[target_col].rolling(window).std()\n",
    "\n",
    "    df['month'] = df.index.month\n",
    "    df['weekofyear'] = df.index.isocalendar().week\n",
    "    df['year'] = df.index.isocalendar().year\n",
    "    df['sin_week'] = np.sin(2 * np.pi * df['weekofyear'] / 52)\n",
    "    df['cos_week'] = np.cos(2 * np.pi * df['weekofyear'] / 52)\n",
    "\n",
    "    df['EMA_3'] = df[target_col].ewm(span=3, adjust=False).mean()\n",
    "    df['EMA_6'] = df[target_col].ewm(span=6, adjust=False).mean()\n",
    "\n",
    "    df['correlation_target_month'] = df[target_col].rolling(window=6).corr(df['month'])\n",
    "    df['correlation_target_week'] = df[target_col].rolling(window=6).corr(df['weekofyear'])\n",
    "\n",
    "    fft_values = np.fft.fft(df[target_col].dropna().values)\n",
    "    fft_real = np.real(fft_values)[:len(df[target_col])]\n",
    "    fft_imag = np.imag(fft_values)[:len(df[target_col])]\n",
    "    df['fft_real'] = np.concatenate([fft_real, np.nan * np.ones(len(df) - len(fft_real))])\n",
    "    df['fft_imag'] = np.concatenate([fft_imag, np.nan * np.ones(len(df) - len(fft_imag))])\n",
    "\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "    return df, target_col\n",
    "\n",
    "# ===========================================\n",
    "#  SCALING\n",
    "# ===========================================\n",
    "def feature_scaling(df, target_col):\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    return pd.DataFrame(X_scaled, columns=X.columns), y, scaler\n",
    "\n",
    "# ===========================================\n",
    "#  MODEL\n",
    "# ===========================================\n",
    "def build_mlp_model(input_dim, num_classes=4, num_units=64, dropout_rate=0.2, activation='relu'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_units, activation=activation, input_dim=input_dim))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(num_units // 2, activation=activation))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# ===========================================\n",
    "#  FEATURE SELECTION\n",
    "# ===========================================\n",
    "def mutual_information_feature_selection(X, y):\n",
    "    scores = mutual_info_classif(X, y)\n",
    "    return scores > 0.01\n",
    "\n",
    "def pearson_correlation_feature_selection(X, y):\n",
    "    scores = np.array([abs(pearsonr(X[col], y)[0]) for col in X.columns])\n",
    "    return scores > 0.1\n",
    "\n",
    "def variance_threshold_feature_selection(X, threshold=0.01):\n",
    "    selector = VarianceThreshold(threshold)\n",
    "    selector.fit(X)\n",
    "    return selector.get_support()\n",
    "\n",
    "def rfe_feature_selection(X, y, n_features_to_select=10):\n",
    "    rfe = RFE(SVC(kernel=\"linear\"), n_features_to_select=n_features_to_select)\n",
    "    rfe.fit(X, y)\n",
    "    return rfe.support_\n",
    "\n",
    "def logistic_regression_wrapper_selection(X, y):\n",
    "    model = LogisticRegression(solver='liblinear')\n",
    "    rfe = RFE(model, n_features_to_select=10)\n",
    "    rfe.fit(X, y)\n",
    "    return rfe.support_\n",
    "\n",
    "def f_classif_filter_selection(X, y):\n",
    "    selector = SelectKBest(score_func=f_classif, k=10)\n",
    "    selector.fit(X, y)\n",
    "    return selector.get_support()\n",
    "\n",
    "def lasso_feature_selection(X, y):\n",
    "    model = Lasso(alpha=0.01)\n",
    "    model.fit(X, y)\n",
    "    return np.abs(model.coef_) > 0.001\n",
    "\n",
    "def tree_based_feature_selection(X, y):\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(X, y)\n",
    "    return model.feature_importances_ > np.mean(model.feature_importances_)\n",
    "\n",
    "def variance_threshold_feature_selection(X, threshold=0.01):\n",
    "    selector = VarianceThreshold(threshold)\n",
    "    selector.fit(X)\n",
    "    return selector.get_support()\n",
    "\n",
    "#  New Filter: Variance Threshold\n",
    "\n",
    "#  New Wrapper: Forward Selection (simple custom greedy implementation)\n",
    "def forward_selection(X, y, n_features=10):\n",
    "    from sklearn.base import clone\n",
    "    model = RandomForestClassifier()\n",
    "    selected = []\n",
    "    remaining = list(X.columns)\n",
    "    while len(selected) < n_features:\n",
    "        scores = []\n",
    "        for feature in remaining:\n",
    "            temp_features = selected + [feature]\n",
    "            model_clone = clone(model)\n",
    "            model_clone.fit(X[temp_features], y)\n",
    "            acc = model_clone.score(X[temp_features], y)\n",
    "            scores.append((acc, feature))\n",
    "        scores.sort(reverse=True)\n",
    "        best_feature = scores[0][1]\n",
    "        selected.append(best_feature)\n",
    "        remaining.remove(best_feature)\n",
    "    return X.columns.isin(selected)\n",
    "\n",
    "#  New Embedded: Linear SVM with L1 penalty\n",
    "\n",
    "def linear_svc_l1_feature_selection(X, y):\n",
    "    svc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(X, y)\n",
    "    selector = SelectFromModel(svc, prefit=True)\n",
    "    return selector.get_support()\n",
    "\n",
    "\n",
    "# ===========================================\n",
    "#  TRAINING AND EVALUATION\n",
    "# ===========================================\n",
    "def train_and_evaluate_model(X_train, X_test, y_train, y_test, selected_features):\n",
    "    selected_columns = X_train.columns[selected_features]\n",
    "    X_train_sel = X_train[selected_columns]\n",
    "    X_test_sel = X_test[selected_columns]\n",
    "\n",
    "    model = build_mlp_model(input_dim=X_train_sel.shape[1])\n",
    "    model.fit(X_train_sel, y_train, epochs=30, batch_size=32, validation_split=0.2, verbose=0)\n",
    "    y_pred = model.predict(X_test_sel).argmax(axis=1)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"\\n Accuracy: {acc:.4f}\")\n",
    "    print(\" Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    print(\" Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    return acc, model\n",
    "\n",
    "# ===========================================\n",
    "#  MAIN\n",
    "# ===========================================\n",
    "def main():\n",
    "    train_fp = r\"E:\\\\Abroad period research\\\\Phenology datasets\\\\PHENOLOGY_H1\\\\Cadiz_train.csv\"\n",
    "    test_fp  = r\"E:\\\\Abroad period research\\\\Phenology datasets\\\\PHENOLOGY_H1\\\\Cadiz_test.csv\"\n",
    "\n",
    "    df_train, target_col = load_and_preprocess_data(train_fp)\n",
    "    X_train, y_train, scaler = feature_scaling(df_train, target_col)\n",
    "\n",
    "    df_test, _ = load_and_preprocess_data(test_fp)\n",
    "    X_test, y_test, _ = feature_scaling(df_test, target_col)\n",
    "\n",
    "    print(\" Mutual Info (Filter)...\")\n",
    "    acc1, _ = train_and_evaluate_model(X_train, X_test, y_train, y_test, mutual_information_feature_selection(X_train, y_train))\n",
    "\n",
    "    print(\" F-ANOVA (Filter)...\")\n",
    "    acc2, _ = train_and_evaluate_model(X_train, X_test, y_train, y_test, f_classif_filter_selection(X_train, y_train))\n",
    "\n",
    "    print(\" Variance Threshold (Filter)...\")\n",
    "    acc3, _ = train_and_evaluate_model(X_train, X_test, y_train, y_test, variance_threshold_feature_selection(X_train))\n",
    "\n",
    "    print(\" RFE SVC (Wrapper)...\")\n",
    "    acc4, _ = train_and_evaluate_model(X_train, X_test, y_train, y_test, rfe_feature_selection(X_train, y_train))\n",
    "\n",
    "    print(\" Logistic Regression Wrapper...\")\n",
    "    acc5, _ = train_and_evaluate_model(X_train, X_test, y_train, y_test, logistic_regression_wrapper_selection(X_train, y_train))\n",
    "\n",
    "    print(\" Forward Selection (Wrapper)...\")\n",
    "    acc6, _ = train_and_evaluate_model(X_train, X_test, y_train, y_test, forward_selection(X_train, y_train))\n",
    "\n",
    "    print(\" Lasso (Embedded)...\")\n",
    "    acc7, _ = train_and_evaluate_model(X_train, X_test, y_train, y_test, lasso_feature_selection(X_train, y_train))\n",
    "\n",
    "    print(\" Tree-Based (Embedded)...\")\n",
    "    acc8, _ = train_and_evaluate_model(X_train, X_test, y_train, y_test, tree_based_feature_selection(X_train, y_train))\n",
    "\n",
    "    print(\" Linear SVC L1 (Embedded)...\")\n",
    "    acc9, _ = train_and_evaluate_model(X_train, X_test, y_train, y_test, linear_svc_l1_feature_selection(X_train, y_train))\n",
    "\n",
    "    print(\"\\n FINAL COMPARISON:\")\n",
    "    methods = [\"MI\", \"F-Classif\", \"Variance\", \"RFE\", \"LogRegWrapper\", \"ForwardSel\", \"Lasso\", \"Tree\", \"LinearSVC-L1\"]\n",
    "    accuracies = [acc1, acc2, acc3, acc4, acc5, acc6, acc7, acc8, acc9]\n",
    "    for name, acc in zip(methods, accuracies):\n",
    "        print(f\"{name:15s}: {acc:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cef1db",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9acc9c2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13510ae4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3887f0f",
   "metadata": {},
   "source": [
    "Feature selection algorithms with proper results for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4aa087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_selection import mutual_info_classif, RFE, VarianceThreshold\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import pearsonr\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Dummy PSO (replace with your real PSO function)\n",
    "def pso(fitness_func, lb, ub, swarmsize=10, maxiter=3):\n",
    "    dim = len(lb)\n",
    "    best = np.random.rand(dim)\n",
    "    return best, 0.5\n",
    "\n",
    "# ======================\n",
    "# Load & Preprocess\n",
    "# ======================\n",
    "def load_and_preprocess_data(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df.columns = df.columns.str.replace('\"', '').str.strip()\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'], errors='coerce')\n",
    "    df.dropna(subset=['TIME'], inplace=True)\n",
    "    df.set_index('TIME', inplace=True)\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "    target_col = 'fenologia_h1'\n",
    "    df[target_col].interpolate(method='linear', inplace=True)\n",
    "    df[target_col] = df[target_col] - 1\n",
    "\n",
    "    for i in range(1, 4):\n",
    "        df[f'{target_col}_lag{i}'] = df[target_col].shift(i)\n",
    "\n",
    "    for window in [3, 6]:\n",
    "        df[f'{target_col}_roll_mean_{window}'] = df[target_col].rolling(window).mean()\n",
    "        df[f'{target_col}_roll_std_{window}'] = df[target_col].rolling(window).std()\n",
    "\n",
    "    df['month'] = df.index.month\n",
    "    df['weekofyear'] = df.index.isocalendar().week\n",
    "    df['year'] = df.index.isocalendar().year\n",
    "    df['sin_week'] = np.sin(2 * np.pi * df['weekofyear'] / 52)\n",
    "    df['cos_week'] = np.cos(2 * np.pi * df['weekofyear'] / 52)\n",
    "\n",
    "    df['EMA_3'] = df[target_col].ewm(span=3, adjust=False).mean()\n",
    "    df['EMA_6'] = df[target_col].ewm(span=6, adjust=False).mean()\n",
    "    df['correlation_target_month'] = df[target_col].rolling(window=6).corr(df['month'])\n",
    "    df['correlation_target_week'] = df[target_col].rolling(window=6).corr(df['weekofyear'])\n",
    "\n",
    "    fft_values = np.fft.fft(df[target_col].dropna().values)\n",
    "    fft_real = np.real(fft_values)[:len(df[target_col])]\n",
    "    fft_imag = np.imag(fft_values)[:len(df[target_col])]\n",
    "    df['fft_real'] = np.concatenate([fft_real, np.nan * np.ones(len(df) - len(fft_real))])\n",
    "    df['fft_imag'] = np.concatenate([fft_imag, np.nan * np.ones(len(df) - len(fft_imag))])\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "    return df, target_col\n",
    "\n",
    "# ======================\n",
    "# Scaling\n",
    "# ======================\n",
    "def feature_scaling(df, target_col):\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    return pd.DataFrame(X_scaled, columns=X.columns), y\n",
    "\n",
    "# ======================\n",
    "# MLP Model\n",
    "# ======================\n",
    "def build_mlp_model(input_dim, num_classes=4, num_units=64, dropout_rate=0.2, activation='relu'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_units, activation=activation, input_dim=input_dim))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(num_units // 2, activation=activation))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# ======================\n",
    "# Feature Selection Methods\n",
    "# ======================\n",
    "def mutual_information_feature_selection(X, y):\n",
    "    scores = mutual_info_classif(X, y)\n",
    "    return scores > 0.01\n",
    "\n",
    "def pearson_correlation_feature_selection(X, y):\n",
    "    scores = np.array([abs(pearsonr(X[col], y)[0]) for col in X.columns])\n",
    "    return scores > 0.1\n",
    "\n",
    "def variance_threshold_feature_selection(X):\n",
    "    selector = VarianceThreshold(threshold=0.01)\n",
    "    selector.fit(X)\n",
    "    return selector.get_support()\n",
    "\n",
    "def rfe_feature_selection(X, y, n_features_to_select=10):\n",
    "    rfe = RFE(SVC(kernel=\"linear\"), n_features_to_select=n_features_to_select)\n",
    "    rfe.fit(X, y)\n",
    "    return rfe.support_\n",
    "\n",
    "def pso_feature_selection(X, y):\n",
    "    def fitness_function(features):\n",
    "        mask = features > 0.5\n",
    "        if mask.sum() == 0:\n",
    "            return 1.0\n",
    "        clf = RandomForestClassifier()\n",
    "        clf.fit(X.iloc[:, mask], y)\n",
    "        acc = clf.score(X.iloc[:, mask], y)\n",
    "        return 1 - acc\n",
    "\n",
    "    lb = np.zeros(X.shape[1])\n",
    "    ub = np.ones(X.shape[1])\n",
    "    best_pos, _ = pso(fitness_function, lb, ub)\n",
    "    return best_pos > 0.5\n",
    "\n",
    "def lasso_feature_selection(X, y):\n",
    "    model = Lasso(alpha=0.01)\n",
    "    model.fit(X, y)\n",
    "    return np.abs(model.coef_) > 0.001\n",
    "\n",
    "def tree_based_feature_selection(X, y):\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(X, y)\n",
    "    return model.feature_importances_ > np.mean(model.feature_importances_)\n",
    "\n",
    "# ======================\n",
    "# Train & Evaluate\n",
    "# ======================\n",
    "def train_and_evaluate(X_train, X_test, y_train, y_test, selector_func, method_name):\n",
    "    start_fs = time.time()\n",
    "    if method_name == \"Variance\":\n",
    "        mask = selector_func(X_train)\n",
    "    else:\n",
    "        mask = selector_func(X_train, y_train)\n",
    "    fs_time = time.time() - start_fs\n",
    "\n",
    "    selected_cols = X_train.columns[mask]\n",
    "    X_train_sel = X_train[selected_cols]\n",
    "    X_test_sel = X_test[selected_cols]\n",
    "\n",
    "    start_train = time.time()\n",
    "    model = build_mlp_model(input_dim=X_train_sel.shape[1])\n",
    "    model.fit(X_train_sel, y_train, epochs=30, batch_size=32, validation_split=0.2, verbose=0)\n",
    "    train_time = time.time() - start_train\n",
    "\n",
    "    start_test = time.time()\n",
    "    y_pred = model.predict(X_test_sel).argmax(axis=1)\n",
    "    test_time = time.time() - start_test\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    n_features = sum(mask)\n",
    "    reduction_pct = 100 * (1 - n_features / X_train.shape[1])\n",
    "\n",
    "    return {\n",
    "        \"Method\": method_name,\n",
    "        \"Accuracy\": round(acc, 4),\n",
    "        \"Precision\": round(prec, 4),\n",
    "        \"Recall\": round(rec, 4),\n",
    "        \"F1 Score\": round(f1, 4),\n",
    "        \"#Features\": n_features,\n",
    "        \"Feature Reduction (%)\": round(reduction_pct, 2),\n",
    "        \"FS Time (s)\": round(fs_time, 2),\n",
    "        \"Train Time (s)\": round(train_time, 2),\n",
    "        \"Test Time (s)\": round(test_time, 2)\n",
    "    }\n",
    "\n",
    "# ======================\n",
    "# Main\n",
    "# ======================\n",
    "def main():\n",
    "    train_fp = r\"E:\\\\Abroad period research\\\\Phenology datasets\\\\PHENOLOGY_H1\\\\Cadiz_train.csv\"\n",
    "    test_fp = r\"E:\\\\Abroad period research\\\\Phenology datasets\\\\PHENOLOGY_H1\\\\Cadiz_test.csv\"\n",
    "\n",
    "    df_train, target_col = load_and_preprocess_data(train_fp)\n",
    "    X_train, y_train = feature_scaling(df_train, target_col)\n",
    "\n",
    "    df_test, _ = load_and_preprocess_data(test_fp)\n",
    "    X_test, y_test = feature_scaling(df_test, target_col)\n",
    "\n",
    "    methods = [\n",
    "        (\"Mutual Info\", mutual_information_feature_selection),\n",
    "        (\"Pearson\", pearson_correlation_feature_selection),\n",
    "        (\"Variance\", variance_threshold_feature_selection),\n",
    "        (\"RFE\", rfe_feature_selection),\n",
    "        (\"PSO\", pso_feature_selection),\n",
    "        (\"Lasso\", lasso_feature_selection),\n",
    "        (\"Tree-Based\", tree_based_feature_selection),\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "    for name, func in methods:\n",
    "        print(f\" {name}...\")\n",
    "        result = train_and_evaluate(X_train, X_test, y_train, y_test, func, name)\n",
    "        results.append(result)\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "    print(\"\\n FINAL COMPARISON TABLE:\")\n",
    "    print(df_results.to_string(index=False))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219cc1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# ðŸ“¦ IMPORTS\n",
    "# ===========================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_selection import mutual_info_classif, RFE, VarianceThreshold, SelectKBest, f_classif\n",
    "from sklearn.linear_model import Lasso, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# ===========================================\n",
    "#  DATA LOADING + PREPROCESSING\n",
    "# ===========================================\n",
    "def load_and_preprocess_data(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df.columns = df.columns.str.replace('\"', '').str.strip()\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'], errors='coerce')\n",
    "    df.dropna(subset=['TIME'], inplace=True)\n",
    "    df.set_index('TIME', inplace=True)\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "    target_col = 'fenologia_h1'\n",
    "    df[target_col].interpolate(method='linear', inplace=True)\n",
    "    df[target_col] = df[target_col] - 1\n",
    "\n",
    "    for i in range(1, 4):\n",
    "        df[f'{target_col}_lag{i}'] = df[target_col].shift(i)\n",
    "\n",
    "    for window in [3, 6]:\n",
    "        df[f'{target_col}_roll_mean_{window}'] = df[target_col].rolling(window).mean()\n",
    "        df[f'{target_col}_roll_std_{window}'] = df[target_col].rolling(window).std()\n",
    "\n",
    "    df['month'] = df.index.month\n",
    "    df['weekofyear'] = df.index.isocalendar().week\n",
    "    df['year'] = df.index.isocalendar().year\n",
    "    df['sin_week'] = np.sin(2 * np.pi * df['weekofyear'] / 52)\n",
    "    df['cos_week'] = np.cos(2 * np.pi * df['weekofyear'] / 52)\n",
    "\n",
    "    df['EMA_3'] = df[target_col].ewm(span=3, adjust=False).mean()\n",
    "    df['EMA_6'] = df[target_col].ewm(span=6, adjust=False).mean()\n",
    "\n",
    "    df['correlation_target_month'] = df[target_col].rolling(window=6).corr(df['month'])\n",
    "    df['correlation_target_week'] = df[target_col].rolling(window=6).corr(df['weekofyear'])\n",
    "\n",
    "    fft_values = np.fft.fft(df[target_col].dropna().values)\n",
    "    fft_real = np.real(fft_values)[:len(df[target_col])]\n",
    "    fft_imag = np.imag(fft_values)[:len(df[target_col])]\n",
    "    df['fft_real'] = np.concatenate([fft_real, np.nan * np.ones(len(df) - len(fft_real))])\n",
    "    df['fft_imag'] = np.concatenate([fft_imag, np.nan * np.ones(len(df) - len(fft_imag))])\n",
    "\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "    return df, target_col\n",
    "\n",
    "# ===========================================\n",
    "#  SCALING\n",
    "# ===========================================\n",
    "def feature_scaling(df, target_col):\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    return pd.DataFrame(X_scaled, columns=X.columns), y, scaler\n",
    "\n",
    "# ===========================================\n",
    "#  MODEL\n",
    "# ===========================================\n",
    "def build_mlp_model(input_dim, num_classes=4, num_units=64, dropout_rate=0.2, activation='relu'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_units, activation=activation, input_dim=input_dim))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(num_units // 2, activation=activation))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# ===========================================\n",
    "#  FEATURE SELECTION FUNCTIONS\n",
    "# ===========================================\n",
    "def timed_feature_selection(method, X, y):\n",
    "    start = time.time()\n",
    "    mask = method(X, y)\n",
    "    duration = time.time() - start\n",
    "    return mask, duration\n",
    "\n",
    "def mutual_information_feature_selection(X, y):\n",
    "    return mutual_info_classif(X, y) > 0.01\n",
    "\n",
    "def pearson_correlation_feature_selection(X, y):\n",
    "    scores = np.array([abs(pearsonr(X[col], y)[0]) for col in X.columns])\n",
    "    return scores > 0.1\n",
    "\n",
    "def variance_threshold_feature_selection(X, y, threshold=0.01):\n",
    "    selector = VarianceThreshold(threshold)\n",
    "    selector.fit(X)\n",
    "    return selector.get_support()\n",
    "\n",
    "def rfe_feature_selection(X, y, n_features_to_select=10):\n",
    "    rfe = RFE(SVC(kernel=\"linear\"), n_features_to_select=n_features_to_select)\n",
    "    rfe.fit(X, y)\n",
    "    return rfe.support_\n",
    "\n",
    "def logistic_regression_wrapper_selection(X, y):\n",
    "    model = LogisticRegression(solver='liblinear')\n",
    "    rfe = RFE(model, n_features_to_select=10)\n",
    "    rfe.fit(X, y)\n",
    "    return rfe.support_\n",
    "\n",
    "def f_classif_filter_selection(X, y):\n",
    "    selector = SelectKBest(score_func=f_classif, k=10)\n",
    "    selector.fit(X, y)\n",
    "    return selector.get_support()\n",
    "\n",
    "def lasso_feature_selection(X, y):\n",
    "    model = Lasso(alpha=0.01)\n",
    "    model.fit(X, y)\n",
    "    return np.abs(model.coef_) > 0.001\n",
    "\n",
    "def tree_based_feature_selection(X, y):\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(X, y)\n",
    "    return model.feature_importances_ > np.mean(model.feature_importances_)\n",
    "\n",
    "# ===========================================\n",
    "#  TRAINING AND EVALUATION\n",
    "# ===========================================\n",
    "def train_and_evaluate_model(X_train, X_test, y_train, y_test, selected_features):\n",
    "    X_train_sel = X_train.loc[:, selected_features]\n",
    "    X_test_sel = X_test.loc[:, selected_features]\n",
    "\n",
    "    model = build_mlp_model(input_dim=X_train_sel.shape[1])\n",
    "\n",
    "    start_train = time.time()\n",
    "    model.fit(X_train_sel, y_train, epochs=30, batch_size=32, validation_split=0.2, verbose=0)\n",
    "    train_time = time.time() - start_train\n",
    "\n",
    "    start_test = time.time()\n",
    "    y_pred = model.predict(X_test_sel).argmax(axis=1)\n",
    "    test_time = time.time() - start_test\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    return acc, prec, rec, f1, train_time, test_time\n",
    "\n",
    "# ===========================================\n",
    "#  MAIN\n",
    "# ===========================================\n",
    "def main():\n",
    "    train_fp = r\"E:\\\\Abroad period research\\\\Phenology datasets\\\\PHENOLOGY_H1\\\\Cadiz_train.csv\"\n",
    "    test_fp  = r\"E:\\\\Abroad period research\\\\Phenology datasets\\\\PHENOLOGY_H1\\\\Cadiz_test.csv\"\n",
    "\n",
    "    df_train, target_col = load_and_preprocess_data(train_fp)\n",
    "    X_train, y_train, scaler = feature_scaling(df_train, target_col)\n",
    "\n",
    "    df_test, _ = load_and_preprocess_data(test_fp)\n",
    "    X_test, y_test, _ = feature_scaling(df_test, target_col)\n",
    "\n",
    "    fs_methods = {\n",
    "        \"Mutual Info\": mutual_information_feature_selection,\n",
    "        \"F-Classif\": f_classif_filter_selection,\n",
    "        \"RFE\": rfe_feature_selection,\n",
    "        \"LogRegWrapper\": logistic_regression_wrapper_selection,\n",
    "        \"Lasso\": lasso_feature_selection,\n",
    "        \"Tree-Based\": tree_based_feature_selection\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for name, method in fs_methods.items():\n",
    "        print(f\"\\nðŸ” Running Feature Selection: {name}\")\n",
    "        mask, fs_time = timed_feature_selection(method, X_train, y_train)\n",
    "        selected_cols = X_train.columns[mask]\n",
    "        n_features = len(selected_cols)\n",
    "        reduction = 100 * (1 - n_features / X_train.shape[1])\n",
    "\n",
    "        acc, prec, rec, f1, train_time, test_time = train_and_evaluate_model(\n",
    "            X_train, X_test, y_train, y_test, selected_cols\n",
    "        )\n",
    "\n",
    "        results.append({\n",
    "            \"Method\": name,\n",
    "            \"Accuracy\": round(acc, 4),\n",
    "            \"Precision\": round(prec, 4),\n",
    "            \"Recall\": round(rec, 4),\n",
    "            \"F1 Score\": round(f1, 4),\n",
    "            \"#Features\": n_features,\n",
    "            \"Feature Reduction (%)\": round(reduction, 2),\n",
    "            \"FS Time (s)\": round(fs_time, 2),\n",
    "            \"Train Time (s)\": round(train_time, 2),\n",
    "            \"Test Time (s)\": round(test_time, 2)\n",
    "        })\n",
    "\n",
    "    print(\"\\n Final Comparison Table:\")\n",
    "    df_results = pd.DataFrame(results)\n",
    "    print(df_results.to_string(index=False))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591fc148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "#  IMPORTS\n",
    "# ===========================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_selection import mutual_info_classif, RFE, VarianceThreshold, SelectKBest, f_classif\n",
    "from sklearn.linear_model import Lasso, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import time\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.base import clone\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# ===========================================\n",
    "#  DATA LOADING + PREPROCESSING\n",
    "# ===========================================\n",
    "def load_and_preprocess_data(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df.columns = df.columns.str.replace('\"', '').str.strip()\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'], errors='coerce')\n",
    "    df.dropna(subset=['TIME'], inplace=True)\n",
    "    df.set_index('TIME', inplace=True)\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "    target_col = 'fenologia_h2'\n",
    "    df[target_col].interpolate(method='linear', inplace=True)\n",
    "    df[target_col] = df[target_col] - 1\n",
    "\n",
    "    for i in range(1, 4):\n",
    "        df[f'{target_col}_lag{i}'] = df[target_col].shift(i)\n",
    "\n",
    "    for window in [3, 6]:\n",
    "        df[f'{target_col}_roll_mean_{window}'] = df[target_col].rolling(window).mean()\n",
    "        df[f'{target_col}_roll_std_{window}'] = df[target_col].rolling(window).std()\n",
    "\n",
    "    df['month'] = df.index.month\n",
    "    df['weekofyear'] = df.index.isocalendar().week\n",
    "    df['year'] = df.index.isocalendar().year\n",
    "    df['sin_week'] = np.sin(2 * np.pi * df['weekofyear'] / 52)\n",
    "    df['cos_week'] = np.cos(2 * np.pi * df['weekofyear'] / 52)\n",
    "\n",
    "    df['EMA_3'] = df[target_col].ewm(span=3, adjust=False).mean()\n",
    "    df['EMA_6'] = df[target_col].ewm(span=6, adjust=False).mean()\n",
    "\n",
    "    df['correlation_target_month'] = df[target_col].rolling(window=6).corr(df['month'])\n",
    "    df['correlation_target_week'] = df[target_col].rolling(window=6).corr(df['weekofyear'])\n",
    "\n",
    "    fft_values = np.fft.fft(df[target_col].dropna().values)\n",
    "    fft_real = np.real(fft_values)[:len(df[target_col])]\n",
    "    fft_imag = np.imag(fft_values)[:len(df[target_col])]\n",
    "    df['fft_real'] = np.concatenate([fft_real, np.nan * np.ones(len(df) - len(fft_real))])\n",
    "    df['fft_imag'] = np.concatenate([fft_imag, np.nan * np.ones(len(df) - len(fft_imag))])\n",
    "\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "    return df, target_col\n",
    "\n",
    "# ===========================================\n",
    "#  SCALING\n",
    "# ===========================================\n",
    "def feature_scaling(df, target_col):\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    return pd.DataFrame(X_scaled, columns=X.columns), y, scaler\n",
    "\n",
    "# ===========================================\n",
    "#  MODEL\n",
    "# ===========================================\n",
    "def build_mlp_model(input_dim, num_classes=4, num_units=64, dropout_rate=0.2, activation='relu'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_units, activation=activation, input_dim=input_dim))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(num_units // 2, activation=activation))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# ===========================================\n",
    "#  FEATURE SELECTION FUNCTIONS\n",
    "# ===========================================\n",
    "def mutual_information_feature_selection(X, y):\n",
    "    return mutual_info_classif(X, y) > 0.01\n",
    "\n",
    "def pearson_correlation_feature_selection(X, y):\n",
    "    scores = np.array([abs(pearsonr(X[col], y)[0]) for col in X.columns])\n",
    "    return scores > 0.1\n",
    "\n",
    "def variance_threshold_feature_selection(X, threshold=0.01):\n",
    "    return VarianceThreshold(threshold).fit(X).get_support()\n",
    "\n",
    "def rfe_feature_selection(X, y, n_features_to_select=10):\n",
    "    return RFE(SVC(kernel=\"linear\"), n_features_to_select=n_features_to_select).fit(X, y).support_\n",
    "\n",
    "def logistic_regression_wrapper_selection(X, y):\n",
    "    return RFE(LogisticRegression(solver='liblinear'), n_features_to_select=10).fit(X, y).support_\n",
    "\n",
    "def f_classif_filter_selection(X, y):\n",
    "    return SelectKBest(score_func=f_classif, k=10).fit(X, y).get_support()\n",
    "\n",
    "def lasso_feature_selection(X, y):\n",
    "    return np.abs(Lasso(alpha=0.01).fit(X, y).coef_) > 0.001\n",
    "\n",
    "def tree_based_feature_selection(X, y):\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(X, y)\n",
    "    return model.feature_importances_ > np.mean(model.feature_importances_)\n",
    "\n",
    "def forward_selection(X, y, n_features=10):\n",
    "    model = RandomForestClassifier()\n",
    "    selected, remaining = [], list(X.columns)\n",
    "    while len(selected) < n_features:\n",
    "        best_feature = max(\n",
    "            ((clone(model).fit(X[selected + [f]], y).score(X[selected + [f]], y), f) for f in remaining),\n",
    "            key=lambda x: x[0]\n",
    "        )[1]\n",
    "        selected.append(best_feature)\n",
    "        remaining.remove(best_feature)\n",
    "    return X.columns.isin(selected)\n",
    "\n",
    "def linear_svc_l1_feature_selection(X, y):\n",
    "    svc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(X, y)\n",
    "    return SelectFromModel(svc, prefit=True).get_support()\n",
    "\n",
    "# ===========================================\n",
    "#  TRAINING AND EVALUATION\n",
    "# ===========================================\n",
    "def train_evaluate_record(X_train, X_test, y_train, y_test, fs_func, method_name):\n",
    "    record = {'Method': method_name}\n",
    "    fs_start = time.time()\n",
    "    selected_features = fs_func(X_train, y_train) if 'y' in fs_func.__code__.co_varnames else fs_func(X_train)\n",
    "    fs_time = time.time() - fs_start\n",
    "    record['FS Time (s)'] = round(fs_time, 4)\n",
    "\n",
    "    X_train_sel = X_train.loc[:, selected_features]\n",
    "    X_test_sel = X_test.loc[:, selected_features]\n",
    "    record['#Features'] = X_train_sel.shape[1]\n",
    "    record['Feature Reduction (%)'] = round(100 * (1 - X_train_sel.shape[1] / X_train.shape[1]), 2)\n",
    "\n",
    "    train_start = time.time()\n",
    "    model = build_mlp_model(X_train_sel.shape[1])\n",
    "    model.fit(X_train_sel, y_train, epochs=30, batch_size=32, validation_split=0.2, verbose=0)\n",
    "    \n",
    "    train_time = time.time() - train_start\n",
    "    record['Train Time (s)'] = round(train_time, 4)\n",
    "\n",
    "    test_start = time.time()\n",
    "    y_pred = model.predict(X_test_sel).argmax(axis=1)\n",
    "    test_time = time.time() - test_start\n",
    "    record['Test Time (s)'] = round(test_time, 4)\n",
    "\n",
    "    record['Accuracy'] = round(accuracy_score(y_test, y_pred), 4)\n",
    "    record['Precision'] = round(precision_score(y_test, y_pred, average='weighted'), 4)\n",
    "    record['Recall'] = round(recall_score(y_test, y_pred, average='weighted'), 4)\n",
    "    record['F1 Score'] = round(f1_score(y_test, y_pred, average='weighted'), 4)\n",
    "\n",
    "    return record\n",
    "\n",
    "# ===========================================\n",
    "#  MAIN\n",
    "# ===========================================\n",
    "def main():\n",
    "    train_fp = r\"E:\\Abroad period research\\Phenology datasets\\PHENOLOGY_H2_train.csv\"\n",
    "    test_fp  = r\"E:\\Abroad period research\\Phenology datasets\\PHENOLOGY_H2_test.csv\"\n",
    "\n",
    "    df_train, target_col = load_and_preprocess_data(train_fp)\n",
    "    X_train, y_train, _ = feature_scaling(df_train, target_col)\n",
    "\n",
    "    df_test, _ = load_and_preprocess_data(test_fp)\n",
    "    X_test, y_test, _ = feature_scaling(df_test, target_col)\n",
    "\n",
    "    fs_methods = [\n",
    "        (\"MI\", mutual_information_feature_selection),\n",
    "        (\"F-Classif\", f_classif_filter_selection),\n",
    "        (\"Variance\", variance_threshold_feature_selection),\n",
    "        (\"RFE\", rfe_feature_selection),\n",
    "        (\"LogRegWrapper\", logistic_regression_wrapper_selection),\n",
    "        (\"ForwardSel\", forward_selection),\n",
    "        (\"Lasso\", lasso_feature_selection),\n",
    "        (\"Tree\", tree_based_feature_selection),\n",
    "        (\"LinearSVC-L1\", linear_svc_l1_feature_selection),\n",
    "    ]\n",
    "\n",
    "    all_results = []\n",
    "    for name, func in fs_methods:\n",
    "        print(f\"\\n Running: {name}...\")\n",
    "        result = train_evaluate_record(X_train, X_test, y_train, y_test, func, name)\n",
    "        all_results.append(result)\n",
    "\n",
    "    df_results = pd.DataFrame(all_results)\n",
    "    print(\"\\n Final Comparison Table:\")\n",
    "    print(df_results.to_string(index=False))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bd53db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import pearsonr\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Dummy PSO (replace with your real PSO logic if needed)\n",
    "def pso(fitness_func, lb, ub, swarmsize=10, maxiter=3):\n",
    "    dim = len(lb)\n",
    "    best = np.random.rand(dim)\n",
    "    return best, 0.5\n",
    "\n",
    "# ======================\n",
    "# Load & Preprocess\n",
    "# ======================\n",
    "def load_and_preprocess_data(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df.columns = df.columns.str.replace('\"', '').str.strip()\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'], errors='coerce')\n",
    "    df.dropna(subset=['TIME'], inplace=True)\n",
    "    df.set_index('TIME', inplace=True)\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "    target_col = 'fenologia_h2'\n",
    "    df[target_col].interpolate(method='linear', inplace=True)\n",
    "    df[target_col] = df[target_col] - 1\n",
    "\n",
    "    for i in range(1, 4):\n",
    "        df[f'{target_col}_lag{i}'] = df[target_col].shift(i)\n",
    "\n",
    "    for window in [3, 6]:\n",
    "        df[f'{target_col}_roll_mean_{window}'] = df[target_col].rolling(window).mean()\n",
    "        df[f'{target_col}_roll_std_{window}'] = df[target_col].rolling(window).std()\n",
    "\n",
    "    df['month'] = df.index.month\n",
    "    df['weekofyear'] = df.index.isocalendar().week\n",
    "    df['year'] = df.index.isocalendar().year\n",
    "    df['sin_week'] = np.sin(2 * np.pi * df['weekofyear'] / 52)\n",
    "    df['cos_week'] = np.cos(2 * np.pi * df['weekofyear'] / 52)\n",
    "\n",
    "    df['EMA_3'] = df[target_col].ewm(span=3, adjust=False).mean()\n",
    "    df['EMA_6'] = df[target_col].ewm(span=6, adjust=False).mean()\n",
    "    df['correlation_target_month'] = df[target_col].rolling(window=6).corr(df['month'])\n",
    "    df['correlation_target_week'] = df[target_col].rolling(window=6).corr(df['weekofyear'])\n",
    "\n",
    "    fft_values = np.fft.fft(df[target_col].dropna().values)\n",
    "    fft_real = np.real(fft_values)[:len(df[target_col])]\n",
    "    fft_imag = np.imag(fft_values)[:len(df[target_col])]\n",
    "    df['fft_real'] = np.concatenate([fft_real, np.nan * np.ones(len(df) - len(fft_real))])\n",
    "    df['fft_imag'] = np.concatenate([fft_imag, np.nan * np.ones(len(df) - len(fft_imag))])\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "    return df, target_col\n",
    "\n",
    "# ======================\n",
    "# Scaling\n",
    "# ======================\n",
    "def feature_scaling(df, target_col):\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    return pd.DataFrame(X_scaled, columns=X.columns), y\n",
    "\n",
    "# ======================\n",
    "# MLP Model\n",
    "# ======================\n",
    "def build_mlp_model(input_dim, num_classes=4, num_units=64, dropout_rate=0.2, activation='relu'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_units, activation=activation, input_dim=input_dim))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(num_units // 2, activation=activation))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# ======================\n",
    "# Feature Selection Methods\n",
    "# ======================\n",
    "def pso_feature_selection(X, y):\n",
    "    def fitness_function(features):\n",
    "        mask = features > 0.5\n",
    "        if mask.sum() == 0:\n",
    "            return 1.0\n",
    "        clf = RandomForestClassifier()\n",
    "        clf.fit(X.iloc[:, mask], y)\n",
    "        acc = clf.score(X.iloc[:, mask], y)\n",
    "        return 1 - acc\n",
    "\n",
    "    lb = np.zeros(X.shape[1])\n",
    "    ub = np.ones(X.shape[1])\n",
    "    best_pos, _ = pso(fitness_function, lb, ub)\n",
    "    return best_pos > 0.5\n",
    "\n",
    "def pearson_feature_selection(X, y, threshold=0.1):\n",
    "    scores = np.array([abs(pearsonr(X[col], y)[0]) for col in X.columns])\n",
    "    return scores > threshold\n",
    "\n",
    "# ======================\n",
    "# Train & Evaluate\n",
    "# ======================\n",
    "def train_and_evaluate(X_train, X_test, y_train, y_test, selector_func, method_name):\n",
    "    start_fs = time.time()\n",
    "    mask = selector_func(X_train, y_train)\n",
    "    fs_time = time.time() - start_fs\n",
    "\n",
    "    selected_cols = X_train.columns[mask]\n",
    "    X_train_sel = X_train[selected_cols]\n",
    "    X_test_sel = X_test[selected_cols]\n",
    "\n",
    "    start_train = time.time()\n",
    "    model = build_mlp_model(input_dim=X_train_sel.shape[1])\n",
    "    model.fit(X_train_sel, y_train, epochs=30, batch_size=32, validation_split=0.2, verbose=0)\n",
    "    train_time = time.time() - start_train\n",
    "\n",
    "    start_test = time.time()\n",
    "    y_pred = model.predict(X_test_sel).argmax(axis=1)\n",
    "    test_time = time.time() - start_test\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    n_features = sum(mask)\n",
    "    reduction_pct = 100 * (1 - n_features / X_train.shape[1])\n",
    "\n",
    "    return {\n",
    "        \"Method\": method_name,\n",
    "        \"Accuracy\": round(acc, 4),\n",
    "        \"Precision\": round(prec, 4),\n",
    "        \"Recall\": round(rec, 4),\n",
    "        \"F1 Score\": round(f1, 4),\n",
    "        \"#Features\": n_features,\n",
    "        \"Feature Reduction (%)\": round(reduction_pct, 2),\n",
    "        \"FS Time (s)\": round(fs_time, 4),\n",
    "        \"Train Time (s)\": round(train_time, 4),\n",
    "        \"Test Time (s)\": round(test_time, 4)\n",
    "    }\n",
    "\n",
    "# ======================\n",
    "# Main\n",
    "# ======================\n",
    "def main():\n",
    "    train_fp = r\"E:\\Abroad period research\\Phenology datasets\\PHENOLOGY_H2_train.csv\"\n",
    "    test_fp  = r\"E:\\Abroad period research\\Phenology datasets\\PHENOLOGY_H2_test.csv\"\n",
    "\n",
    "    df_train, target_col = load_and_preprocess_data(train_fp)\n",
    "    X_train, y_train = feature_scaling(df_train, target_col)\n",
    "\n",
    "    df_test, _ = load_and_preprocess_data(test_fp)\n",
    "    X_test, y_test = feature_scaling(df_test, target_col)\n",
    "\n",
    "    methods = [\n",
    "        (\"PSO\", pso_feature_selection),\n",
    "        (\"Pearson\", pearson_feature_selection)\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "    for name, func in methods:\n",
    "        print(f\"\\nðŸ” Running Feature Selection: {name}\")\n",
    "        result = train_and_evaluate(X_train, X_test, y_train, y_test, func, name)\n",
    "        results.append(result)\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "    print(\"\\n FINAL COMPARISON TABLE:\")\n",
    "    print(df_results.to_string(index=False))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f5b801",
   "metadata": {},
   "source": [
    "Final feature selection code with all features selection algorithms for comparison"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
