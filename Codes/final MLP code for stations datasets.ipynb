{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Code with all feature engineering techniques "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BM + Lag + RWS + TBF + TLS + STL + Fourier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cadiz dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Optimizing Hyperparameters...\n",
      "|   iter    |  target   | batch_... | dropou... |  epochs   | num_units |\n",
      "-------------------------------------------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.9385   \u001b[39m | \u001b[39m33.98    \u001b[39m | \u001b[39m0.4803   \u001b[39m | \u001b[39m51.96    \u001b[39m | \u001b[39m89.47    \u001b[39m |\n",
      "| \u001b[39m2        \u001b[39m | \u001b[39m0.9385   \u001b[39m | \u001b[39m23.49    \u001b[39m | \u001b[39m0.1624   \u001b[39m | \u001b[39m31.74    \u001b[39m | \u001b[39m115.2    \u001b[39m |\n",
      "| \u001b[39m3        \u001b[39m | \u001b[39m0.9077   \u001b[39m | \u001b[39m44.85    \u001b[39m | \u001b[39m0.3832   \u001b[39m | \u001b[39m30.62    \u001b[39m | \u001b[39m125.1    \u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m0.9077   \u001b[39m | \u001b[39m55.96    \u001b[39m | \u001b[39m0.1849   \u001b[39m | \u001b[39m35.45    \u001b[39m | \u001b[39m49.61    \u001b[39m |\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m0.9385   \u001b[39m | \u001b[39m30.6     \u001b[39m | \u001b[39m0.3099   \u001b[39m | \u001b[39m42.96    \u001b[39m | \u001b[39m59.96    \u001b[39m |\n",
      "| \u001b[39m6        \u001b[39m | \u001b[39m0.9385   \u001b[39m | \u001b[39m22.21    \u001b[39m | \u001b[39m0.3256   \u001b[39m | \u001b[39m31.47    \u001b[39m | \u001b[39m115.9    \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m0.9385   \u001b[39m | \u001b[39m18.02    \u001b[39m | \u001b[39m0.4178   \u001b[39m | \u001b[39m37.39    \u001b[39m | \u001b[39m96.29    \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m0.9385   \u001b[39m | \u001b[39m16.29    \u001b[39m | \u001b[39m0.3761   \u001b[39m | \u001b[39m56.62    \u001b[39m | \u001b[39m69.8     \u001b[39m |\n",
      "| \u001b[35m9        \u001b[39m | \u001b[35m0.9846   \u001b[39m | \u001b[35m16.43    \u001b[39m | \u001b[35m0.3942   \u001b[39m | \u001b[35m59.5     \u001b[39m | \u001b[35m113.4    \u001b[39m |\n",
      "| \u001b[35m10       \u001b[39m | \u001b[35m1.0      \u001b[39m | \u001b[35m19.73    \u001b[39m | \u001b[35m0.1139   \u001b[39m | \u001b[35m59.16    \u001b[39m | \u001b[35m125.8    \u001b[39m |\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m0.9692   \u001b[39m | \u001b[39m33.07    \u001b[39m | \u001b[39m0.2687   \u001b[39m | \u001b[39m59.19    \u001b[39m | \u001b[39m125.8    \u001b[39m |\n",
      "| \u001b[39m12       \u001b[39m | \u001b[39m0.9846   \u001b[39m | \u001b[39m16.3     \u001b[39m | \u001b[39m0.1833   \u001b[39m | \u001b[39m53.52    \u001b[39m | \u001b[39m123.6    \u001b[39m |\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m0.9385   \u001b[39m | \u001b[39m17.81    \u001b[39m | \u001b[39m0.2956   \u001b[39m | \u001b[39m58.87    \u001b[39m | \u001b[39m32.18    \u001b[39m |\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m0.9846   \u001b[39m | \u001b[39m16.6     \u001b[39m | \u001b[39m0.4329   \u001b[39m | \u001b[39m58.76    \u001b[39m | \u001b[39m126.3    \u001b[39m |\n",
      "| \u001b[39m15       \u001b[39m | \u001b[39m0.9538   \u001b[39m | \u001b[39m23.41    \u001b[39m | \u001b[39m0.4105   \u001b[39m | \u001b[39m59.0     \u001b[39m | \u001b[39m118.3    \u001b[39m |\n",
      "| \u001b[39m16       \u001b[39m | \u001b[39m0.9538   \u001b[39m | \u001b[39m23.42    \u001b[39m | \u001b[39m0.2729   \u001b[39m | \u001b[39m54.01    \u001b[39m | \u001b[39m127.5    \u001b[39m |\n",
      "| \u001b[39m17       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m17.25    \u001b[39m | \u001b[39m0.1331   \u001b[39m | \u001b[39m57.92    \u001b[39m | \u001b[39m120.0    \u001b[39m |\n",
      "| \u001b[39m18       \u001b[39m | \u001b[39m0.9538   \u001b[39m | \u001b[39m16.6     \u001b[39m | \u001b[39m0.4275   \u001b[39m | \u001b[39m52.1     \u001b[39m | \u001b[39m116.5    \u001b[39m |\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m16.01    \u001b[39m | \u001b[39m0.3415   \u001b[39m | \u001b[39m59.86    \u001b[39m | \u001b[39m106.9    \u001b[39m |\n",
      "| \u001b[39m20       \u001b[39m | \u001b[39m0.9538   \u001b[39m | \u001b[39m20.33    \u001b[39m | \u001b[39m0.425    \u001b[39m | \u001b[39m58.34    \u001b[39m | \u001b[39m103.1    \u001b[39m |\n",
      "=========================================================================\n",
      "\n",
      " Best Hyperparameters: {'batch_size': 19.73070168682728, 'dropout_rate': 0.11388648091359346, 'epochs': 59.15614005666498, 'num_units': 125.84664986800777}\n",
      "\n",
      " Training Final MLP Model...\n",
      "Epoch 1/59\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - accuracy: 0.3565 - loss: 1.2866 - val_accuracy: 0.3077 - val_loss: 1.1639\n",
      "Epoch 2/59\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3994 - loss: 1.0920 - val_accuracy: 0.6615 - val_loss: 0.9408\n",
      "Epoch 3/59\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6930 - loss: 0.9346 - val_accuracy: 0.7692 - val_loss: 0.7731\n",
      "Epoch 4/59\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7066 - loss: 0.8029 - val_accuracy: 0.8308 - val_loss: 0.6364\n",
      "Epoch 5/59\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8050 - loss: 0.6534 - val_accuracy: 0.8615 - val_loss: 0.5371\n",
      "Epoch 6/59\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7921 - loss: 0.5877 - val_accuracy: 0.8615 - val_loss: 0.4732\n",
      "Epoch 7/59\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7980 - loss: 0.5236 - val_accuracy: 0.8923 - val_loss: 0.4153\n",
      "Epoch 8/59\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8333 - loss: 0.4450 - val_accuracy: 0.8615 - val_loss: 0.3935\n",
      "Epoch 9/59\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8437 - loss: 0.4225 - val_accuracy: 0.8769 - val_loss: 0.3733\n",
      "Epoch 10/59\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8563 - loss: 0.3881 - val_accuracy: 0.8923 - val_loss: 0.3441\n",
      "Epoch 11/59\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8290 - loss: 0.4251 - val_accuracy: 0.8923 - val_loss: 0.3316\n",
      "Epoch 12/59\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8556 - loss: 0.3676 - val_accuracy: 0.8923 - val_loss: 0.3042\n",
      "Epoch 13/59\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8471 - loss: 0.3805 - val_accuracy: 0.8923 - val_loss: 0.2978\n",
      "Epoch 14/59\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8681 - loss: 0.3535 - val_accuracy: 0.9077 - val_loss: 0.2752\n",
      "Epoch 15/59\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8489 - loss: 0.3529 - val_accuracy: 0.9077 - val_loss: 0.2623\n",
      "Epoch 16/59\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8564 - loss: 0.3503 - val_accuracy: 0.9077 - val_loss: 0.2457\n",
      "Epoch 17/59\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8652 - loss: 0.3224 - val_accuracy: 0.9385 - val_loss: 0.2396\n",
      "Epoch 18/59\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8688 - loss: 0.3036 - val_accuracy: 0.9385 - val_loss: 0.2316\n",
      "Epoch 19/59\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8906 - loss: 0.3045 - val_accuracy: 0.9231 - val_loss: 0.2000\n",
      "Epoch 20/59\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8698 - loss: 0.3101 - val_accuracy: 0.9385 - val_loss: 0.2176\n",
      "Epoch 21/59\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8871 - loss: 0.3054 - val_accuracy: 0.9385 - val_loss: 0.1914\n",
      "Epoch 22/59\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8960 - loss: 0.2770 - val_accuracy: 0.9385 - val_loss: 0.1795\n",
      "Epoch 23/59\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8997 - loss: 0.2861 - val_accuracy: 0.9385 - val_loss: 0.1668\n",
      "Epoch 24/59\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8934 - loss: 0.2790 - val_accuracy: 0.9385 - val_loss: 0.1649\n",
      "Epoch 25/59\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9073 - loss: 0.2490 - val_accuracy: 0.9385 - val_loss: 0.1590\n",
      "Epoch 26/59\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9001 - loss: 0.2411 - val_accuracy: 0.9385 - val_loss: 0.1458\n",
      "Epoch 27/59\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9033 - loss: 0.2328 - val_accuracy: 0.9385 - val_loss: 0.1435\n",
      "Epoch 28/59\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8864 - loss: 0.2453 - val_accuracy: 0.9538 - val_loss: 0.1250\n",
      "Epoch 29/59\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9078 - loss: 0.2217 - val_accuracy: 0.9385 - val_loss: 0.1338\n",
      "Epoch 30/59\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9071 - loss: 0.2140 - val_accuracy: 0.9538 - val_loss: 0.1146\n",
      "Epoch 31/59\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9196 - loss: 0.2212 - val_accuracy: 0.9538 - val_loss: 0.1109\n",
      "Epoch 32/59\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9108 - loss: 0.2010 - val_accuracy: 0.9538 - val_loss: 0.1073\n",
      "Epoch 33/59\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9124 - loss: 0.2116 - val_accuracy: 0.9846 - val_loss: 0.0963\n",
      "Epoch 34/59\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9211 - loss: 0.2069 - val_accuracy: 0.9538 - val_loss: 0.1054\n",
      "Epoch 35/59\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9379 - loss: 0.1873 - val_accuracy: 0.9846 - val_loss: 0.0890\n",
      "Epoch 36/59\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9342 - loss: 0.1649 - val_accuracy: 0.9692 - val_loss: 0.0877\n",
      "Epoch 37/59\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9443 - loss: 0.1592 - val_accuracy: 0.9692 - val_loss: 0.0826\n",
      "Epoch 38/59\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9250 - loss: 0.1610 - val_accuracy: 0.9692 - val_loss: 0.0830\n",
      "Epoch 39/59\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9369 - loss: 0.1700 - val_accuracy: 0.9846 - val_loss: 0.0714\n",
      "Epoch 40/59\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9439 - loss: 0.1870 - val_accuracy: 0.9846 - val_loss: 0.0703\n",
      "Epoch 41/59\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9260 - loss: 0.1660 - val_accuracy: 0.9846 - val_loss: 0.0744\n",
      "Epoch 42/59\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9292 - loss: 0.1674 - val_accuracy: 0.9692 - val_loss: 0.0699\n",
      "Epoch 43/59\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9503 - loss: 0.1544 - val_accuracy: 0.9846 - val_loss: 0.0667\n",
      "Epoch 44/59\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9468 - loss: 0.1383 - val_accuracy: 0.9846 - val_loss: 0.0684\n",
      "Epoch 45/59\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9649 - loss: 0.1281 - val_accuracy: 0.9846 - val_loss: 0.0628\n",
      "Epoch 46/59\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9360 - loss: 0.1581 - val_accuracy: 0.9846 - val_loss: 0.0616\n",
      "Epoch 47/59\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9612 - loss: 0.1294 - val_accuracy: 0.9846 - val_loss: 0.0593\n",
      "Epoch 48/59\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9575 - loss: 0.1383 - val_accuracy: 0.9846 - val_loss: 0.0515\n",
      "Epoch 49/59\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9450 - loss: 0.1364 - val_accuracy: 1.0000 - val_loss: 0.0557\n",
      "Epoch 50/59\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9423 - loss: 0.1411 - val_accuracy: 0.9846 - val_loss: 0.0514\n",
      "Epoch 51/59\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9709 - loss: 0.1109 - val_accuracy: 0.9846 - val_loss: 0.0561\n",
      "Epoch 52/59\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9481 - loss: 0.1316 - val_accuracy: 1.0000 - val_loss: 0.0513\n",
      "Epoch 53/59\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9670 - loss: 0.1132 - val_accuracy: 0.9846 - val_loss: 0.0423\n",
      "Epoch 54/59\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9698 - loss: 0.1116 - val_accuracy: 1.0000 - val_loss: 0.0503\n",
      "Epoch 55/59\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9592 - loss: 0.1111 - val_accuracy: 0.9846 - val_loss: 0.0461\n",
      "Epoch 56/59\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9470 - loss: 0.1230 - val_accuracy: 1.0000 - val_loss: 0.0430\n",
      "Epoch 57/59\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9406 - loss: 0.1129 - val_accuracy: 0.9846 - val_loss: 0.0434\n",
      "Epoch 58/59\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9721 - loss: 0.1106 - val_accuracy: 1.0000 - val_loss: 0.0369\n",
      "Epoch 59/59\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9710 - loss: 0.1206 - val_accuracy: 0.9846 - val_loss: 0.0501\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Evaluation =====\n",
      "Accuracy: 0.9440\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8000    0.5000    0.6154        16\n",
      "           1     0.8816    0.9710    0.9241        69\n",
      "           2     0.9926    0.9854    0.9890       137\n",
      "           3     0.9000    0.9000    0.9000        10\n",
      "\n",
      "    accuracy                         0.9440       232\n",
      "   macro avg     0.8936    0.8391    0.8571       232\n",
      "weighted avg     0.9423    0.9440    0.9401       232\n",
      "\n",
      "[[  8   8   0   0]\n",
      " [  2  67   0   0]\n",
      " [  0   1 135   1]\n",
      " [  0   0   1   9]]\n",
      "\n",
      "===== Model Metrics =====\n",
      "Training Time: 15.96s\n",
      "Testing Time (total): 0.38s\n",
      "Inference/sample: 0.001623s\n",
      "Model Size: 0.15 MB\n",
      "Trainable Params: 10,314\n",
      "RAM Usage: 734.82 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import psutil\n",
    "import warnings\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from bayes_opt import BayesianOptimization\n",
    "from scipy.fftpack import fft\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 1. Load and Preprocess Data\n",
    "def load_and_preprocess_data(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df.columns = df.columns.str.replace('\"', '').str.strip()\n",
    "    if 'TIME' not in df.columns:\n",
    "        raise KeyError(\"Column 'TIME' missing.\")\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'], errors='coerce')\n",
    "    df.dropna(subset=['TIME'], inplace=True)\n",
    "    df.set_index('TIME', inplace=True)\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "    target_col = 'fenologia_h1'\n",
    "    if target_col not in df.columns:\n",
    "        target_col = 'target'\n",
    "    if target_col not in df.columns:\n",
    "        raise KeyError(\"Target column not found.\")\n",
    "\n",
    "    df[target_col].interpolate(method='linear', inplace=True)\n",
    "    df[target_col] = df[target_col].astype(int)\n",
    "    df[target_col] = df[target_col] - 1\n",
    "\n",
    "    for i in range(1, 4):\n",
    "        df[f'{target_col}_lag{i}'] = df[target_col].shift(i)\n",
    "    for window in [3, 6]:\n",
    "        df[f'{target_col}_roll_mean_{window}'] = df[target_col].rolling(window).mean()\n",
    "        df[f'{target_col}_roll_std_{window}'] = df[target_col].rolling(window).std()\n",
    "\n",
    "\n",
    "\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "    return df, target_col\n",
    "\n",
    "# 2. Feature Scaling\n",
    "def feature_scaling(df, target_col):\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    return X_scaled, y, scaler\n",
    "\n",
    "# 3. MLP Model\n",
    "def build_mlp_model(input_shape, num_classes=4, num_units=64, dropout_rate=0.2):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Input(shape=(input_shape,)))\n",
    "    model.add(layers.Dense(num_units, activation='relu'))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    model.add(layers.Dense(num_units // 2, activation='relu'))\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "# 4. Objective Function\n",
    "def objective_function(num_units, dropout_rate, epochs, batch_size, X_train, y_train):\n",
    "    model = build_mlp_model(\n",
    "        input_shape=X_train.shape[1],\n",
    "        num_classes=4,\n",
    "        num_units=int(num_units),\n",
    "        dropout_rate=dropout_rate\n",
    "    )\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=int(epochs),\n",
    "        batch_size=int(batch_size),\n",
    "        validation_split=0.2,\n",
    "        verbose=0\n",
    "    )\n",
    "    return np.max(history.history['val_accuracy'])\n",
    "\n",
    "# 5. Hyperparameter Optimization\n",
    "def optimize_hyperparameters(X_train, y_train):\n",
    "    pbounds = {\n",
    "        'num_units': (32, 128),\n",
    "        'dropout_rate': (0.1, 0.5),\n",
    "        'epochs': (30, 60),\n",
    "        'batch_size': (16, 64)\n",
    "    }\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=lambda num_units, dropout_rate, epochs, batch_size: objective_function(\n",
    "            num_units, dropout_rate, epochs, batch_size, X_train, y_train\n",
    "        ),\n",
    "        pbounds=pbounds,\n",
    "        random_state=42,\n",
    "        verbose=2\n",
    "    )\n",
    "    optimizer.maximize(init_points=5, n_iter=15)\n",
    "    return optimizer.max['params']\n",
    "\n",
    "# 6. Train and Evaluate\n",
    "def train_and_evaluate_model(X_train, X_test, y_train, y_test, best_params):\n",
    "    model = build_mlp_model(\n",
    "        input_shape=X_train.shape[1],\n",
    "        num_classes=4,\n",
    "        num_units=int(best_params['num_units']),\n",
    "        dropout_rate=float(best_params['dropout_rate'])\n",
    "    )\n",
    "    start = time.time()\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=int(best_params['epochs']),\n",
    "        batch_size=int(best_params['batch_size']),\n",
    "        validation_split=0.2,\n",
    "        verbose=1\n",
    "    )\n",
    "    training_time = time.time() - start\n",
    "\n",
    "    start = time.time()\n",
    "    y_pred = model.predict(X_test).argmax(axis=1)\n",
    "    testing_time = time.time() - start\n",
    "    inference_time = testing_time / len(X_test)\n",
    "\n",
    "    print(\"\\n===== Evaluation =====\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(classification_report(y_test, y_pred, digits=4))\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    model.save(\"temp_model.h5\")\n",
    "    model_size = os.path.getsize(\"temp_model.h5\") / (1024 ** 2)\n",
    "    os.remove(\"temp_model.h5\")\n",
    "\n",
    "    total_params = np.sum([np.prod(v.shape) for v in model.trainable_weights])\n",
    "    ram_usage = psutil.Process(os.getpid()).memory_info().rss / (1024 ** 2)\n",
    "\n",
    "    print(\"\\n===== Model Metrics =====\")\n",
    "    print(f\"Training Time: {training_time:.2f}s\")\n",
    "    print(f\"Testing Time (total): {testing_time:.2f}s\")\n",
    "    print(f\"Inference/sample: {inference_time:.6f}s\")\n",
    "    print(f\"Model Size: {model_size:.2f} MB\")\n",
    "    print(f\"Trainable Params: {total_params:,}\")\n",
    "    print(f\"RAM Usage: {ram_usage:.2f} MB\")\n",
    "\n",
    "    return model, history\n",
    "\n",
    "# 7. Main\n",
    "def main():\n",
    "    train_path = r\"E:\\Abroad period research\\Phenology datasets\\PHENOLOGY_H1\\Cadiz_train.csv\"\n",
    "    test_path = r\"E:\\Abroad period research\\Phenology datasets\\PHENOLOGY_H1\\Cadiz_test.csv\"\n",
    "\n",
    "    df_train, target_col = load_and_preprocess_data(train_path)\n",
    "    X_train, y_train, _ = feature_scaling(df_train, target_col)\n",
    "\n",
    "    df_test, _ = load_and_preprocess_data(test_path)\n",
    "    X_test, y_test, _ = feature_scaling(df_test, target_col)\n",
    "\n",
    "    print(\"\\n Optimizing Hyperparameters...\")\n",
    "    best_params = optimize_hyperparameters(X_train, y_train)\n",
    "    print(\"\\n Best Hyperparameters:\", best_params)\n",
    "\n",
    "    print(\"\\n Training Final MLP Model...\")\n",
    "    model, history = train_and_evaluate_model(X_train, X_test, y_train, y_test, best_params)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cordoba dataset MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Optimizing Hyperparameters...\n",
      "|   iter    |  target   | batch_... | dropou... |  epochs   | num_units |\n",
      "-------------------------------------------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.8871   \u001b[39m | \u001b[39m33.98    \u001b[39m | \u001b[39m0.4803   \u001b[39m | \u001b[39m51.96    \u001b[39m | \u001b[39m89.47    \u001b[39m |\n",
      "| \u001b[35m2        \u001b[39m | \u001b[35m0.9194   \u001b[39m | \u001b[35m23.49    \u001b[39m | \u001b[35m0.1624   \u001b[39m | \u001b[35m31.74    \u001b[39m | \u001b[35m115.2    \u001b[39m |\n",
      "| \u001b[39m3        \u001b[39m | \u001b[39m0.8871   \u001b[39m | \u001b[39m44.85    \u001b[39m | \u001b[39m0.3832   \u001b[39m | \u001b[39m30.62    \u001b[39m | \u001b[39m125.1    \u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m0.8548   \u001b[39m | \u001b[39m55.96    \u001b[39m | \u001b[39m0.1849   \u001b[39m | \u001b[39m35.45    \u001b[39m | \u001b[39m49.61    \u001b[39m |\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m0.8871   \u001b[39m | \u001b[39m30.6     \u001b[39m | \u001b[39m0.3099   \u001b[39m | \u001b[39m42.96    \u001b[39m | \u001b[39m59.96    \u001b[39m |\n",
      "| \u001b[39m6        \u001b[39m | \u001b[39m0.9194   \u001b[39m | \u001b[39m23.48    \u001b[39m | \u001b[39m0.2078   \u001b[39m | \u001b[39m33.0     \u001b[39m | \u001b[39m115.1    \u001b[39m |\n",
      "| \u001b[35m7        \u001b[39m | \u001b[35m0.9677   \u001b[39m | \u001b[35m16.31    \u001b[39m | \u001b[35m0.145    \u001b[39m | \u001b[35m56.54    \u001b[39m | \u001b[35m126.3    \u001b[39m |\n",
      "| \u001b[35m8        \u001b[39m | \u001b[35m0.9839   \u001b[39m | \u001b[35m29.18    \u001b[39m | \u001b[35m0.2319   \u001b[39m | \u001b[35m58.96    \u001b[39m | \u001b[35m127.9    \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m0.9355   \u001b[39m | \u001b[39m62.74    \u001b[39m | \u001b[39m0.2653   \u001b[39m | \u001b[39m59.6     \u001b[39m | \u001b[39m126.9    \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m0.9355   \u001b[39m | \u001b[39m35.19    \u001b[39m | \u001b[39m0.4501   \u001b[39m | \u001b[39m59.97    \u001b[39m | \u001b[39m117.4    \u001b[39m |\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m0.9194   \u001b[39m | \u001b[39m16.18    \u001b[39m | \u001b[39m0.1528   \u001b[39m | \u001b[39m58.82    \u001b[39m | \u001b[39m34.98    \u001b[39m |\n",
      "| \u001b[39m12       \u001b[39m | \u001b[39m0.9839   \u001b[39m | \u001b[39m25.58    \u001b[39m | \u001b[39m0.1051   \u001b[39m | \u001b[39m51.04    \u001b[39m | \u001b[39m127.5    \u001b[39m |\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m0.9677   \u001b[39m | \u001b[39m16.75    \u001b[39m | \u001b[39m0.4975   \u001b[39m | \u001b[39m42.66    \u001b[39m | \u001b[39m127.3    \u001b[39m |\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m0.871    \u001b[39m | \u001b[39m17.93    \u001b[39m | \u001b[39m0.4922   \u001b[39m | \u001b[39m30.28    \u001b[39m | \u001b[39m34.7     \u001b[39m |\n",
      "| \u001b[39m15       \u001b[39m | \u001b[39m0.9032   \u001b[39m | \u001b[39m35.26    \u001b[39m | \u001b[39m0.4147   \u001b[39m | \u001b[39m53.74    \u001b[39m | \u001b[39m127.9    \u001b[39m |\n",
      "| \u001b[39m16       \u001b[39m | \u001b[39m0.9677   \u001b[39m | \u001b[39m23.48    \u001b[39m | \u001b[39m0.3988   \u001b[39m | \u001b[39m58.05    \u001b[39m | \u001b[39m122.2    \u001b[39m |\n",
      "| \u001b[39m17       \u001b[39m | \u001b[39m0.9677   \u001b[39m | \u001b[39m22.33    \u001b[39m | \u001b[39m0.3505   \u001b[39m | \u001b[39m57.37    \u001b[39m | \u001b[39m127.9    \u001b[39m |\n",
      "| \u001b[39m18       \u001b[39m | \u001b[39m0.9677   \u001b[39m | \u001b[39m17.77    \u001b[39m | \u001b[39m0.3675   \u001b[39m | \u001b[39m48.85    \u001b[39m | \u001b[39m119.7    \u001b[39m |\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m0.9839   \u001b[39m | \u001b[39m16.01    \u001b[39m | \u001b[39m0.3415   \u001b[39m | \u001b[39m59.86    \u001b[39m | \u001b[39m106.9    \u001b[39m |\n",
      "| \u001b[39m20       \u001b[39m | \u001b[39m0.9194   \u001b[39m | \u001b[39m17.35    \u001b[39m | \u001b[39m0.4605   \u001b[39m | \u001b[39m50.77    \u001b[39m | \u001b[39m105.3    \u001b[39m |\n",
      "=========================================================================\n",
      "\n",
      " Best Hyperparameters: {'batch_size': 29.17764456011239, 'dropout_rate': 0.2318874610196986, 'epochs': 58.95575335001614, 'num_units': 127.85257981802381}\n",
      "\n",
      " Training Final MLP Model...\n",
      "Epoch 1/58\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.4193 - loss: 1.2910 - val_accuracy: 0.5806 - val_loss: 1.0808\n",
      "Epoch 2/58\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5231 - loss: 1.0812 - val_accuracy: 0.5806 - val_loss: 0.9540\n",
      "Epoch 3/58\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5548 - loss: 0.9458 - val_accuracy: 0.6129 - val_loss: 0.8445\n",
      "Epoch 4/58\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6991 - loss: 0.8287 - val_accuracy: 0.6774 - val_loss: 0.7277\n",
      "Epoch 5/58\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7502 - loss: 0.7231 - val_accuracy: 0.7742 - val_loss: 0.6355\n",
      "Epoch 6/58\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8032 - loss: 0.6664 - val_accuracy: 0.8065 - val_loss: 0.5735\n",
      "Epoch 7/58\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8197 - loss: 0.5794 - val_accuracy: 0.8387 - val_loss: 0.5230\n",
      "Epoch 8/58\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8156 - loss: 0.5260 - val_accuracy: 0.8548 - val_loss: 0.4857\n",
      "Epoch 9/58\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8352 - loss: 0.5004 - val_accuracy: 0.8710 - val_loss: 0.4580\n",
      "Epoch 10/58\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8552 - loss: 0.4509 - val_accuracy: 0.8710 - val_loss: 0.4431\n",
      "Epoch 11/58\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8204 - loss: 0.4452 - val_accuracy: 0.8710 - val_loss: 0.4237\n",
      "Epoch 12/58\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8452 - loss: 0.3887 - val_accuracy: 0.8710 - val_loss: 0.4054\n",
      "Epoch 13/58\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8713 - loss: 0.3737 - val_accuracy: 0.8710 - val_loss: 0.3922\n",
      "Epoch 14/58\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8898 - loss: 0.3547 - val_accuracy: 0.8710 - val_loss: 0.3879\n",
      "Epoch 15/58\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8457 - loss: 0.3737 - val_accuracy: 0.8710 - val_loss: 0.3715\n",
      "Epoch 16/58\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8666 - loss: 0.3476 - val_accuracy: 0.8710 - val_loss: 0.3556\n",
      "Epoch 17/58\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8772 - loss: 0.3317 - val_accuracy: 0.8710 - val_loss: 0.3511\n",
      "Epoch 18/58\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8762 - loss: 0.3215 - val_accuracy: 0.8871 - val_loss: 0.3469\n",
      "Epoch 19/58\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8902 - loss: 0.2887 - val_accuracy: 0.8871 - val_loss: 0.3365\n",
      "Epoch 20/58\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8983 - loss: 0.2925 - val_accuracy: 0.9032 - val_loss: 0.3230\n",
      "Epoch 21/58\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8877 - loss: 0.2942 - val_accuracy: 0.8710 - val_loss: 0.3260\n",
      "Epoch 22/58\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9064 - loss: 0.2760 - val_accuracy: 0.9032 - val_loss: 0.3128\n",
      "Epoch 23/58\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9146 - loss: 0.2553 - val_accuracy: 0.9032 - val_loss: 0.3011\n",
      "Epoch 24/58\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9110 - loss: 0.2682 - val_accuracy: 0.8871 - val_loss: 0.2954\n",
      "Epoch 25/58\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9181 - loss: 0.2538 - val_accuracy: 0.9032 - val_loss: 0.2872\n",
      "Epoch 26/58\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9098 - loss: 0.2611 - val_accuracy: 0.8871 - val_loss: 0.2758\n",
      "Epoch 27/58\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9257 - loss: 0.2314 - val_accuracy: 0.8871 - val_loss: 0.2744\n",
      "Epoch 28/58\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9019 - loss: 0.2371 - val_accuracy: 0.8871 - val_loss: 0.2620\n",
      "Epoch 29/58\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9280 - loss: 0.2143 - val_accuracy: 0.8871 - val_loss: 0.2622\n",
      "Epoch 30/58\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8998 - loss: 0.2365 - val_accuracy: 0.8871 - val_loss: 0.2574\n",
      "Epoch 31/58\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9377 - loss: 0.2093 - val_accuracy: 0.8871 - val_loss: 0.2507\n",
      "Epoch 32/58\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9088 - loss: 0.2370 - val_accuracy: 0.8710 - val_loss: 0.2385\n",
      "Epoch 33/58\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9197 - loss: 0.2077 - val_accuracy: 0.8871 - val_loss: 0.2357\n",
      "Epoch 34/58\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9427 - loss: 0.1850 - val_accuracy: 0.8871 - val_loss: 0.2300\n",
      "Epoch 35/58\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9279 - loss: 0.1879 - val_accuracy: 0.8710 - val_loss: 0.2298\n",
      "Epoch 36/58\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9324 - loss: 0.1877 - val_accuracy: 0.8871 - val_loss: 0.2205\n",
      "Epoch 37/58\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9347 - loss: 0.1848 - val_accuracy: 0.8710 - val_loss: 0.2210\n",
      "Epoch 38/58\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9439 - loss: 0.1686 - val_accuracy: 0.8710 - val_loss: 0.2124\n",
      "Epoch 39/58\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9434 - loss: 0.1764 - val_accuracy: 0.9032 - val_loss: 0.2020\n",
      "Epoch 40/58\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9276 - loss: 0.1903 - val_accuracy: 0.8871 - val_loss: 0.2035\n",
      "Epoch 41/58\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9612 - loss: 0.1601 - val_accuracy: 0.9032 - val_loss: 0.1909\n",
      "Epoch 42/58\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9464 - loss: 0.1664 - val_accuracy: 0.9032 - val_loss: 0.1892\n",
      "Epoch 43/58\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9389 - loss: 0.1555 - val_accuracy: 0.9194 - val_loss: 0.1885\n",
      "Epoch 44/58\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9697 - loss: 0.1435 - val_accuracy: 0.9032 - val_loss: 0.1790\n",
      "Epoch 45/58\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9403 - loss: 0.1650 - val_accuracy: 0.9194 - val_loss: 0.1694\n",
      "Epoch 46/58\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9532 - loss: 0.1498 - val_accuracy: 0.9194 - val_loss: 0.1676\n",
      "Epoch 47/58\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9628 - loss: 0.1375 - val_accuracy: 0.9194 - val_loss: 0.1669\n",
      "Epoch 48/58\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9560 - loss: 0.1393 - val_accuracy: 0.9194 - val_loss: 0.1696\n",
      "Epoch 49/58\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9586 - loss: 0.1357 - val_accuracy: 0.9194 - val_loss: 0.1611\n",
      "Epoch 50/58\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9589 - loss: 0.1205 - val_accuracy: 0.9194 - val_loss: 0.1577\n",
      "Epoch 51/58\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9184 - loss: 0.1694 - val_accuracy: 0.9516 - val_loss: 0.1429\n",
      "Epoch 52/58\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9577 - loss: 0.1408 - val_accuracy: 0.9194 - val_loss: 0.1473\n",
      "Epoch 53/58\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9572 - loss: 0.1284 - val_accuracy: 0.9194 - val_loss: 0.1467\n",
      "Epoch 54/58\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9668 - loss: 0.1214 - val_accuracy: 0.9516 - val_loss: 0.1315\n",
      "Epoch 55/58\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9561 - loss: 0.1283 - val_accuracy: 0.9355 - val_loss: 0.1320\n",
      "Epoch 56/58\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9616 - loss: 0.1171 - val_accuracy: 0.9194 - val_loss: 0.1467\n",
      "Epoch 57/58\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9726 - loss: 0.1192 - val_accuracy: 0.9194 - val_loss: 0.1396\n",
      "Epoch 58/58\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9698 - loss: 0.1169 - val_accuracy: 0.9516 - val_loss: 0.1234\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Evaluation =====\n",
      "Accuracy: 0.8736\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6667    0.1176    0.2000        17\n",
      "           1     0.7049    0.9556    0.8113        45\n",
      "           2     0.9897    0.9505    0.9697       101\n",
      "           3     0.8462    1.0000    0.9167        11\n",
      "\n",
      "    accuracy                         0.8736       174\n",
      "   macro avg     0.8019    0.7559    0.7244       174\n",
      "weighted avg     0.8754    0.8736    0.8502       174\n",
      "\n",
      "[[ 2 15  0  0]\n",
      " [ 1 43  1  0]\n",
      " [ 0  3 96  2]\n",
      " [ 0  0  0 11]]\n",
      "\n",
      "===== Model Metrics =====\n",
      "Training Time: 9.37s\n",
      "Testing Time (total): 0.26s\n",
      "Inference/sample: 0.001497s\n",
      "Model Size: 0.15 MB\n",
      "Trainable Params: 10,606\n",
      "RAM Usage: 954.82 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import psutil\n",
    "import warnings\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from bayes_opt import BayesianOptimization\n",
    "from scipy.fftpack import fft\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 1. Load and Preprocess Data\n",
    "def load_and_preprocess_data(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df.columns = df.columns.str.replace('\"', '').str.strip()\n",
    "    if 'TIME' not in df.columns:\n",
    "        raise KeyError(\"Column 'TIME' missing.\")\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'], errors='coerce')\n",
    "    df.dropna(subset=['TIME'], inplace=True)\n",
    "    df.set_index('TIME', inplace=True)\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "    target_col = 'fenologia_h1'\n",
    "    if target_col not in df.columns:\n",
    "        target_col = 'target'\n",
    "    if target_col not in df.columns:\n",
    "        raise KeyError(\"Target column not found.\")\n",
    "\n",
    "    df[target_col].interpolate(method='linear', inplace=True)\n",
    "    df[target_col] = df[target_col].astype(int)\n",
    "    df[target_col] = df[target_col] - 1\n",
    "\n",
    "    for i in range(1, 4):\n",
    "        df[f'{target_col}_lag{i}'] = df[target_col].shift(i)\n",
    "    for window in [3, 6]:\n",
    "        df[f'{target_col}_roll_mean_{window}'] = df[target_col].rolling(window).mean()\n",
    "        df[f'{target_col}_roll_std_{window}'] = df[target_col].rolling(window).std()\n",
    "\n",
    "\n",
    "\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "    return df, target_col\n",
    "\n",
    "# 2. Feature Scaling\n",
    "def feature_scaling(df, target_col):\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    return X_scaled, y, scaler\n",
    "\n",
    "# 3. MLP Model\n",
    "def build_mlp_model(input_shape, num_classes=4, num_units=64, dropout_rate=0.2):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Input(shape=(input_shape,)))\n",
    "    model.add(layers.Dense(num_units, activation='relu'))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    model.add(layers.Dense(num_units // 2, activation='relu'))\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "# 4. Objective Function\n",
    "def objective_function(num_units, dropout_rate, epochs, batch_size, X_train, y_train):\n",
    "    model = build_mlp_model(\n",
    "        input_shape=X_train.shape[1],\n",
    "        num_classes=4,\n",
    "        num_units=int(num_units),\n",
    "        dropout_rate=dropout_rate\n",
    "    )\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=int(epochs),\n",
    "        batch_size=int(batch_size),\n",
    "        validation_split=0.2,\n",
    "        verbose=0\n",
    "    )\n",
    "    return np.max(history.history['val_accuracy'])\n",
    "\n",
    "# 5. Hyperparameter Optimization\n",
    "def optimize_hyperparameters(X_train, y_train):\n",
    "    pbounds = {\n",
    "        'num_units': (32, 128),\n",
    "        'dropout_rate': (0.1, 0.5),\n",
    "        'epochs': (30, 60),\n",
    "        'batch_size': (16, 64)\n",
    "    }\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=lambda num_units, dropout_rate, epochs, batch_size: objective_function(\n",
    "            num_units, dropout_rate, epochs, batch_size, X_train, y_train\n",
    "        ),\n",
    "        pbounds=pbounds,\n",
    "        random_state=42,\n",
    "        verbose=2\n",
    "    )\n",
    "    optimizer.maximize(init_points=5, n_iter=15)\n",
    "    return optimizer.max['params']\n",
    "\n",
    "# 6. Train and Evaluate\n",
    "def train_and_evaluate_model(X_train, X_test, y_train, y_test, best_params):\n",
    "    model = build_mlp_model(\n",
    "        input_shape=X_train.shape[1],\n",
    "        num_classes=4,\n",
    "        num_units=int(best_params['num_units']),\n",
    "        dropout_rate=float(best_params['dropout_rate'])\n",
    "    )\n",
    "    start = time.time()\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=int(best_params['epochs']),\n",
    "        batch_size=int(best_params['batch_size']),\n",
    "        validation_split=0.2,\n",
    "        verbose=1\n",
    "    )\n",
    "    training_time = time.time() - start\n",
    "\n",
    "    start = time.time()\n",
    "    y_pred = model.predict(X_test).argmax(axis=1)\n",
    "    testing_time = time.time() - start\n",
    "    inference_time = testing_time / len(X_test)\n",
    "\n",
    "    print(\"\\n===== Evaluation =====\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(classification_report(y_test, y_pred, digits=4))\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    model.save(\"temp_model.h5\")\n",
    "    model_size = os.path.getsize(\"temp_model.h5\") / (1024 ** 2)\n",
    "    os.remove(\"temp_model.h5\")\n",
    "\n",
    "    total_params = np.sum([np.prod(v.shape) for v in model.trainable_weights])\n",
    "    ram_usage = psutil.Process(os.getpid()).memory_info().rss / (1024 ** 2)\n",
    "\n",
    "    print(\"\\n===== Model Metrics =====\")\n",
    "    print(f\"Training Time: {training_time:.2f}s\")\n",
    "    print(f\"Testing Time (total): {testing_time:.2f}s\")\n",
    "    print(f\"Inference/sample: {inference_time:.6f}s\")\n",
    "    print(f\"Model Size: {model_size:.2f} MB\")\n",
    "    print(f\"Trainable Params: {total_params:,}\")\n",
    "    print(f\"RAM Usage: {ram_usage:.2f} MB\")\n",
    "\n",
    "    return model, history\n",
    "\n",
    "# 7. Main\n",
    "def main():\n",
    "    train_path = r\"E:\\Abroad period research\\Phenology datasets\\PHENOLOGY_H1\\Cordoba_train.csv\"\n",
    "    test_path = r\"E:\\Abroad period research\\Phenology datasets\\PHENOLOGY_H1\\Cordoba_test.csv\"\n",
    "\n",
    "    df_train, target_col = load_and_preprocess_data(train_path)\n",
    "    X_train, y_train, _ = feature_scaling(df_train, target_col)\n",
    "\n",
    "    df_test, _ = load_and_preprocess_data(test_path)\n",
    "    X_test, y_test, _ = feature_scaling(df_test, target_col)\n",
    "\n",
    "    print(\"\\n Optimizing Hyperparameters...\")\n",
    "    best_params = optimize_hyperparameters(X_train, y_train)\n",
    "    print(\"\\n Best Hyperparameters:\", best_params)\n",
    "\n",
    "    print(\"\\n Training Final MLP Model...\")\n",
    "    model, history = train_and_evaluate_model(X_train, X_test, y_train, y_test, best_params)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jaen Dataset MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Optimizing Hyperparameters...\n",
      "|   iter    |  target   | batch_... | dropou... |  epochs   | num_units |\n",
      "-------------------------------------------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.96     \u001b[39m | \u001b[39m33.98    \u001b[39m | \u001b[39m0.4803   \u001b[39m | \u001b[39m51.96    \u001b[39m | \u001b[39m89.47    \u001b[39m |\n",
      "| \u001b[39m2        \u001b[39m | \u001b[39m0.96     \u001b[39m | \u001b[39m23.49    \u001b[39m | \u001b[39m0.1624   \u001b[39m | \u001b[39m31.74    \u001b[39m | \u001b[39m115.2    \u001b[39m |\n",
      "| \u001b[39m3        \u001b[39m | \u001b[39m0.92     \u001b[39m | \u001b[39m44.85    \u001b[39m | \u001b[39m0.3832   \u001b[39m | \u001b[39m30.62    \u001b[39m | \u001b[39m125.1    \u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m0.86     \u001b[39m | \u001b[39m55.96    \u001b[39m | \u001b[39m0.1849   \u001b[39m | \u001b[39m35.45    \u001b[39m | \u001b[39m49.61    \u001b[39m |\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m0.92     \u001b[39m | \u001b[39m30.6     \u001b[39m | \u001b[39m0.3099   \u001b[39m | \u001b[39m42.96    \u001b[39m | \u001b[39m59.96    \u001b[39m |\n",
      "| \u001b[35m6        \u001b[39m | \u001b[35m1.0      \u001b[39m | \u001b[35m16.71    \u001b[39m | \u001b[35m0.4236   \u001b[39m | \u001b[35m58.15    \u001b[39m | \u001b[35m107.8    \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m17.5     \u001b[39m | \u001b[39m0.2856   \u001b[39m | \u001b[39m59.64    \u001b[39m | \u001b[39m125.8    \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m18.61    \u001b[39m | \u001b[39m0.1896   \u001b[39m | \u001b[39m59.15    \u001b[39m | \u001b[39m107.9    \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m16.02    \u001b[39m | \u001b[39m0.4024   \u001b[39m | \u001b[39m59.33    \u001b[39m | \u001b[39m93.7     \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m0.98     \u001b[39m | \u001b[39m17.07    \u001b[39m | \u001b[39m0.4931   \u001b[39m | \u001b[39m59.86    \u001b[39m | \u001b[39m120.1    \u001b[39m |\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m37.68    \u001b[39m | \u001b[39m0.349    \u001b[39m | \u001b[39m59.9     \u001b[39m | \u001b[39m127.7    \u001b[39m |\n",
      "| \u001b[39m12       \u001b[39m | \u001b[39m0.96     \u001b[39m | \u001b[39m63.69    \u001b[39m | \u001b[39m0.2096   \u001b[39m | \u001b[39m59.96    \u001b[39m | \u001b[39m111.4    \u001b[39m |\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m0.94     \u001b[39m | \u001b[39m16.03    \u001b[39m | \u001b[39m0.3871   \u001b[39m | \u001b[39m33.75    \u001b[39m | \u001b[39m91.33    \u001b[39m |\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m17.24    \u001b[39m | \u001b[39m0.2042   \u001b[39m | \u001b[39m45.03    \u001b[39m | \u001b[39m127.7    \u001b[39m |\n",
      "| \u001b[39m15       \u001b[39m | \u001b[39m0.96     \u001b[39m | \u001b[39m18.3     \u001b[39m | \u001b[39m0.1748   \u001b[39m | \u001b[39m59.76    \u001b[39m | \u001b[39m32.26    \u001b[39m |\n",
      "| \u001b[39m16       \u001b[39m | \u001b[39m0.94     \u001b[39m | \u001b[39m16.47    \u001b[39m | \u001b[39m0.1417   \u001b[39m | \u001b[39m33.62    \u001b[39m | \u001b[39m33.46    \u001b[39m |\n",
      "| \u001b[39m17       \u001b[39m | \u001b[39m0.98     \u001b[39m | \u001b[39m27.11    \u001b[39m | \u001b[39m0.4395   \u001b[39m | \u001b[39m50.26    \u001b[39m | \u001b[39m127.6    \u001b[39m |\n",
      "| \u001b[39m18       \u001b[39m | \u001b[39m0.96     \u001b[39m | \u001b[39m38.08    \u001b[39m | \u001b[39m0.4733   \u001b[39m | \u001b[39m59.14    \u001b[39m | \u001b[39m110.2    \u001b[39m |\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m0.94     \u001b[39m | \u001b[39m62.79    \u001b[39m | \u001b[39m0.4373   \u001b[39m | \u001b[39m59.92    \u001b[39m | \u001b[39m127.9    \u001b[39m |\n",
      "| \u001b[39m20       \u001b[39m | \u001b[39m0.92     \u001b[39m | \u001b[39m63.17    \u001b[39m | \u001b[39m0.4601   \u001b[39m | \u001b[39m59.66    \u001b[39m | \u001b[39m86.01    \u001b[39m |\n",
      "=========================================================================\n",
      "\n",
      " Best Hyperparameters: {'batch_size': 16.711847796721976, 'dropout_rate': 0.4236412867292184, 'epochs': 58.15386159374006, 'num_units': 107.77284311438241}\n",
      "\n",
      " Training Final MLP Model...\n",
      "Epoch 1/58\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.3992 - loss: 1.2956 - val_accuracy: 0.6000 - val_loss: 1.0359\n",
      "Epoch 2/58\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5733 - loss: 1.0649 - val_accuracy: 0.6000 - val_loss: 0.9966\n",
      "Epoch 3/58\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5784 - loss: 0.9922 - val_accuracy: 0.6200 - val_loss: 0.9187\n",
      "Epoch 4/58\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6353 - loss: 0.9107 - val_accuracy: 0.6400 - val_loss: 0.8010\n",
      "Epoch 5/58\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6946 - loss: 0.7668 - val_accuracy: 0.6800 - val_loss: 0.6769\n",
      "Epoch 6/58\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7766 - loss: 0.6511 - val_accuracy: 0.7200 - val_loss: 0.5810\n",
      "Epoch 7/58\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7953 - loss: 0.5699 - val_accuracy: 0.7800 - val_loss: 0.4897\n",
      "Epoch 8/58\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8530 - loss: 0.4617 - val_accuracy: 0.8000 - val_loss: 0.4220\n",
      "Epoch 9/58\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8588 - loss: 0.3995 - val_accuracy: 0.8200 - val_loss: 0.3673\n",
      "Epoch 10/58\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8608 - loss: 0.3455 - val_accuracy: 0.8200 - val_loss: 0.3211\n",
      "Epoch 11/58\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8452 - loss: 0.3321 - val_accuracy: 0.8400 - val_loss: 0.2946\n",
      "Epoch 12/58\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8615 - loss: 0.3124 - val_accuracy: 0.8600 - val_loss: 0.2587\n",
      "Epoch 13/58\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8515 - loss: 0.3061 - val_accuracy: 0.8800 - val_loss: 0.2408\n",
      "Epoch 14/58\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8698 - loss: 0.3153 - val_accuracy: 0.9200 - val_loss: 0.2160\n",
      "Epoch 15/58\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8851 - loss: 0.2812 - val_accuracy: 0.8800 - val_loss: 0.2278\n",
      "Epoch 16/58\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8934 - loss: 0.2981 - val_accuracy: 0.9200 - val_loss: 0.1938\n",
      "Epoch 17/58\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9112 - loss: 0.2552 - val_accuracy: 0.9200 - val_loss: 0.1863\n",
      "Epoch 18/58\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9235 - loss: 0.2108 - val_accuracy: 0.9200 - val_loss: 0.1874\n",
      "Epoch 19/58\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9026 - loss: 0.2244 - val_accuracy: 0.9200 - val_loss: 0.1692\n",
      "Epoch 20/58\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9468 - loss: 0.1717 - val_accuracy: 0.9200 - val_loss: 0.1684\n",
      "Epoch 21/58\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9292 - loss: 0.1935 - val_accuracy: 0.9400 - val_loss: 0.1501\n",
      "Epoch 22/58\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9549 - loss: 0.1748 - val_accuracy: 0.9400 - val_loss: 0.1613\n",
      "Epoch 23/58\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9451 - loss: 0.1592 - val_accuracy: 0.9200 - val_loss: 0.1601\n",
      "Epoch 24/58\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9464 - loss: 0.1481 - val_accuracy: 0.9400 - val_loss: 0.1490\n",
      "Epoch 25/58\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9033 - loss: 0.1945 - val_accuracy: 0.9400 - val_loss: 0.1364\n",
      "Epoch 26/58\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9328 - loss: 0.1782 - val_accuracy: 0.9400 - val_loss: 0.1361\n",
      "Epoch 27/58\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9468 - loss: 0.1555 - val_accuracy: 0.9400 - val_loss: 0.1315\n",
      "Epoch 28/58\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9506 - loss: 0.1589 - val_accuracy: 0.9400 - val_loss: 0.1309\n",
      "Epoch 29/58\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9328 - loss: 0.1574 - val_accuracy: 0.9400 - val_loss: 0.1222\n",
      "Epoch 30/58\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9678 - loss: 0.1328 - val_accuracy: 0.9400 - val_loss: 0.1262\n",
      "Epoch 31/58\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9573 - loss: 0.1228 - val_accuracy: 0.9400 - val_loss: 0.1155\n",
      "Epoch 32/58\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9471 - loss: 0.1403 - val_accuracy: 0.9400 - val_loss: 0.1134\n",
      "Epoch 33/58\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9650 - loss: 0.1161 - val_accuracy: 0.9400 - val_loss: 0.1046\n",
      "Epoch 34/58\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9916 - loss: 0.0811 - val_accuracy: 0.9600 - val_loss: 0.0944\n",
      "Epoch 35/58\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9553 - loss: 0.1370 - val_accuracy: 0.9400 - val_loss: 0.0954\n",
      "Epoch 36/58\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9640 - loss: 0.1334 - val_accuracy: 0.9600 - val_loss: 0.0984\n",
      "Epoch 37/58\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9435 - loss: 0.1113 - val_accuracy: 0.9400 - val_loss: 0.1016\n",
      "Epoch 38/58\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9650 - loss: 0.0793 - val_accuracy: 0.9600 - val_loss: 0.0912\n",
      "Epoch 39/58\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9269 - loss: 0.1267 - val_accuracy: 0.9600 - val_loss: 0.0862\n",
      "Epoch 40/58\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9749 - loss: 0.0857 - val_accuracy: 0.9400 - val_loss: 0.0880\n",
      "Epoch 41/58\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9649 - loss: 0.1028 - val_accuracy: 0.9600 - val_loss: 0.0828\n",
      "Epoch 42/58\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9569 - loss: 0.1126 - val_accuracy: 0.9400 - val_loss: 0.0904\n",
      "Epoch 43/58\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9718 - loss: 0.0789 - val_accuracy: 0.9400 - val_loss: 0.0976\n",
      "Epoch 44/58\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9488 - loss: 0.0961 - val_accuracy: 0.9600 - val_loss: 0.0800\n",
      "Epoch 45/58\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9693 - loss: 0.0784 - val_accuracy: 0.9800 - val_loss: 0.0685\n",
      "Epoch 46/58\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9616 - loss: 0.0846 - val_accuracy: 0.9600 - val_loss: 0.0813\n",
      "Epoch 47/58\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9601 - loss: 0.0909 - val_accuracy: 0.9600 - val_loss: 0.0797\n",
      "Epoch 48/58\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9696 - loss: 0.0788 - val_accuracy: 0.9400 - val_loss: 0.0844\n",
      "Epoch 49/58\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9587 - loss: 0.0801 - val_accuracy: 0.9600 - val_loss: 0.0663\n",
      "Epoch 50/58\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9756 - loss: 0.0880 - val_accuracy: 0.9800 - val_loss: 0.0623\n",
      "Epoch 51/58\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9806 - loss: 0.0631 - val_accuracy: 0.9800 - val_loss: 0.0609\n",
      "Epoch 52/58\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9552 - loss: 0.0766 - val_accuracy: 0.9800 - val_loss: 0.0579\n",
      "Epoch 53/58\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9767 - loss: 0.0790 - val_accuracy: 0.9800 - val_loss: 0.0564\n",
      "Epoch 54/58\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9786 - loss: 0.0742 - val_accuracy: 0.9600 - val_loss: 0.0629\n",
      "Epoch 55/58\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9805 - loss: 0.0598 - val_accuracy: 0.9800 - val_loss: 0.0604\n",
      "Epoch 56/58\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9886 - loss: 0.0674 - val_accuracy: 0.9800 - val_loss: 0.0563\n",
      "Epoch 57/58\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9424 - loss: 0.0896 - val_accuracy: 0.9800 - val_loss: 0.0532\n",
      "Epoch 58/58\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9863 - loss: 0.0527 - val_accuracy: 0.9800 - val_loss: 0.0513\n",
      "WARNING:tensorflow:5 out of the last 15 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000002307B4C13A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 15 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000002307B4C13A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Evaluation =====\n",
      "Accuracy: 0.9565\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.6154    0.7619        13\n",
      "           1     0.9038    0.9400    0.9216        50\n",
      "           2     0.9758    1.0000    0.9878       121\n",
      "\n",
      "    accuracy                         0.9565       184\n",
      "   macro avg     0.9599    0.8518    0.8904       184\n",
      "weighted avg     0.9580    0.9565    0.9538       184\n",
      "\n",
      "[[  8   5   0]\n",
      " [  0  47   3]\n",
      " [  0   0 121]]\n",
      "\n",
      "===== Model Metrics =====\n",
      "Training Time: 6.66s\n",
      "Testing Time (total): 0.17s\n",
      "Inference/sample: 0.000900s\n",
      "Model Size: 0.13 MB\n",
      "Trainable Params: 8,829\n",
      "RAM Usage: 1113.25 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import psutil\n",
    "import warnings\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from bayes_opt import BayesianOptimization\n",
    "from scipy.fftpack import fft\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==============================\n",
    "# 1. Data Loading and Preprocessing\n",
    "# ==============================\n",
    "def load_and_preprocess_data(filepath):\n",
    "    df = pd.read_csv(filepath, delimiter=\",\", low_memory=False)\n",
    "    df.columns = df.columns.str.replace('\"', '').str.strip()\n",
    "\n",
    "    if 'TIME' not in df.columns:\n",
    "        raise KeyError(\"The column 'TIME' does not exist in the dataset.\")\n",
    "\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'], errors='coerce')\n",
    "    df.dropna(subset=['TIME'], inplace=True)\n",
    "    df.set_index('TIME', inplace=True)\n",
    "\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "    target_col = 'fenologia_h1'  # Change target column to 'fenologia_h1' for classification\n",
    "    if target_col not in df.columns:\n",
    "        raise KeyError(f\"The target column '{target_col}' does not exist.\")\n",
    "\n",
    "    df[target_col].interpolate(method='linear', inplace=True)\n",
    "\n",
    "    # Subtract 1 from the target column to make it 0-indexed (0, 1, 2, 3)\n",
    "    df[target_col] = df[target_col] - 1\n",
    "\n",
    "    # Lag features\n",
    "    for i in range(1, 4):\n",
    "        df[f'{target_col}_lag{i}'] = df[target_col].shift(i)\n",
    "\n",
    "    # Rolling statistics\n",
    "    for window in [3, 6]:\n",
    "        df[f'{target_col}_roll_mean_{window}'] = df[target_col].rolling(window).mean()\n",
    "        df[f'{target_col}_roll_std_{window}'] = df[target_col].rolling(window).std()\n",
    "\n",
    "    # Time-based features for weekly data\n",
    "    df['month'] = df.index.month                        # Useful for seasonal crop cycles\n",
    "    df['weekofyear'] = df.index.isocalendar().week      # Week number in the year (1–52)\n",
    "    df['year'] = df.index.isocalendar().year            # Optional: if data spans multiple years\n",
    "\n",
    "    # Smooth cyclical encoding for week-of-year (annual periodicity)\n",
    "    df['sin_week'] = np.sin(2 * np.pi * df['weekofyear'] / 52)\n",
    "    df['cos_week'] = np.cos(2 * np.pi * df['weekofyear'] / 52)\n",
    "\n",
    "    # Additional Feature 1: Exponential Moving Average (EMA)\n",
    "    df['EMA_3'] = df[target_col].ewm(span=3, adjust=False).mean()  # EMA with span 3\n",
    "    df['EMA_6'] = df[target_col].ewm(span=6, adjust=False).mean()  # EMA with span 6\n",
    "\n",
    "    # Additional Feature 2: Rolling Correlation\n",
    "    # We compute the correlation between the target and 'month' as an example\n",
    "    df['correlation_target_month'] = df[target_col].rolling(window=6).corr(df['month'])\n",
    "    df['correlation_target_week'] = df[target_col].rolling(window=6).corr(df['weekofyear'])\n",
    "\n",
    "    # Additional Feature 3: Fourier Transform (FFT) Features\n",
    "    # Apply FFT to the target variable and extract real and imaginary components\n",
    "    # fft_values = fft(df[target_col].dropna().values)\n",
    "    # fft_real = np.real(fft_values)[:len(df[target_col])]\n",
    "    # fft_imag = np.imag(fft_values)[:len(df[target_col])]\n",
    "    \n",
    "    # df['fft_real'] = np.concatenate([fft_real, np.nan * np.ones(len(df) - len(fft_real))], axis=0)\n",
    "    # df['fft_imag'] = np.concatenate([fft_imag, np.nan * np.ones(len(df) - len(fft_imag))], axis=0)\n",
    "\n",
    "    # Fill missing values\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "\n",
    "    return df, target_col\n",
    "\n",
    "# 2. Feature Scaling\n",
    "def feature_scaling(df, target_col):\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    return X_scaled, y, scaler\n",
    "\n",
    "# 3. MLP Model\n",
    "def build_mlp_model(input_shape, num_classes=4, num_units=64, dropout_rate=0.2):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Input(shape=(input_shape,)))\n",
    "    model.add(layers.Dense(num_units, activation='relu'))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    model.add(layers.Dense(num_units // 2, activation='relu'))\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "# 4. Objective Function\n",
    "def objective_function(num_units, dropout_rate, epochs, batch_size, X_train, y_train):\n",
    "    model = build_mlp_model(\n",
    "        input_shape=X_train.shape[1],\n",
    "        num_classes=4,\n",
    "        num_units=int(num_units),\n",
    "        dropout_rate=dropout_rate\n",
    "    )\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=int(epochs),\n",
    "        batch_size=int(batch_size),\n",
    "        validation_split=0.2,\n",
    "        verbose=0\n",
    "    )\n",
    "    return np.max(history.history['val_accuracy'])\n",
    "\n",
    "# 5. Hyperparameter Optimization\n",
    "def optimize_hyperparameters(X_train, y_train):\n",
    "    pbounds = {\n",
    "        'num_units': (32, 128),\n",
    "        'dropout_rate': (0.1, 0.5),\n",
    "        'epochs': (30, 60),\n",
    "        'batch_size': (16, 64)\n",
    "    }\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=lambda num_units, dropout_rate, epochs, batch_size: objective_function(\n",
    "            num_units, dropout_rate, epochs, batch_size, X_train, y_train\n",
    "        ),\n",
    "        pbounds=pbounds,\n",
    "        random_state=42,\n",
    "        verbose=2\n",
    "    )\n",
    "    optimizer.maximize(init_points=5, n_iter=15)\n",
    "    return optimizer.max['params']\n",
    "\n",
    "# 6. Train and Evaluate\n",
    "def train_and_evaluate_model(X_train, X_test, y_train, y_test, best_params):\n",
    "    model = build_mlp_model(\n",
    "        input_shape=X_train.shape[1],\n",
    "        num_classes=4,\n",
    "        num_units=int(best_params['num_units']),\n",
    "        dropout_rate=float(best_params['dropout_rate'])\n",
    "    )\n",
    "    start = time.time()\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=int(best_params['epochs']),\n",
    "        batch_size=int(best_params['batch_size']),\n",
    "        validation_split=0.2,\n",
    "        verbose=1\n",
    "    )\n",
    "    training_time = time.time() - start\n",
    "\n",
    "    start = time.time()\n",
    "    y_pred = model.predict(X_test).argmax(axis=1)\n",
    "    testing_time = time.time() - start\n",
    "    inference_time = testing_time / len(X_test)\n",
    "\n",
    "    print(\"\\n===== Evaluation =====\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(classification_report(y_test, y_pred, digits=4))\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    model.save(\"temp_model.h5\")\n",
    "    model_size = os.path.getsize(\"temp_model.h5\") / (1024 ** 2)\n",
    "    os.remove(\"temp_model.h5\")\n",
    "\n",
    "    total_params = np.sum([np.prod(v.shape) for v in model.trainable_weights])\n",
    "    ram_usage = psutil.Process(os.getpid()).memory_info().rss / (1024 ** 2)\n",
    "\n",
    "    print(\"\\n===== Model Metrics =====\")\n",
    "    print(f\"Training Time: {training_time:.2f}s\")\n",
    "    print(f\"Testing Time (total): {testing_time:.2f}s\")\n",
    "    print(f\"Inference/sample: {inference_time:.6f}s\")\n",
    "    print(f\"Model Size: {model_size:.2f} MB\")\n",
    "    print(f\"Trainable Params: {total_params:,}\")\n",
    "    print(f\"RAM Usage: {ram_usage:.2f} MB\")\n",
    "\n",
    "    return model, history\n",
    "\n",
    "# 7. Main\n",
    "def main():\n",
    "    train_path = r\"E:\\Abroad period research\\Phenology datasets\\PHENOLOGY_H1\\Jaen_train.csv\"\n",
    "    test_path = r\"E:\\Abroad period research\\Phenology datasets\\PHENOLOGY_H1\\Jaen_test.csv\"\n",
    "\n",
    "    df_train, target_col = load_and_preprocess_data(train_path)\n",
    "    X_train, y_train, _ = feature_scaling(df_train, target_col)\n",
    "\n",
    "    df_test, _ = load_and_preprocess_data(test_path)\n",
    "    X_test, y_test, _ = feature_scaling(df_test, target_col)\n",
    "\n",
    "    print(\"\\n Optimizing Hyperparameters...\")\n",
    "    best_params = optimize_hyperparameters(X_train, y_train)\n",
    "    print(\"\\n Best Hyperparameters:\", best_params)\n",
    "\n",
    "    print(\"\\n Training Final MLP Model...\")\n",
    "    model, history = train_and_evaluate_model(X_train, X_test, y_train, y_test, best_params)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sevilla MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Optimizing Hyperparameters...\n",
      "|   iter    |  target   | batch_... | dropou... |  epochs   | num_units |\n",
      "-------------------------------------------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m33.98    \u001b[39m | \u001b[39m0.4803   \u001b[39m | \u001b[39m51.96    \u001b[39m | \u001b[39m89.47    \u001b[39m |\n",
      "| \u001b[39m2        \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m23.49    \u001b[39m | \u001b[39m0.1624   \u001b[39m | \u001b[39m31.74    \u001b[39m | \u001b[39m115.2    \u001b[39m |\n",
      "| \u001b[39m3        \u001b[39m | \u001b[39m0.9857   \u001b[39m | \u001b[39m44.85    \u001b[39m | \u001b[39m0.3832   \u001b[39m | \u001b[39m30.62    \u001b[39m | \u001b[39m125.1    \u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m0.9571   \u001b[39m | \u001b[39m55.96    \u001b[39m | \u001b[39m0.1849   \u001b[39m | \u001b[39m35.45    \u001b[39m | \u001b[39m49.61    \u001b[39m |\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m0.9857   \u001b[39m | \u001b[39m30.6     \u001b[39m | \u001b[39m0.3099   \u001b[39m | \u001b[39m42.96    \u001b[39m | \u001b[39m59.96    \u001b[39m |\n",
      "| \u001b[39m6        \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m16.71    \u001b[39m | \u001b[39m0.4236   \u001b[39m | \u001b[39m58.15    \u001b[39m | \u001b[39m107.8    \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m17.01    \u001b[39m | \u001b[39m0.1588   \u001b[39m | \u001b[39m30.56    \u001b[39m | \u001b[39m88.97    \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m0.9857   \u001b[39m | \u001b[39m24.08    \u001b[39m | \u001b[39m0.3354   \u001b[39m | \u001b[39m32.56    \u001b[39m | \u001b[39m116.3    \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m0.9857   \u001b[39m | \u001b[39m62.6     \u001b[39m | \u001b[39m0.3838   \u001b[39m | \u001b[39m59.82    \u001b[39m | \u001b[39m67.9     \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m0.9857   \u001b[39m | \u001b[39m38.63    \u001b[39m | \u001b[39m0.1642   \u001b[39m | \u001b[39m54.06    \u001b[39m | \u001b[39m41.26    \u001b[39m |\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m0.9857   \u001b[39m | \u001b[39m61.04    \u001b[39m | \u001b[39m0.3825   \u001b[39m | \u001b[39m43.5     \u001b[39m | \u001b[39m127.9    \u001b[39m |\n",
      "| \u001b[39m12       \u001b[39m | \u001b[39m0.9714   \u001b[39m | \u001b[39m53.21    \u001b[39m | \u001b[39m0.4529   \u001b[39m | \u001b[39m36.82    \u001b[39m | \u001b[39m67.75    \u001b[39m |\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m25.29    \u001b[39m | \u001b[39m0.3485   \u001b[39m | \u001b[39m37.45    \u001b[39m | \u001b[39m105.7    \u001b[39m |\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m0.9857   \u001b[39m | \u001b[39m16.3     \u001b[39m | \u001b[39m0.2837   \u001b[39m | \u001b[39m59.3     \u001b[39m | \u001b[39m82.57    \u001b[39m |\n",
      "| \u001b[39m15       \u001b[39m | \u001b[39m0.9143   \u001b[39m | \u001b[39m51.64    \u001b[39m | \u001b[39m0.31     \u001b[39m | \u001b[39m39.01    \u001b[39m | \u001b[39m33.89    \u001b[39m |\n",
      "| \u001b[39m16       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m23.09    \u001b[39m | \u001b[39m0.3106   \u001b[39m | \u001b[39m32.3     \u001b[39m | \u001b[39m111.5    \u001b[39m |\n",
      "| \u001b[39m17       \u001b[39m | \u001b[39m0.9857   \u001b[39m | \u001b[39m17.98    \u001b[39m | \u001b[39m0.4513   \u001b[39m | \u001b[39m31.46    \u001b[39m | \u001b[39m115.9    \u001b[39m |\n",
      "| \u001b[39m18       \u001b[39m | \u001b[39m0.9857   \u001b[39m | \u001b[39m26.41    \u001b[39m | \u001b[39m0.3674   \u001b[39m | \u001b[39m31.43    \u001b[39m | \u001b[39m105.8    \u001b[39m |\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m19.45    \u001b[39m | \u001b[39m0.4598   \u001b[39m | \u001b[39m37.23    \u001b[39m | \u001b[39m105.8    \u001b[39m |\n",
      "| \u001b[39m20       \u001b[39m | \u001b[39m0.9857   \u001b[39m | \u001b[39m22.42    \u001b[39m | \u001b[39m0.3138   \u001b[39m | \u001b[39m41.18    \u001b[39m | \u001b[39m102.0    \u001b[39m |\n",
      "=========================================================================\n",
      "\n",
      " Best Hyperparameters: {'batch_size': 33.977925704673396, 'dropout_rate': 0.4802857225639665, 'epochs': 51.95981825434215, 'num_units': 89.47121448291551}\n",
      "\n",
      " Training Final MLP Model...\n",
      "Epoch 1/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.3756 - loss: 1.3193 - val_accuracy: 0.5143 - val_loss: 1.1195\n",
      "Epoch 2/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5008 - loss: 1.1427 - val_accuracy: 0.5143 - val_loss: 1.0243\n",
      "Epoch 3/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5382 - loss: 1.1008 - val_accuracy: 0.6000 - val_loss: 0.9546\n",
      "Epoch 4/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5442 - loss: 0.9987 - val_accuracy: 0.6714 - val_loss: 0.8840\n",
      "Epoch 5/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6194 - loss: 0.9398 - val_accuracy: 0.6857 - val_loss: 0.8123\n",
      "Epoch 6/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6250 - loss: 0.9170 - val_accuracy: 0.7714 - val_loss: 0.7367\n",
      "Epoch 7/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6786 - loss: 0.8480 - val_accuracy: 0.8286 - val_loss: 0.6449\n",
      "Epoch 8/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7554 - loss: 0.7484 - val_accuracy: 0.9000 - val_loss: 0.5606\n",
      "Epoch 9/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8152 - loss: 0.6504 - val_accuracy: 0.9429 - val_loss: 0.4789\n",
      "Epoch 10/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7954 - loss: 0.6328 - val_accuracy: 0.9571 - val_loss: 0.4091\n",
      "Epoch 11/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8269 - loss: 0.5816 - val_accuracy: 0.9571 - val_loss: 0.3548\n",
      "Epoch 12/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8100 - loss: 0.5250 - val_accuracy: 0.9429 - val_loss: 0.3161\n",
      "Epoch 13/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8491 - loss: 0.4899 - val_accuracy: 0.9571 - val_loss: 0.2861\n",
      "Epoch 14/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8427 - loss: 0.4795 - val_accuracy: 0.9429 - val_loss: 0.2588\n",
      "Epoch 15/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8722 - loss: 0.4027 - val_accuracy: 0.9571 - val_loss: 0.2309\n",
      "Epoch 16/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8672 - loss: 0.4032 - val_accuracy: 0.9571 - val_loss: 0.2096\n",
      "Epoch 17/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8414 - loss: 0.3948 - val_accuracy: 0.9429 - val_loss: 0.1984\n",
      "Epoch 18/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8544 - loss: 0.3775 - val_accuracy: 0.9429 - val_loss: 0.1889\n",
      "Epoch 19/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8743 - loss: 0.3489 - val_accuracy: 0.9714 - val_loss: 0.1769\n",
      "Epoch 20/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9024 - loss: 0.3332 - val_accuracy: 0.9714 - val_loss: 0.1634\n",
      "Epoch 21/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8332 - loss: 0.3745 - val_accuracy: 0.9571 - val_loss: 0.1540\n",
      "Epoch 22/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8785 - loss: 0.3608 - val_accuracy: 0.9714 - val_loss: 0.1452\n",
      "Epoch 23/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9072 - loss: 0.3035 - val_accuracy: 0.9714 - val_loss: 0.1408\n",
      "Epoch 24/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8960 - loss: 0.3094 - val_accuracy: 0.9714 - val_loss: 0.1415\n",
      "Epoch 25/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8857 - loss: 0.2952 - val_accuracy: 0.9714 - val_loss: 0.1348\n",
      "Epoch 26/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8902 - loss: 0.3033 - val_accuracy: 0.9714 - val_loss: 0.1287\n",
      "Epoch 27/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8959 - loss: 0.3016 - val_accuracy: 0.9714 - val_loss: 0.1229\n",
      "Epoch 28/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9078 - loss: 0.2596 - val_accuracy: 0.9714 - val_loss: 0.1136\n",
      "Epoch 29/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9387 - loss: 0.2365 - val_accuracy: 0.9714 - val_loss: 0.1058\n",
      "Epoch 30/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9071 - loss: 0.2407 - val_accuracy: 0.9714 - val_loss: 0.1095\n",
      "Epoch 31/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9351 - loss: 0.2240 - val_accuracy: 0.9714 - val_loss: 0.1033\n",
      "Epoch 32/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9245 - loss: 0.2086 - val_accuracy: 0.9714 - val_loss: 0.0940\n",
      "Epoch 33/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9387 - loss: 0.1942 - val_accuracy: 0.9714 - val_loss: 0.0932\n",
      "Epoch 34/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8913 - loss: 0.2387 - val_accuracy: 0.9714 - val_loss: 0.0885\n",
      "Epoch 35/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9462 - loss: 0.1701 - val_accuracy: 0.9714 - val_loss: 0.0829\n",
      "Epoch 36/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9156 - loss: 0.2260 - val_accuracy: 0.9714 - val_loss: 0.0837\n",
      "Epoch 37/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9310 - loss: 0.2166 - val_accuracy: 0.9714 - val_loss: 0.0848\n",
      "Epoch 38/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9233 - loss: 0.2215 - val_accuracy: 0.9714 - val_loss: 0.0823\n",
      "Epoch 39/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9346 - loss: 0.1877 - val_accuracy: 0.9714 - val_loss: 0.0757\n",
      "Epoch 40/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9207 - loss: 0.1751 - val_accuracy: 0.9714 - val_loss: 0.0757\n",
      "Epoch 41/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9534 - loss: 0.1961 - val_accuracy: 0.9714 - val_loss: 0.0682\n",
      "Epoch 42/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9436 - loss: 0.1542 - val_accuracy: 0.9714 - val_loss: 0.0663\n",
      "Epoch 43/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9378 - loss: 0.1639 - val_accuracy: 0.9714 - val_loss: 0.0663\n",
      "Epoch 44/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9258 - loss: 0.2018 - val_accuracy: 0.9857 - val_loss: 0.0633\n",
      "Epoch 45/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9444 - loss: 0.1765 - val_accuracy: 0.9714 - val_loss: 0.0640\n",
      "Epoch 46/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9365 - loss: 0.1749 - val_accuracy: 0.9714 - val_loss: 0.0589\n",
      "Epoch 47/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9462 - loss: 0.1582 - val_accuracy: 0.9714 - val_loss: 0.0570\n",
      "Epoch 48/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9599 - loss: 0.1317 - val_accuracy: 0.9714 - val_loss: 0.0596\n",
      "Epoch 49/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9586 - loss: 0.1690 - val_accuracy: 0.9714 - val_loss: 0.0567\n",
      "Epoch 50/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9368 - loss: 0.1626 - val_accuracy: 0.9857 - val_loss: 0.0525\n",
      "Epoch 51/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9695 - loss: 0.1273 - val_accuracy: 0.9857 - val_loss: 0.0482\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000002300BB3E340> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 13 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000002300BB3E340> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Evaluation =====\n",
      "Accuracy: 0.8744\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3750    0.3000    0.3333        10\n",
      "           1     0.9054    0.8816    0.8933        76\n",
      "           2     0.9516    0.9219    0.9365       128\n",
      "           3     0.4118    0.7778    0.5385         9\n",
      "\n",
      "    accuracy                         0.8744       223\n",
      "   macro avg     0.6609    0.7203    0.6754       223\n",
      "weighted avg     0.8882    0.8744    0.8787       223\n",
      "\n",
      "[[  3   7   0   0]\n",
      " [  5  67   4   0]\n",
      " [  0   0 118  10]\n",
      " [  0   0   2   7]]\n",
      "\n",
      "===== Model Metrics =====\n",
      "Training Time: 6.05s\n",
      "Testing Time (total): 0.24s\n",
      "Inference/sample: 0.001055s\n",
      "Model Size: 0.11 MB\n",
      "Trainable Params: 6,543\n",
      "RAM Usage: 1337.34 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import psutil\n",
    "import warnings\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from bayes_opt import BayesianOptimization\n",
    "from scipy.fftpack import fft\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==============================\n",
    "# 1. Data Loading and Preprocessing\n",
    "# ==============================\n",
    "def load_and_preprocess_data(filepath):\n",
    "    df = pd.read_csv(filepath, delimiter=\",\", low_memory=False)\n",
    "    df.columns = df.columns.str.replace('\"', '').str.strip()\n",
    "\n",
    "    if 'TIME' not in df.columns:\n",
    "        raise KeyError(\"The column 'TIME' does not exist in the dataset.\")\n",
    "\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'], errors='coerce')\n",
    "    df.dropna(subset=['TIME'], inplace=True)\n",
    "    df.set_index('TIME', inplace=True)\n",
    "\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "    target_col = 'fenologia_h1'  # Change target column to 'fenologia_h1' for classification\n",
    "    if target_col not in df.columns:\n",
    "        raise KeyError(f\"The target column '{target_col}' does not exist.\")\n",
    "\n",
    "    df[target_col].interpolate(method='linear', inplace=True)\n",
    "\n",
    "    # Subtract 1 from the target column to make it 0-indexed (0, 1, 2, 3)\n",
    "    df[target_col] = df[target_col] - 1\n",
    "\n",
    "    # Lag features\n",
    "    for i in range(1, 4):\n",
    "        df[f'{target_col}_lag{i}'] = df[target_col].shift(i)\n",
    "\n",
    "    # Rolling statistics\n",
    "    for window in [3, 6]:\n",
    "        df[f'{target_col}_roll_mean_{window}'] = df[target_col].rolling(window).mean()\n",
    "        df[f'{target_col}_roll_std_{window}'] = df[target_col].rolling(window).std()\n",
    "\n",
    "    # Time-based features for weekly data\n",
    "    df['month'] = df.index.month                        # Useful for seasonal crop cycles\n",
    "    df['weekofyear'] = df.index.isocalendar().week      # Week number in the year (1–52)\n",
    "    df['year'] = df.index.isocalendar().year            # Optional: if data spans multiple years\n",
    "\n",
    "    # Smooth cyclical encoding for week-of-year (annual periodicity)\n",
    "    df['sin_week'] = np.sin(2 * np.pi * df['weekofyear'] / 52)\n",
    "    df['cos_week'] = np.cos(2 * np.pi * df['weekofyear'] / 52)\n",
    "\n",
    "    # Additional Feature 1: Exponential Moving Average (EMA)\n",
    "    df['EMA_3'] = df[target_col].ewm(span=3, adjust=False).mean()  # EMA with span 3\n",
    "    df['EMA_6'] = df[target_col].ewm(span=6, adjust=False).mean()  # EMA with span 6\n",
    "\n",
    "    # Additional Feature 2: Rolling Correlation\n",
    "    # We compute the correlation between the target and 'month' as an example\n",
    "    df['correlation_target_month'] = df[target_col].rolling(window=6).corr(df['month'])\n",
    "    df['correlation_target_week'] = df[target_col].rolling(window=6).corr(df['weekofyear'])\n",
    "\n",
    "    # Additional Feature 3: Fourier Transform (FFT) Features\n",
    "    # Apply FFT to the target variable and extract real and imaginary components\n",
    "    # fft_values = fft(df[target_col].dropna().values)\n",
    "    # fft_real = np.real(fft_values)[:len(df[target_col])]\n",
    "    # fft_imag = np.imag(fft_values)[:len(df[target_col])]\n",
    "    \n",
    "    # df['fft_real'] = np.concatenate([fft_real, np.nan * np.ones(len(df) - len(fft_real))], axis=0)\n",
    "    # df['fft_imag'] = np.concatenate([fft_imag, np.nan * np.ones(len(df) - len(fft_imag))], axis=0)\n",
    "\n",
    "    # Fill missing values\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "\n",
    "    return df, target_col\n",
    "\n",
    "# 2. Feature Scaling\n",
    "def feature_scaling(df, target_col):\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    return X_scaled, y, scaler\n",
    "\n",
    "# 3. MLP Model\n",
    "def build_mlp_model(input_shape, num_classes=4, num_units=64, dropout_rate=0.2):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Input(shape=(input_shape,)))\n",
    "    model.add(layers.Dense(num_units, activation='relu'))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    model.add(layers.Dense(num_units // 2, activation='relu'))\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "# 4. Objective Function\n",
    "def objective_function(num_units, dropout_rate, epochs, batch_size, X_train, y_train):\n",
    "    model = build_mlp_model(\n",
    "        input_shape=X_train.shape[1],\n",
    "        num_classes=4,\n",
    "        num_units=int(num_units),\n",
    "        dropout_rate=dropout_rate\n",
    "    )\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=int(epochs),\n",
    "        batch_size=int(batch_size),\n",
    "        validation_split=0.2,\n",
    "        verbose=0\n",
    "    )\n",
    "    return np.max(history.history['val_accuracy'])\n",
    "\n",
    "# 5. Hyperparameter Optimization\n",
    "def optimize_hyperparameters(X_train, y_train):\n",
    "    pbounds = {\n",
    "        'num_units': (32, 128),\n",
    "        'dropout_rate': (0.1, 0.5),\n",
    "        'epochs': (30, 60),\n",
    "        'batch_size': (16, 64)\n",
    "    }\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=lambda num_units, dropout_rate, epochs, batch_size: objective_function(\n",
    "            num_units, dropout_rate, epochs, batch_size, X_train, y_train\n",
    "        ),\n",
    "        pbounds=pbounds,\n",
    "        random_state=42,\n",
    "        verbose=2\n",
    "    )\n",
    "    optimizer.maximize(init_points=5, n_iter=15)\n",
    "    return optimizer.max['params']\n",
    "\n",
    "# 6. Train and Evaluate\n",
    "def train_and_evaluate_model(X_train, X_test, y_train, y_test, best_params):\n",
    "    model = build_mlp_model(\n",
    "        input_shape=X_train.shape[1],\n",
    "        num_classes=4,\n",
    "        num_units=int(best_params['num_units']),\n",
    "        dropout_rate=float(best_params['dropout_rate'])\n",
    "    )\n",
    "    start = time.time()\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=int(best_params['epochs']),\n",
    "        batch_size=int(best_params['batch_size']),\n",
    "        validation_split=0.2,\n",
    "        verbose=1\n",
    "    )\n",
    "    training_time = time.time() - start\n",
    "\n",
    "    start = time.time()\n",
    "    y_pred = model.predict(X_test).argmax(axis=1)\n",
    "    testing_time = time.time() - start\n",
    "    inference_time = testing_time / len(X_test)\n",
    "\n",
    "    print(\"\\n===== Evaluation =====\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(classification_report(y_test, y_pred, digits=4))\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    model.save(\"temp_model.h5\")\n",
    "    model_size = os.path.getsize(\"temp_model.h5\") / (1024 ** 2)\n",
    "    os.remove(\"temp_model.h5\")\n",
    "\n",
    "    total_params = np.sum([np.prod(v.shape) for v in model.trainable_weights])\n",
    "    ram_usage = psutil.Process(os.getpid()).memory_info().rss / (1024 ** 2)\n",
    "\n",
    "    print(\"\\n===== Model Metrics =====\")\n",
    "    print(f\"Training Time: {training_time:.2f}s\")\n",
    "    print(f\"Testing Time (total): {testing_time:.2f}s\")\n",
    "    print(f\"Inference/sample: {inference_time:.6f}s\")\n",
    "    print(f\"Model Size: {model_size:.2f} MB\")\n",
    "    print(f\"Trainable Params: {total_params:,}\")\n",
    "    print(f\"RAM Usage: {ram_usage:.2f} MB\")\n",
    "\n",
    "    return model, history\n",
    "\n",
    "# 7. Main\n",
    "def main():\n",
    "    train_path = r\"E:\\Abroad period research\\Phenology datasets\\PHENOLOGY_H1\\sevilla_train.csv\"\n",
    "    test_path = r\"E:\\Abroad period research\\Phenology datasets\\PHENOLOGY_H1\\sevilla_test.csv\"\n",
    "\n",
    "    df_train, target_col = load_and_preprocess_data(train_path)\n",
    "    X_train, y_train, _ = feature_scaling(df_train, target_col)\n",
    "\n",
    "    df_test, _ = load_and_preprocess_data(test_path)\n",
    "    X_test, y_test, _ = feature_scaling(df_test, target_col)\n",
    "\n",
    "    print(\"\\n Optimizing Hyperparameters...\")\n",
    "    best_params = optimize_hyperparameters(X_train, y_train)\n",
    "    print(\"\\n Best Hyperparameters:\", best_params)\n",
    "\n",
    "    print(\"\\n Training Final MLP Model...\")\n",
    "    model, history = train_and_evaluate_model(X_train, X_test, y_train, y_test, best_params)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
