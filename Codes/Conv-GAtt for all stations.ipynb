{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f24a52aa",
   "metadata": {},
   "source": [
    "Cadiz station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5de2719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Optimizing Hyperparameters...\n",
      "|   iter    |  target   | batch_... | dropou... |  epochs   | num_units |\n",
      "-------------------------------------------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.9692   \u001b[39m | \u001b[39m33.98    \u001b[39m | \u001b[39m0.4803   \u001b[39m | \u001b[39m86.6     \u001b[39m | \u001b[39m89.47    \u001b[39m |\n",
      "| \u001b[35m2        \u001b[39m | \u001b[35m0.9846   \u001b[39m | \u001b[35m23.49    \u001b[39m | \u001b[35m0.1624   \u001b[39m | \u001b[35m52.9     \u001b[39m | \u001b[35m115.2    \u001b[39m |\n",
      "| \u001b[39m3        \u001b[39m | \u001b[39m0.9385   \u001b[39m | \u001b[39m44.85    \u001b[39m | \u001b[39m0.3832   \u001b[39m | \u001b[39m51.03    \u001b[39m | \u001b[39m125.1    \u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m0.9231   \u001b[39m | \u001b[39m55.96    \u001b[39m | \u001b[39m0.1849   \u001b[39m | \u001b[39m59.09    \u001b[39m | \u001b[39m49.61    \u001b[39m |\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m0.9692   \u001b[39m | \u001b[39m30.6     \u001b[39m | \u001b[39m0.3099   \u001b[39m | \u001b[39m71.6     \u001b[39m | \u001b[39m59.96    \u001b[39m |\n",
      "| \u001b[39m6        \u001b[39m | \u001b[39m0.9846   \u001b[39m | \u001b[39m22.97    \u001b[39m | \u001b[39m0.3185   \u001b[39m | \u001b[39m53.09    \u001b[39m | \u001b[39m116.6    \u001b[39m |\n",
      "| \u001b[35m7        \u001b[39m | \u001b[35m1.0      \u001b[39m | \u001b[35m17.11    \u001b[39m | \u001b[35m0.4126   \u001b[39m | \u001b[35m70.21    \u001b[39m | \u001b[35m108.7    \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m16.83    \u001b[39m | \u001b[39m0.444    \u001b[39m | \u001b[39m94.02    \u001b[39m | \u001b[39m122.9    \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m0.9538   \u001b[39m | \u001b[39m17.02    \u001b[39m | \u001b[39m0.3506   \u001b[39m | \u001b[39m98.53    \u001b[39m | \u001b[39m35.15    \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m0.9692   \u001b[39m | \u001b[39m16.32    \u001b[39m | \u001b[39m0.2557   \u001b[39m | \u001b[39m51.28    \u001b[39m | \u001b[39m88.33    \u001b[39m |\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m37.68    \u001b[39m | \u001b[39m0.349    \u001b[39m | \u001b[39m99.83    \u001b[39m | \u001b[39m127.7    \u001b[39m |\n",
      "| \u001b[39m12       \u001b[39m | \u001b[39m0.9385   \u001b[39m | \u001b[39m63.12    \u001b[39m | \u001b[39m0.4684   \u001b[39m | \u001b[39m99.19    \u001b[39m | \u001b[39m123.0    \u001b[39m |\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m25.04    \u001b[39m | \u001b[39m0.1232   \u001b[39m | \u001b[39m81.64    \u001b[39m | \u001b[39m128.0    \u001b[39m |\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m16.65    \u001b[39m | \u001b[39m0.1562   \u001b[39m | \u001b[39m98.99    \u001b[39m | \u001b[39m103.9    \u001b[39m |\n",
      "| \u001b[39m15       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m30.21    \u001b[39m | \u001b[39m0.4319   \u001b[39m | \u001b[39m99.18    \u001b[39m | \u001b[39m114.1    \u001b[39m |\n",
      "| \u001b[39m16       \u001b[39m | \u001b[39m0.9385   \u001b[39m | \u001b[39m16.47    \u001b[39m | \u001b[39m0.1417   \u001b[39m | \u001b[39m56.04    \u001b[39m | \u001b[39m33.46    \u001b[39m |\n",
      "| \u001b[39m17       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m16.73    \u001b[39m | \u001b[39m0.2657   \u001b[39m | \u001b[39m99.97    \u001b[39m | \u001b[39m75.71    \u001b[39m |\n",
      "| \u001b[39m18       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m16.44    \u001b[39m | \u001b[39m0.4275   \u001b[39m | \u001b[39m81.03    \u001b[39m | \u001b[39m78.69    \u001b[39m |\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m24.81    \u001b[39m | \u001b[39m0.3108   \u001b[39m | \u001b[39m99.44    \u001b[39m | \u001b[39m127.9    \u001b[39m |\n",
      "| \u001b[39m20       \u001b[39m | \u001b[39m0.9231   \u001b[39m | \u001b[39m62.34    \u001b[39m | \u001b[39m0.287    \u001b[39m | \u001b[39m100.0    \u001b[39m | \u001b[39m33.69    \u001b[39m |\n",
      "=========================================================================\n",
      "\n",
      " Best Params: {'batch_size': 17.11070879197914, 'dropout_rate': 0.4125874984407447, 'epochs': 70.20757794979225, 'num_units': 108.73751135819677}\n",
      "\n",
      " Training Final Gate Attention CNN Model...\n",
      "Epoch 1/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.3174 - loss: 1.3269 - val_accuracy: 0.3077 - val_loss: 1.1952\n",
      "Epoch 2/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4405 - loss: 1.1589 - val_accuracy: 0.3077 - val_loss: 1.0786\n",
      "Epoch 3/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4405 - loss: 1.0694 - val_accuracy: 0.3692 - val_loss: 0.9529\n",
      "Epoch 4/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5585 - loss: 0.9778 - val_accuracy: 0.8615 - val_loss: 0.8003\n",
      "Epoch 5/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6930 - loss: 0.8331 - val_accuracy: 0.8923 - val_loss: 0.6640\n",
      "Epoch 6/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7191 - loss: 0.7140 - val_accuracy: 0.8923 - val_loss: 0.5512\n",
      "Epoch 7/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7752 - loss: 0.6158 - val_accuracy: 0.8923 - val_loss: 0.4854\n",
      "Epoch 8/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8332 - loss: 0.5216 - val_accuracy: 0.8308 - val_loss: 0.4587\n",
      "Epoch 9/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7941 - loss: 0.5985 - val_accuracy: 0.8769 - val_loss: 0.4226\n",
      "Epoch 10/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8169 - loss: 0.5321 - val_accuracy: 0.8923 - val_loss: 0.3906\n",
      "Epoch 11/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8379 - loss: 0.4874 - val_accuracy: 0.9077 - val_loss: 0.3762\n",
      "Epoch 12/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8319 - loss: 0.5084 - val_accuracy: 0.8462 - val_loss: 0.3903\n",
      "Epoch 13/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7907 - loss: 0.5021 - val_accuracy: 0.9077 - val_loss: 0.3432\n",
      "Epoch 14/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8251 - loss: 0.4398 - val_accuracy: 0.9077 - val_loss: 0.3385\n",
      "Epoch 15/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8441 - loss: 0.4237 - val_accuracy: 0.9231 - val_loss: 0.3077\n",
      "Epoch 16/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8783 - loss: 0.3639 - val_accuracy: 0.9231 - val_loss: 0.3023\n",
      "Epoch 17/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8536 - loss: 0.4321 - val_accuracy: 0.9231 - val_loss: 0.2793\n",
      "Epoch 18/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8592 - loss: 0.3866 - val_accuracy: 0.9231 - val_loss: 0.2697\n",
      "Epoch 19/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8237 - loss: 0.3701 - val_accuracy: 0.9231 - val_loss: 0.2748\n",
      "Epoch 20/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8260 - loss: 0.3791 - val_accuracy: 0.9231 - val_loss: 0.2464\n",
      "Epoch 21/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8784 - loss: 0.3224 - val_accuracy: 0.9385 - val_loss: 0.2530\n",
      "Epoch 22/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8623 - loss: 0.3459 - val_accuracy: 0.9385 - val_loss: 0.2259\n",
      "Epoch 23/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8733 - loss: 0.3354 - val_accuracy: 0.9385 - val_loss: 0.2163\n",
      "Epoch 24/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8634 - loss: 0.3264 - val_accuracy: 0.9385 - val_loss: 0.2098\n",
      "Epoch 25/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9007 - loss: 0.2524 - val_accuracy: 0.9385 - val_loss: 0.1983\n",
      "Epoch 26/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8893 - loss: 0.2693 - val_accuracy: 0.9385 - val_loss: 0.1769\n",
      "Epoch 27/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8974 - loss: 0.2965 - val_accuracy: 0.9385 - val_loss: 0.1841\n",
      "Epoch 28/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8894 - loss: 0.2650 - val_accuracy: 0.9385 - val_loss: 0.1769\n",
      "Epoch 29/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8791 - loss: 0.2978 - val_accuracy: 0.9385 - val_loss: 0.1657\n",
      "Epoch 30/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8917 - loss: 0.2809 - val_accuracy: 0.9385 - val_loss: 0.1780\n",
      "Epoch 31/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8783 - loss: 0.3191 - val_accuracy: 0.9385 - val_loss: 0.1475\n",
      "Epoch 32/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8802 - loss: 0.2728 - val_accuracy: 0.9538 - val_loss: 0.1616\n",
      "Epoch 33/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9059 - loss: 0.2571 - val_accuracy: 0.9385 - val_loss: 0.1406\n",
      "Epoch 34/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8887 - loss: 0.2328 - val_accuracy: 0.9385 - val_loss: 0.1226\n",
      "Epoch 35/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8876 - loss: 0.2481 - val_accuracy: 0.9538 - val_loss: 0.1292\n",
      "Epoch 36/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9224 - loss: 0.1943 - val_accuracy: 0.9538 - val_loss: 0.1285\n",
      "Epoch 37/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9004 - loss: 0.2500 - val_accuracy: 0.9538 - val_loss: 0.1528\n",
      "Epoch 38/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9200 - loss: 0.2605 - val_accuracy: 0.9385 - val_loss: 0.1371\n",
      "Epoch 39/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8961 - loss: 0.2219 - val_accuracy: 0.9538 - val_loss: 0.1280\n",
      "Epoch 40/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9269 - loss: 0.1984 - val_accuracy: 0.9538 - val_loss: 0.1122\n",
      "Epoch 41/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9179 - loss: 0.2412 - val_accuracy: 0.9538 - val_loss: 0.1009\n",
      "Epoch 42/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9082 - loss: 0.2299 - val_accuracy: 0.9538 - val_loss: 0.0873\n",
      "Epoch 43/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9259 - loss: 0.1711 - val_accuracy: 0.9538 - val_loss: 0.1035\n",
      "Epoch 44/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9332 - loss: 0.2011 - val_accuracy: 0.9538 - val_loss: 0.1151\n",
      "Epoch 45/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9378 - loss: 0.2033 - val_accuracy: 0.9538 - val_loss: 0.0978\n",
      "Epoch 46/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9064 - loss: 0.2123 - val_accuracy: 0.9538 - val_loss: 0.0939\n",
      "Epoch 47/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9500 - loss: 0.1727 - val_accuracy: 0.9538 - val_loss: 0.0876\n",
      "Epoch 48/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9458 - loss: 0.1851 - val_accuracy: 0.9692 - val_loss: 0.0861\n",
      "Epoch 49/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9143 - loss: 0.1942 - val_accuracy: 0.9538 - val_loss: 0.0992\n",
      "Epoch 50/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9225 - loss: 0.2064 - val_accuracy: 0.9538 - val_loss: 0.0747\n",
      "Epoch 51/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9303 - loss: 0.1733 - val_accuracy: 0.9538 - val_loss: 0.0952\n",
      "Epoch 52/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9321 - loss: 0.1468 - val_accuracy: 0.9692 - val_loss: 0.0735\n",
      "Epoch 53/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9464 - loss: 0.1540 - val_accuracy: 0.9538 - val_loss: 0.0761\n",
      "Epoch 54/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9061 - loss: 0.1689 - val_accuracy: 0.9692 - val_loss: 0.0654\n",
      "Epoch 55/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9301 - loss: 0.1618 - val_accuracy: 0.9538 - val_loss: 0.0802\n",
      "Epoch 56/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9520 - loss: 0.1530 - val_accuracy: 0.9538 - val_loss: 0.0895\n",
      "Epoch 57/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9445 - loss: 0.1490 - val_accuracy: 0.9538 - val_loss: 0.0760\n",
      "Epoch 58/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9534 - loss: 0.1298 - val_accuracy: 0.9846 - val_loss: 0.0558\n",
      "Epoch 59/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9331 - loss: 0.1402 - val_accuracy: 0.9538 - val_loss: 0.0729\n",
      "Epoch 60/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9769 - loss: 0.1138 - val_accuracy: 0.9538 - val_loss: 0.0630\n",
      "Epoch 61/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9591 - loss: 0.1606 - val_accuracy: 1.0000 - val_loss: 0.0487\n",
      "Epoch 62/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9385 - loss: 0.1571 - val_accuracy: 0.9538 - val_loss: 0.0766\n",
      "Epoch 63/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9582 - loss: 0.1500 - val_accuracy: 0.9692 - val_loss: 0.0661\n",
      "Epoch 64/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9364 - loss: 0.1302 - val_accuracy: 0.9692 - val_loss: 0.0537\n",
      "Epoch 65/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9652 - loss: 0.0962 - val_accuracy: 0.9692 - val_loss: 0.0509\n",
      "Epoch 66/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9228 - loss: 0.1517 - val_accuracy: 0.9538 - val_loss: 0.0644\n",
      "Epoch 67/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9260 - loss: 0.1440 - val_accuracy: 0.9846 - val_loss: 0.0501\n",
      "Epoch 68/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9706 - loss: 0.1094 - val_accuracy: 0.9846 - val_loss: 0.0486\n",
      "Epoch 69/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9525 - loss: 0.1109 - val_accuracy: 0.9846 - val_loss: 0.0498\n",
      "Epoch 70/70\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9404 - loss: 0.1309 - val_accuracy: 0.9692 - val_loss: 0.0575\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Evaluation =====\n",
      "Accuracy: 0.9569\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8462    0.6875    0.7586        16\n",
      "           1     0.8933    0.9710    0.9306        69\n",
      "           2     1.0000    0.9781    0.9889       137\n",
      "           3     1.0000    1.0000    1.0000        10\n",
      "\n",
      "    accuracy                         0.9569       232\n",
      "   macro avg     0.9349    0.9092    0.9195       232\n",
      "weighted avg     0.9577    0.9569    0.9562       232\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 11   5   0   0]\n",
      " [  2  67   0   0]\n",
      " [  0   3 134   0]\n",
      " [  0   0   0  10]]\n",
      "\n",
      "===== Model Metrics =====\n",
      "Training Time: 14.43s\n",
      "Inference/sample: 0.001417s\n",
      "Testing Time: 0.33s\n",
      "Model Size: 0.10 MB\n",
      "Trainable Params: 19,822\n",
      "RAM Usage: 867.14 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import psutil\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from bayes_opt import BayesianOptimization\n",
    "from scipy.fftpack import fft\n",
    "import warnings\n",
    "\n",
    "# Set seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================\n",
    "# 1. Data Loading and Preprocessing\n",
    "# ============================================\n",
    "def load_and_preprocess_data(filepath):\n",
    "    df = pd.read_csv(filepath, delimiter=\",\", low_memory=False)\n",
    "    df.columns = df.columns.str.replace('\"', '').str.strip()\n",
    "\n",
    "    if 'TIME' not in df.columns:\n",
    "        raise KeyError(\"The column 'TIME' does not exist in the dataset.\")\n",
    "\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'], errors='coerce')\n",
    "    df.dropna(subset=['TIME'], inplace=True)\n",
    "    df.set_index('TIME', inplace=True)\n",
    "\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "    target_col = 'fenologia_h1'\n",
    "    if target_col not in df.columns:\n",
    "        raise KeyError(f\"The target column '{target_col}' does not exist.\")\n",
    "\n",
    "    df[target_col].interpolate(method='linear', inplace=True)\n",
    "    df[target_col] = df[target_col] - 1\n",
    "\n",
    "    for i in range(1, 4):\n",
    "        df[f'{target_col}_lag{i}'] = df[target_col].shift(i)\n",
    "\n",
    "    for window in [3, 6]:\n",
    "        df[f'{target_col}_roll_mean_{window}'] = df[target_col].rolling(window).mean()\n",
    "        df[f'{target_col}_roll_std_{window}'] = df[target_col].rolling(window).std()\n",
    "\n",
    "    # df['month'] = df.index.month\n",
    "    # df['weekofyear'] = df.index.isocalendar().week\n",
    "    # df['year'] = df.index.isocalendar().year\n",
    "    # df['sin_week'] = np.sin(2 * np.pi * df['weekofyear'] / 52)\n",
    "    # df['cos_week'] = np.cos(2 * np.pi * df['weekofyear'] / 52)\n",
    "    # df['EMA_3'] = df[target_col].ewm(span=3, adjust=False).mean()\n",
    "    # df['EMA_6'] = df[target_col].ewm(span=6, adjust=False).mean()\n",
    "    # df['correlation_target_month'] = df[target_col].rolling(window=6).corr(df['month'])\n",
    "    # df['correlation_target_week'] = df[target_col].rolling(window=6).corr(df['weekofyear'])\n",
    "\n",
    "    # fft_values = fft(df[target_col].dropna().values)\n",
    "    # fft_real = np.real(fft_values)[:len(df[target_col])]\n",
    "    # fft_imag = np.imag(fft_values)[:len(df[target_col])]\n",
    "    # df['fft_real'] = np.concatenate([fft_real, np.nan * np.ones(len(df) - len(fft_real))])\n",
    "    # df['fft_imag'] = np.concatenate([fft_imag, np.nan * np.ones(len(df) - len(fft_imag))])\n",
    "\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "    return df, target_col\n",
    "\n",
    "# ============================================\n",
    "# 2. Feature Scaling\n",
    "# ============================================\n",
    "def feature_scaling(df, target_col):\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))  # 3D\n",
    "    return X_scaled, y, scaler\n",
    "\n",
    "# ============================================\n",
    "# 3. CNN + Gate Attention Model\n",
    "# ============================================\n",
    "def gated_attention_block(inputs, units):\n",
    "    conv_output = layers.Conv1D(units, kernel_size=1, activation='relu')(inputs)\n",
    "    attention_scores = layers.Dense(units, activation='sigmoid')(conv_output)\n",
    "    gated_output = layers.Multiply()([conv_output, attention_scores])\n",
    "    return gated_output\n",
    "\n",
    "def build_cnn_gatt_model(input_shape, num_classes=4, num_units=64, dropout_rate=0.2):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = gated_attention_block(inputs, units=num_units)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    x = layers.Dense(num_units // 2, activation='relu')(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# ============================================\n",
    "# 4. Bayesian Optimization Objective\n",
    "# ============================================\n",
    "def objective_function(num_units, dropout_rate, epochs, batch_size, X_train, y_train):\n",
    "    model = build_cnn_gatt_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        num_classes=4,\n",
    "        num_units=int(num_units),\n",
    "        dropout_rate=dropout_rate\n",
    "    )\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=int(epochs),\n",
    "        batch_size=int(batch_size),\n",
    "        validation_split=0.2,\n",
    "        verbose=0\n",
    "    )\n",
    "    return np.max(history.history['val_accuracy'])\n",
    "\n",
    "# ============================================\n",
    "# 5. Optimize Hyperparameters\n",
    "# ============================================\n",
    "def optimize_hyperparameters(X_train, y_train):\n",
    "    pbounds = {\n",
    "        'num_units': (32, 128),\n",
    "        'dropout_rate': (0.1, 0.5),\n",
    "        'epochs': (50, 100),\n",
    "        'batch_size': (16, 64)\n",
    "    }\n",
    "\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=lambda num_units, dropout_rate, epochs, batch_size: objective_function(\n",
    "            num_units, dropout_rate, epochs, batch_size, X_train, y_train\n",
    "        ),\n",
    "        pbounds=pbounds,\n",
    "        random_state=42,\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    optimizer.maximize(init_points=5, n_iter=15)\n",
    "    return optimizer.max['params']\n",
    "\n",
    "# ============================================\n",
    "# 6. Train & Evaluate Model\n",
    "# ============================================\n",
    "def train_and_evaluate_model(X_train, X_test, y_train, y_test, best_params):\n",
    "    model = build_cnn_gatt_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        num_classes=4,\n",
    "        num_units=int(best_params['num_units']),\n",
    "        dropout_rate=float(best_params['dropout_rate'])\n",
    "    )\n",
    "\n",
    "    start_train = time.time()\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=int(best_params['epochs']),\n",
    "        batch_size=int(best_params['batch_size']),\n",
    "        validation_split=0.2,\n",
    "        verbose=1\n",
    "    )\n",
    "    training_time = time.time() - start_train\n",
    "\n",
    "    start_infer = time.time()\n",
    "    y_pred = model.predict(X_test).argmax(axis=1)\n",
    "    inference_time = (time.time() - start_infer) / len(X_test)\n",
    "    testing_time = time.time() - start_infer\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(\"\\n===== Evaluation =====\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(\"Report:\\n\", classification_report(y_test, y_pred, digits=4))\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "    model.save(\"temp_model.h5\", include_optimizer=False)\n",
    "    model_size = os.path.getsize(\"temp_model.h5\") / (1024 ** 2)\n",
    "    os.remove(\"temp_model.h5\")\n",
    "\n",
    "    total_params = np.sum([np.prod(v.shape) for v in model.trainable_weights])\n",
    "    ram_usage = psutil.Process(os.getpid()).memory_info().rss / (1024 ** 2)\n",
    "\n",
    "    print(\"\\n===== Model Metrics =====\")\n",
    "    print(f\"Training Time: {training_time:.2f}s\")\n",
    "    print(f\"Inference/sample: {inference_time:.6f}s\")\n",
    "    print(f\"Testing Time: {testing_time:.2f}s\")\n",
    "    print(f\"Model Size: {model_size:.2f} MB\")\n",
    "    print(f\"Trainable Params: {total_params:,}\")\n",
    "    print(f\"RAM Usage: {ram_usage:.2f} MB\")\n",
    "\n",
    "    return model, history\n",
    "\n",
    "# ============================================\n",
    "# 7. Main\n",
    "# ============================================\n",
    "def main():\n",
    "    train_path =  r\"E:\\Abroad period research\\Phenology datasets\\PHENOLOGY_H1\\Cadiz_train.csv\"\n",
    "    test_path =  r\"E:\\Abroad period research\\Phenology datasets\\PHENOLOGY_H1\\Cadiz_test.csv\"\n",
    "\n",
    "    df_train, target_col = load_and_preprocess_data(train_path)\n",
    "    X_train, y_train, _ = feature_scaling(df_train, target_col)\n",
    "\n",
    "    df_test, _ = load_and_preprocess_data(test_path)\n",
    "    X_test, y_test, _ = feature_scaling(df_test, target_col)\n",
    "\n",
    "    print(\"\\n Optimizing Hyperparameters...\")\n",
    "    best_params = optimize_hyperparameters(X_train, y_train)\n",
    "    print(\"\\n Best Params:\", best_params)\n",
    "\n",
    "    print(\"\\n Training Final Gate Attention CNN Model...\")\n",
    "    model, history = train_and_evaluate_model(X_train, X_test, y_train, y_test, best_params)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663d1bb2",
   "metadata": {},
   "source": [
    "Cordoba station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60cca872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Optimizing Hyperparameters...\n",
      "|   iter    |  target   | batch_... | dropou... |  epochs   | num_units |\n",
      "-------------------------------------------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.9677   \u001b[39m | \u001b[39m33.98    \u001b[39m | \u001b[39m0.4803   \u001b[39m | \u001b[39m86.6     \u001b[39m | \u001b[39m89.47    \u001b[39m |\n",
      "| \u001b[39m2        \u001b[39m | \u001b[39m0.9516   \u001b[39m | \u001b[39m23.49    \u001b[39m | \u001b[39m0.1624   \u001b[39m | \u001b[39m52.9     \u001b[39m | \u001b[39m115.2    \u001b[39m |\n",
      "| \u001b[39m3        \u001b[39m | \u001b[39m0.9194   \u001b[39m | \u001b[39m44.85    \u001b[39m | \u001b[39m0.3832   \u001b[39m | \u001b[39m51.03    \u001b[39m | \u001b[39m125.1    \u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m0.8871   \u001b[39m | \u001b[39m55.96    \u001b[39m | \u001b[39m0.1849   \u001b[39m | \u001b[39m59.09    \u001b[39m | \u001b[39m49.61    \u001b[39m |\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m0.9194   \u001b[39m | \u001b[39m30.6     \u001b[39m | \u001b[39m0.3099   \u001b[39m | \u001b[39m71.6     \u001b[39m | \u001b[39m59.96    \u001b[39m |\n",
      "| \u001b[39m6        \u001b[39m | \u001b[39m0.9677   \u001b[39m | \u001b[39m34.7     \u001b[39m | \u001b[39m0.242    \u001b[39m | \u001b[39m86.44    \u001b[39m | \u001b[39m89.42    \u001b[39m |\n",
      "| \u001b[35m7        \u001b[39m | \u001b[35m0.9839   \u001b[39m | \u001b[35m22.31    \u001b[39m | \u001b[35m0.218    \u001b[39m | \u001b[35m99.29    \u001b[39m | \u001b[35m121.0    \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m0.9839   \u001b[39m | \u001b[39m63.23    \u001b[39m | \u001b[39m0.4393   \u001b[39m | \u001b[39m99.83    \u001b[39m | \u001b[39m122.0    \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m0.9839   \u001b[39m | \u001b[39m63.42    \u001b[39m | \u001b[39m0.2116   \u001b[39m | \u001b[39m98.95    \u001b[39m | \u001b[39m90.87    \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m0.9677   \u001b[39m | \u001b[39m47.1     \u001b[39m | \u001b[39m0.3922   \u001b[39m | \u001b[39m99.78    \u001b[39m | \u001b[39m107.9    \u001b[39m |\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m0.9839   \u001b[39m | \u001b[39m16.06    \u001b[39m | \u001b[39m0.2302   \u001b[39m | \u001b[39m85.84    \u001b[39m | \u001b[39m127.9    \u001b[39m |\n",
      "| \u001b[39m12       \u001b[39m | \u001b[39m0.9194   \u001b[39m | \u001b[39m62.56    \u001b[39m | \u001b[39m0.3907   \u001b[39m | \u001b[39m99.4     \u001b[39m | \u001b[39m71.21    \u001b[39m |\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m0.9677   \u001b[39m | \u001b[39m63.75    \u001b[39m | \u001b[39m0.1313   \u001b[39m | \u001b[39m85.34    \u001b[39m | \u001b[39m105.6    \u001b[39m |\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m0.9839   \u001b[39m | \u001b[39m16.05    \u001b[39m | \u001b[39m0.2937   \u001b[39m | \u001b[39m81.84    \u001b[39m | \u001b[39m106.9    \u001b[39m |\n",
      "| \u001b[39m15       \u001b[39m | \u001b[39m0.9839   \u001b[39m | \u001b[39m18.3     \u001b[39m | \u001b[39m0.1748   \u001b[39m | \u001b[39m99.59    \u001b[39m | \u001b[39m32.26    \u001b[39m |\n",
      "| \u001b[39m16       \u001b[39m | \u001b[39m0.9839   \u001b[39m | \u001b[39m16.55    \u001b[39m | \u001b[39m0.113    \u001b[39m | \u001b[39m99.32    \u001b[39m | \u001b[39m53.17    \u001b[39m |\n",
      "| \u001b[39m17       \u001b[39m | \u001b[39m0.9839   \u001b[39m | \u001b[39m16.29    \u001b[39m | \u001b[39m0.2091   \u001b[39m | \u001b[39m99.41    \u001b[39m | \u001b[39m82.59    \u001b[39m |\n",
      "| \u001b[39m18       \u001b[39m | \u001b[39m0.9839   \u001b[39m | \u001b[39m17.53    \u001b[39m | \u001b[39m0.3459   \u001b[39m | \u001b[39m99.52    \u001b[39m | \u001b[39m101.5    \u001b[39m |\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m0.9355   \u001b[39m | \u001b[39m17.2     \u001b[39m | \u001b[39m0.3137   \u001b[39m | \u001b[39m76.56    \u001b[39m | \u001b[39m32.66    \u001b[39m |\n",
      "| \u001b[35m20       \u001b[39m | \u001b[35m1.0      \u001b[39m | \u001b[35m16.17    \u001b[39m | \u001b[35m0.3032   \u001b[39m | \u001b[35m59.58    \u001b[39m | \u001b[35m89.97    \u001b[39m |\n",
      "=========================================================================\n",
      "\n",
      " Best Params: {'batch_size': 16.168832204349393, 'dropout_rate': 0.3031818858100779, 'epochs': 59.57943838417318, 'num_units': 89.97319802673024}\n",
      "\n",
      " Training Final Gate Attention CNN Model...\n",
      "Epoch 1/59\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.5215 - loss: 1.2940 - val_accuracy: 0.5806 - val_loss: 1.1407\n",
      "Epoch 2/59\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5215 - loss: 1.1461 - val_accuracy: 0.5806 - val_loss: 0.9991\n",
      "Epoch 3/59\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5222 - loss: 1.0292 - val_accuracy: 0.5806 - val_loss: 0.8869\n",
      "Epoch 4/59\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5836 - loss: 0.9235 - val_accuracy: 0.6129 - val_loss: 0.7777\n",
      "Epoch 5/59\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6817 - loss: 0.8234 - val_accuracy: 0.7258 - val_loss: 0.6797\n",
      "Epoch 6/59\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7626 - loss: 0.7029 - val_accuracy: 0.7581 - val_loss: 0.5896\n",
      "Epoch 7/59\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7789 - loss: 0.6011 - val_accuracy: 0.7742 - val_loss: 0.5195\n",
      "Epoch 8/59\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8147 - loss: 0.5303 - val_accuracy: 0.8710 - val_loss: 0.4548\n",
      "Epoch 9/59\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8497 - loss: 0.4567 - val_accuracy: 0.8548 - val_loss: 0.4316\n",
      "Epoch 10/59\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8108 - loss: 0.4616 - val_accuracy: 0.8710 - val_loss: 0.3982\n",
      "Epoch 11/59\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8205 - loss: 0.3964 - val_accuracy: 0.8710 - val_loss: 0.3853\n",
      "Epoch 12/59\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8481 - loss: 0.3685 - val_accuracy: 0.8871 - val_loss: 0.3516\n",
      "Epoch 13/59\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8444 - loss: 0.3841 - val_accuracy: 0.8710 - val_loss: 0.3592\n",
      "Epoch 14/59\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8452 - loss: 0.3652 - val_accuracy: 0.8871 - val_loss: 0.3373\n",
      "Epoch 15/59\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8763 - loss: 0.3202 - val_accuracy: 0.8871 - val_loss: 0.3435\n",
      "Epoch 16/59\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8543 - loss: 0.3405 - val_accuracy: 0.8710 - val_loss: 0.3265\n",
      "Epoch 17/59\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9060 - loss: 0.2863 - val_accuracy: 0.8871 - val_loss: 0.3102\n",
      "Epoch 18/59\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8474 - loss: 0.3144 - val_accuracy: 0.9032 - val_loss: 0.3008\n",
      "Epoch 19/59\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8696 - loss: 0.3236 - val_accuracy: 0.8871 - val_loss: 0.2906\n",
      "Epoch 20/59\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8925 - loss: 0.2646 - val_accuracy: 0.8710 - val_loss: 0.2933\n",
      "Epoch 21/59\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8772 - loss: 0.2972 - val_accuracy: 0.9032 - val_loss: 0.2684\n",
      "Epoch 22/59\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8644 - loss: 0.2915 - val_accuracy: 0.8871 - val_loss: 0.2775\n",
      "Epoch 23/59\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9276 - loss: 0.2386 - val_accuracy: 0.8871 - val_loss: 0.2669\n",
      "Epoch 24/59\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9344 - loss: 0.2271 - val_accuracy: 0.8871 - val_loss: 0.2654\n",
      "Epoch 25/59\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9236 - loss: 0.2187 - val_accuracy: 0.8871 - val_loss: 0.2491\n",
      "Epoch 26/59\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9369 - loss: 0.2403 - val_accuracy: 0.8871 - val_loss: 0.2511\n",
      "Epoch 27/59\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9173 - loss: 0.2134 - val_accuracy: 0.8710 - val_loss: 0.2598\n",
      "Epoch 28/59\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8947 - loss: 0.2309 - val_accuracy: 0.8871 - val_loss: 0.2361\n",
      "Epoch 29/59\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9085 - loss: 0.2342 - val_accuracy: 0.8871 - val_loss: 0.2286\n",
      "Epoch 30/59\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9191 - loss: 0.2164 - val_accuracy: 0.9032 - val_loss: 0.2185\n",
      "Epoch 31/59\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9405 - loss: 0.1880 - val_accuracy: 0.9032 - val_loss: 0.2249\n",
      "Epoch 32/59\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9227 - loss: 0.2141 - val_accuracy: 0.8871 - val_loss: 0.2124\n",
      "Epoch 33/59\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9469 - loss: 0.1889 - val_accuracy: 0.9194 - val_loss: 0.1996\n",
      "Epoch 34/59\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9126 - loss: 0.2042 - val_accuracy: 0.9032 - val_loss: 0.2120\n",
      "Epoch 35/59\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9098 - loss: 0.1973 - val_accuracy: 0.9032 - val_loss: 0.1914\n",
      "Epoch 36/59\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9354 - loss: 0.1867 - val_accuracy: 0.9194 - val_loss: 0.1871\n",
      "Epoch 37/59\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9300 - loss: 0.1751 - val_accuracy: 0.9355 - val_loss: 0.1774\n",
      "Epoch 38/59\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9214 - loss: 0.2001 - val_accuracy: 0.9194 - val_loss: 0.1758\n",
      "Epoch 39/59\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9343 - loss: 0.1729 - val_accuracy: 0.9194 - val_loss: 0.1779\n",
      "Epoch 40/59\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9538 - loss: 0.1616 - val_accuracy: 0.9516 - val_loss: 0.1600\n",
      "Epoch 41/59\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9413 - loss: 0.1564 - val_accuracy: 0.9355 - val_loss: 0.1644\n",
      "Epoch 42/59\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9425 - loss: 0.1644 - val_accuracy: 0.9355 - val_loss: 0.1615\n",
      "Epoch 43/59\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9354 - loss: 0.1798 - val_accuracy: 0.9677 - val_loss: 0.1592\n",
      "Epoch 44/59\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9461 - loss: 0.1572 - val_accuracy: 0.9355 - val_loss: 0.1552\n",
      "Epoch 45/59\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9489 - loss: 0.1468 - val_accuracy: 0.9516 - val_loss: 0.1500\n",
      "Epoch 46/59\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9427 - loss: 0.1539 - val_accuracy: 0.9516 - val_loss: 0.1452\n",
      "Epoch 47/59\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9634 - loss: 0.1336 - val_accuracy: 0.9677 - val_loss: 0.1406\n",
      "Epoch 48/59\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9595 - loss: 0.1367 - val_accuracy: 0.9516 - val_loss: 0.1396\n",
      "Epoch 49/59\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9382 - loss: 0.1611 - val_accuracy: 0.9677 - val_loss: 0.1342\n",
      "Epoch 50/59\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9279 - loss: 0.1572 - val_accuracy: 0.9677 - val_loss: 0.1378\n",
      "Epoch 51/59\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9191 - loss: 0.1379 - val_accuracy: 0.9516 - val_loss: 0.1319\n",
      "Epoch 52/59\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9497 - loss: 0.1269 - val_accuracy: 0.9677 - val_loss: 0.1285\n",
      "Epoch 53/59\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9424 - loss: 0.1478 - val_accuracy: 0.9677 - val_loss: 0.1229\n",
      "Epoch 54/59\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9679 - loss: 0.1133 - val_accuracy: 0.9516 - val_loss: 0.1263\n",
      "Epoch 55/59\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9667 - loss: 0.1282 - val_accuracy: 0.9355 - val_loss: 0.1357\n",
      "Epoch 56/59\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9609 - loss: 0.1260 - val_accuracy: 0.9516 - val_loss: 0.1289\n",
      "Epoch 57/59\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9684 - loss: 0.1187 - val_accuracy: 0.9516 - val_loss: 0.1277\n",
      "Epoch 58/59\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9531 - loss: 0.1215 - val_accuracy: 0.9677 - val_loss: 0.1166\n",
      "Epoch 59/59\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9383 - loss: 0.1283 - val_accuracy: 0.9677 - val_loss: 0.1211\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\n",
      "===== Evaluation =====\n",
      "Accuracy: 0.8621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8333    0.2941    0.4348        17\n",
      "           1     0.7049    0.9556    0.8113        45\n",
      "           2     0.9891    0.9010    0.9430       101\n",
      "           3     0.7333    1.0000    0.8462        11\n",
      "\n",
      "    accuracy                         0.8621       174\n",
      "   macro avg     0.8152    0.7877    0.7588       174\n",
      "weighted avg     0.8842    0.8621    0.8532       174\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 5 12  0  0]\n",
      " [ 1 43  1  0]\n",
      " [ 0  6 91  4]\n",
      " [ 0  0  0 11]]\n",
      "\n",
      "===== Model Metrics =====\n",
      "Training Time: 7.81s\n",
      "Inference/sample: 0.001051s\n",
      "Testing Time: 0.18s\n",
      "Model Size: 0.08 MB\n",
      "Trainable Params: 13,752\n",
      "RAM Usage: 1171.67 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import psutil\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from bayes_opt import BayesianOptimization\n",
    "from scipy.fftpack import fft\n",
    "import warnings\n",
    "\n",
    "# Set seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================\n",
    "# 1. Data Loading and Preprocessing\n",
    "# ============================================\n",
    "def load_and_preprocess_data(filepath):\n",
    "    df = pd.read_csv(filepath, delimiter=\",\", low_memory=False)\n",
    "    df.columns = df.columns.str.replace('\"', '').str.strip()\n",
    "\n",
    "    if 'TIME' not in df.columns:\n",
    "        raise KeyError(\"The column 'TIME' does not exist in the dataset.\")\n",
    "\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'], errors='coerce')\n",
    "    df.dropna(subset=['TIME'], inplace=True)\n",
    "    df.set_index('TIME', inplace=True)\n",
    "\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "    target_col = 'fenologia_h1'\n",
    "    if target_col not in df.columns:\n",
    "        raise KeyError(f\"The target column '{target_col}' does not exist.\")\n",
    "\n",
    "    df[target_col].interpolate(method='linear', inplace=True)\n",
    "    df[target_col] = df[target_col] - 1\n",
    "\n",
    "    for i in range(1, 4):\n",
    "        df[f'{target_col}_lag{i}'] = df[target_col].shift(i)\n",
    "\n",
    "    for window in [3, 6]:\n",
    "        df[f'{target_col}_roll_mean_{window}'] = df[target_col].rolling(window).mean()\n",
    "        df[f'{target_col}_roll_std_{window}'] = df[target_col].rolling(window).std()\n",
    "\n",
    "    # df['month'] = df.index.month\n",
    "    # df['weekofyear'] = df.index.isocalendar().week\n",
    "    # df['year'] = df.index.isocalendar().year\n",
    "    # df['sin_week'] = np.sin(2 * np.pi * df['weekofyear'] / 52)\n",
    "    # df['cos_week'] = np.cos(2 * np.pi * df['weekofyear'] / 52)\n",
    "    # df['EMA_3'] = df[target_col].ewm(span=3, adjust=False).mean()\n",
    "    # df['EMA_6'] = df[target_col].ewm(span=6, adjust=False).mean()\n",
    "    # df['correlation_target_month'] = df[target_col].rolling(window=6).corr(df['month'])\n",
    "    # df['correlation_target_week'] = df[target_col].rolling(window=6).corr(df['weekofyear'])\n",
    "\n",
    "    # fft_values = fft(df[target_col].dropna().values)\n",
    "    # fft_real = np.real(fft_values)[:len(df[target_col])]\n",
    "    # fft_imag = np.imag(fft_values)[:len(df[target_col])]\n",
    "    # df['fft_real'] = np.concatenate([fft_real, np.nan * np.ones(len(df) - len(fft_real))])\n",
    "    # df['fft_imag'] = np.concatenate([fft_imag, np.nan * np.ones(len(df) - len(fft_imag))])\n",
    "\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "    return df, target_col\n",
    "\n",
    "# ============================================\n",
    "# 2. Feature Scaling\n",
    "# ============================================\n",
    "def feature_scaling(df, target_col):\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))  # 3D\n",
    "    return X_scaled, y, scaler\n",
    "\n",
    "# ============================================\n",
    "# 3. CNN + Gate Attention Model\n",
    "# ============================================\n",
    "def gated_attention_block(inputs, units):\n",
    "    conv_output = layers.Conv1D(units, kernel_size=1, activation='relu')(inputs)\n",
    "    attention_scores = layers.Dense(units, activation='sigmoid')(conv_output)\n",
    "    gated_output = layers.Multiply()([conv_output, attention_scores])\n",
    "    return gated_output\n",
    "\n",
    "def build_cnn_gatt_model(input_shape, num_classes=4, num_units=64, dropout_rate=0.2):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = gated_attention_block(inputs, units=num_units)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    x = layers.Dense(num_units // 2, activation='relu')(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# ============================================\n",
    "# 4. Bayesian Optimization Objective\n",
    "# ============================================\n",
    "def objective_function(num_units, dropout_rate, epochs, batch_size, X_train, y_train):\n",
    "    model = build_cnn_gatt_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        num_classes=4,\n",
    "        num_units=int(num_units),\n",
    "        dropout_rate=dropout_rate\n",
    "    )\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=int(epochs),\n",
    "        batch_size=int(batch_size),\n",
    "        validation_split=0.2,\n",
    "        verbose=0\n",
    "    )\n",
    "    return np.max(history.history['val_accuracy'])\n",
    "\n",
    "# ============================================\n",
    "# 5. Optimize Hyperparameters\n",
    "# ============================================\n",
    "def optimize_hyperparameters(X_train, y_train):\n",
    "    pbounds = {\n",
    "        'num_units': (32, 128),\n",
    "        'dropout_rate': (0.1, 0.5),\n",
    "        'epochs': (50, 100),\n",
    "        'batch_size': (16, 64)\n",
    "    }\n",
    "\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=lambda num_units, dropout_rate, epochs, batch_size: objective_function(\n",
    "            num_units, dropout_rate, epochs, batch_size, X_train, y_train\n",
    "        ),\n",
    "        pbounds=pbounds,\n",
    "        random_state=42,\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    optimizer.maximize(init_points=5, n_iter=15)\n",
    "    return optimizer.max['params']\n",
    "\n",
    "# ============================================\n",
    "# 6. Train & Evaluate Model\n",
    "# ============================================\n",
    "def train_and_evaluate_model(X_train, X_test, y_train, y_test, best_params):\n",
    "    model = build_cnn_gatt_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        num_classes=4,\n",
    "        num_units=int(best_params['num_units']),\n",
    "        dropout_rate=float(best_params['dropout_rate'])\n",
    "    )\n",
    "\n",
    "    start_train = time.time()\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=int(best_params['epochs']),\n",
    "        batch_size=int(best_params['batch_size']),\n",
    "        validation_split=0.2,\n",
    "        verbose=1\n",
    "    )\n",
    "    training_time = time.time() - start_train\n",
    "\n",
    "    start_infer = time.time()\n",
    "    y_pred = model.predict(X_test).argmax(axis=1)\n",
    "    inference_time = (time.time() - start_infer) / len(X_test)\n",
    "    testing_time = time.time() - start_infer\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(\"\\n===== Evaluation =====\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(\"Report:\\n\", classification_report(y_test, y_pred, digits=4))\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "    model.save(\"temp_model.h5\", include_optimizer=False)\n",
    "    model_size = os.path.getsize(\"temp_model.h5\") / (1024 ** 2)\n",
    "    os.remove(\"temp_model.h5\")\n",
    "\n",
    "    total_params = np.sum([np.prod(v.shape) for v in model.trainable_weights])\n",
    "    ram_usage = psutil.Process(os.getpid()).memory_info().rss / (1024 ** 2)\n",
    "\n",
    "    print(\"\\n===== Model Metrics =====\")\n",
    "    print(f\"Training Time: {training_time:.2f}s\")\n",
    "    print(f\"Inference/sample: {inference_time:.6f}s\")\n",
    "    print(f\"Testing Time: {testing_time:.2f}s\")\n",
    "    print(f\"Model Size: {model_size:.2f} MB\")\n",
    "    print(f\"Trainable Params: {total_params:,}\")\n",
    "    print(f\"RAM Usage: {ram_usage:.2f} MB\")\n",
    "\n",
    "    return model, history\n",
    "\n",
    "# ============================================\n",
    "# 7. Main\n",
    "# ============================================\n",
    "def main():\n",
    "    train_path = r\"E:\\Abroad period research\\Phenology datasets\\PHENOLOGY_H1\\Cordoba_train.csv\"\n",
    "    test_path = r\"E:\\Abroad period research\\Phenology datasets\\PHENOLOGY_H1\\Cordoba_test.csv\"\n",
    "\n",
    "    df_train, target_col = load_and_preprocess_data(train_path)\n",
    "    X_train, y_train, _ = feature_scaling(df_train, target_col)\n",
    "\n",
    "    df_test, _ = load_and_preprocess_data(test_path)\n",
    "    X_test, y_test, _ = feature_scaling(df_test, target_col)\n",
    "\n",
    "    print(\"\\n Optimizing Hyperparameters...\")\n",
    "    best_params = optimize_hyperparameters(X_train, y_train)\n",
    "    print(\"\\n Best Params:\", best_params)\n",
    "\n",
    "    print(\"\\n Training Final Gate Attention CNN Model...\")\n",
    "    model, history = train_and_evaluate_model(X_train, X_test, y_train, y_test, best_params)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a5b40a",
   "metadata": {},
   "source": [
    "Jaen station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "630d1610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Optimizing Hyperparameters...\n",
      "|   iter    |  target   | batch_... | dropou... |  epochs   | num_units |\n",
      "-------------------------------------------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.98     \u001b[39m | \u001b[39m33.98    \u001b[39m | \u001b[39m0.4803   \u001b[39m | \u001b[39m86.6     \u001b[39m | \u001b[39m89.47    \u001b[39m |\n",
      "| \u001b[39m2        \u001b[39m | \u001b[39m0.98     \u001b[39m | \u001b[39m23.49    \u001b[39m | \u001b[39m0.1624   \u001b[39m | \u001b[39m52.9     \u001b[39m | \u001b[39m115.2    \u001b[39m |\n",
      "| \u001b[39m3        \u001b[39m | \u001b[39m0.96     \u001b[39m | \u001b[39m44.85    \u001b[39m | \u001b[39m0.3832   \u001b[39m | \u001b[39m51.03    \u001b[39m | \u001b[39m125.1    \u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m0.9      \u001b[39m | \u001b[39m55.96    \u001b[39m | \u001b[39m0.1849   \u001b[39m | \u001b[39m59.09    \u001b[39m | \u001b[39m49.61    \u001b[39m |\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m0.94     \u001b[39m | \u001b[39m30.6     \u001b[39m | \u001b[39m0.3099   \u001b[39m | \u001b[39m71.6     \u001b[39m | \u001b[39m59.96    \u001b[39m |\n",
      "| \u001b[35m6        \u001b[39m | \u001b[35m1.0      \u001b[39m | \u001b[35m16.05    \u001b[39m | \u001b[35m0.4217   \u001b[39m | \u001b[35m94.47    \u001b[39m | \u001b[35m125.1    \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m0.98     \u001b[39m | \u001b[39m60.56    \u001b[39m | \u001b[39m0.1673   \u001b[39m | \u001b[39m99.98    \u001b[39m | \u001b[39m127.1    \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m16.83    \u001b[39m | \u001b[39m0.444    \u001b[39m | \u001b[39m94.02    \u001b[39m | \u001b[39m122.9    \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m16.02    \u001b[39m | \u001b[39m0.4024   \u001b[39m | \u001b[39m98.89    \u001b[39m | \u001b[39m93.7     \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m16.47    \u001b[39m | \u001b[39m0.1784   \u001b[39m | \u001b[39m70.32    \u001b[39m | \u001b[39m127.6    \u001b[39m |\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m16.43    \u001b[39m | \u001b[39m0.488    \u001b[39m | \u001b[39m80.06    \u001b[39m | \u001b[39m107.5    \u001b[39m |\n",
      "| \u001b[39m12       \u001b[39m | \u001b[39m0.98     \u001b[39m | \u001b[39m16.11    \u001b[39m | \u001b[39m0.4422   \u001b[39m | \u001b[39m99.44    \u001b[39m | \u001b[39m104.1    \u001b[39m |\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m17.0     \u001b[39m | \u001b[39m0.1007   \u001b[39m | \u001b[39m98.42    \u001b[39m | \u001b[39m66.91    \u001b[39m |\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m0.96     \u001b[39m | \u001b[39m18.33    \u001b[39m | \u001b[39m0.1216   \u001b[39m | \u001b[39m99.57    \u001b[39m | \u001b[39m33.35    \u001b[39m |\n",
      "| \u001b[39m15       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m36.51    \u001b[39m | \u001b[39m0.2428   \u001b[39m | \u001b[39m85.27    \u001b[39m | \u001b[39m127.3    \u001b[39m |\n",
      "| \u001b[39m16       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m16.11    \u001b[39m | \u001b[39m0.3919   \u001b[39m | \u001b[39m80.4     \u001b[39m | \u001b[39m83.33    \u001b[39m |\n",
      "| \u001b[39m17       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m16.4     \u001b[39m | \u001b[39m0.2157   \u001b[39m | \u001b[39m50.75    \u001b[39m | \u001b[39m88.01    \u001b[39m |\n",
      "| \u001b[39m18       \u001b[39m | \u001b[39m0.96     \u001b[39m | \u001b[39m63.4     \u001b[39m | \u001b[39m0.3763   \u001b[39m | \u001b[39m99.22    \u001b[39m | \u001b[39m95.92    \u001b[39m |\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m17.46    \u001b[39m | \u001b[39m0.1704   \u001b[39m | \u001b[39m98.84    \u001b[39m | \u001b[39m81.41    \u001b[39m |\n",
      "| \u001b[39m20       \u001b[39m | \u001b[39m0.96     \u001b[39m | \u001b[39m16.11    \u001b[39m | \u001b[39m0.4754   \u001b[39m | \u001b[39m62.52    \u001b[39m | \u001b[39m96.69    \u001b[39m |\n",
      "=========================================================================\n",
      "\n",
      " Best Params: {'batch_size': 16.047951628813248, 'dropout_rate': 0.4217173855108457, 'epochs': 94.4668806010724, 'num_units': 125.06849883639988}\n",
      "\n",
      " Training Final Gate Attention CNN Model...\n",
      "Epoch 1/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.2889 - loss: 1.3746 - val_accuracy: 0.6000 - val_loss: 1.1759\n",
      "Epoch 2/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5693 - loss: 1.1596 - val_accuracy: 0.6000 - val_loss: 0.9644\n",
      "Epoch 3/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5693 - loss: 0.9811 - val_accuracy: 0.6000 - val_loss: 0.8673\n",
      "Epoch 4/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5843 - loss: 0.8841 - val_accuracy: 0.6400 - val_loss: 0.7841\n",
      "Epoch 5/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6710 - loss: 0.7690 - val_accuracy: 0.6800 - val_loss: 0.6717\n",
      "Epoch 6/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7362 - loss: 0.6560 - val_accuracy: 0.7400 - val_loss: 0.5685\n",
      "Epoch 7/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8046 - loss: 0.5308 - val_accuracy: 0.7600 - val_loss: 0.4988\n",
      "Epoch 8/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8122 - loss: 0.4985 - val_accuracy: 0.8000 - val_loss: 0.4401\n",
      "Epoch 9/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8596 - loss: 0.4273 - val_accuracy: 0.8200 - val_loss: 0.3928\n",
      "Epoch 10/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8512 - loss: 0.4047 - val_accuracy: 0.8400 - val_loss: 0.3478\n",
      "Epoch 11/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8577 - loss: 0.4216 - val_accuracy: 0.8400 - val_loss: 0.3351\n",
      "Epoch 12/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8720 - loss: 0.3675 - val_accuracy: 0.8600 - val_loss: 0.3047\n",
      "Epoch 13/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8842 - loss: 0.3562 - val_accuracy: 0.8600 - val_loss: 0.2903\n",
      "Epoch 14/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8877 - loss: 0.3361 - val_accuracy: 0.8800 - val_loss: 0.2684\n",
      "Epoch 15/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8800 - loss: 0.3298 - val_accuracy: 0.8800 - val_loss: 0.2621\n",
      "Epoch 16/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8551 - loss: 0.3281 - val_accuracy: 0.9000 - val_loss: 0.2407\n",
      "Epoch 17/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8953 - loss: 0.3047 - val_accuracy: 0.8800 - val_loss: 0.2353\n",
      "Epoch 18/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8795 - loss: 0.3030 - val_accuracy: 0.9200 - val_loss: 0.2131\n",
      "Epoch 19/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9067 - loss: 0.2687 - val_accuracy: 0.9200 - val_loss: 0.2077\n",
      "Epoch 20/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9190 - loss: 0.2817 - val_accuracy: 0.9000 - val_loss: 0.2139\n",
      "Epoch 21/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9169 - loss: 0.2666 - val_accuracy: 0.9400 - val_loss: 0.1867\n",
      "Epoch 22/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9400 - loss: 0.2405 - val_accuracy: 0.9200 - val_loss: 0.1915\n",
      "Epoch 23/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9116 - loss: 0.2421 - val_accuracy: 0.9400 - val_loss: 0.1713\n",
      "Epoch 24/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9136 - loss: 0.2414 - val_accuracy: 0.9400 - val_loss: 0.1722\n",
      "Epoch 25/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9254 - loss: 0.1807 - val_accuracy: 0.9400 - val_loss: 0.1740\n",
      "Epoch 26/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9138 - loss: 0.2357 - val_accuracy: 0.9400 - val_loss: 0.1678\n",
      "Epoch 27/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9220 - loss: 0.1789 - val_accuracy: 0.9400 - val_loss: 0.1649\n",
      "Epoch 28/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9298 - loss: 0.2378 - val_accuracy: 0.9400 - val_loss: 0.1494\n",
      "Epoch 29/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9371 - loss: 0.1688 - val_accuracy: 0.9400 - val_loss: 0.1427\n",
      "Epoch 30/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9320 - loss: 0.1808 - val_accuracy: 0.9400 - val_loss: 0.1361\n",
      "Epoch 31/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9395 - loss: 0.1839 - val_accuracy: 0.9400 - val_loss: 0.1363\n",
      "Epoch 32/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9319 - loss: 0.1694 - val_accuracy: 0.9400 - val_loss: 0.1250\n",
      "Epoch 33/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9075 - loss: 0.1726 - val_accuracy: 0.9400 - val_loss: 0.1230\n",
      "Epoch 34/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9547 - loss: 0.1561 - val_accuracy: 0.9400 - val_loss: 0.1312\n",
      "Epoch 35/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9288 - loss: 0.1875 - val_accuracy: 0.9600 - val_loss: 0.1067\n",
      "Epoch 36/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9580 - loss: 0.1676 - val_accuracy: 0.9600 - val_loss: 0.1017\n",
      "Epoch 37/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9459 - loss: 0.1450 - val_accuracy: 0.9600 - val_loss: 0.0998\n",
      "Epoch 38/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9491 - loss: 0.1551 - val_accuracy: 0.9600 - val_loss: 0.0889\n",
      "Epoch 39/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9558 - loss: 0.1311 - val_accuracy: 0.9600 - val_loss: 0.0892\n",
      "Epoch 40/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9401 - loss: 0.1562 - val_accuracy: 0.9600 - val_loss: 0.0885\n",
      "Epoch 41/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9367 - loss: 0.1445 - val_accuracy: 0.9800 - val_loss: 0.0825\n",
      "Epoch 42/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9523 - loss: 0.1054 - val_accuracy: 0.9800 - val_loss: 0.0778\n",
      "Epoch 43/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9631 - loss: 0.0957 - val_accuracy: 0.9800 - val_loss: 0.0788\n",
      "Epoch 44/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9410 - loss: 0.1572 - val_accuracy: 0.9600 - val_loss: 0.0765\n",
      "Epoch 45/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9683 - loss: 0.1134 - val_accuracy: 0.9600 - val_loss: 0.0790\n",
      "Epoch 46/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9531 - loss: 0.1066 - val_accuracy: 0.9800 - val_loss: 0.0724\n",
      "Epoch 47/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9760 - loss: 0.0757 - val_accuracy: 0.9800 - val_loss: 0.0657\n",
      "Epoch 48/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9617 - loss: 0.1144 - val_accuracy: 0.9800 - val_loss: 0.0622\n",
      "Epoch 49/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9507 - loss: 0.1247 - val_accuracy: 0.9800 - val_loss: 0.0673\n",
      "Epoch 50/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9419 - loss: 0.1239 - val_accuracy: 1.0000 - val_loss: 0.0614\n",
      "Epoch 51/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9614 - loss: 0.0995 - val_accuracy: 1.0000 - val_loss: 0.0571\n",
      "Epoch 52/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9614 - loss: 0.0988 - val_accuracy: 1.0000 - val_loss: 0.0569\n",
      "Epoch 53/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9596 - loss: 0.0953 - val_accuracy: 1.0000 - val_loss: 0.0591\n",
      "Epoch 54/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9646 - loss: 0.0907 - val_accuracy: 1.0000 - val_loss: 0.0519\n",
      "Epoch 55/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9708 - loss: 0.0952 - val_accuracy: 1.0000 - val_loss: 0.0577\n",
      "Epoch 56/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9684 - loss: 0.0868 - val_accuracy: 1.0000 - val_loss: 0.0550\n",
      "Epoch 57/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9708 - loss: 0.0852 - val_accuracy: 1.0000 - val_loss: 0.0494\n",
      "Epoch 58/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9877 - loss: 0.0768 - val_accuracy: 1.0000 - val_loss: 0.0522\n",
      "Epoch 59/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9782 - loss: 0.0721 - val_accuracy: 1.0000 - val_loss: 0.0470\n",
      "Epoch 60/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9868 - loss: 0.0627 - val_accuracy: 1.0000 - val_loss: 0.0478\n",
      "Epoch 61/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9745 - loss: 0.0895 - val_accuracy: 1.0000 - val_loss: 0.0450\n",
      "Epoch 62/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9899 - loss: 0.0756 - val_accuracy: 1.0000 - val_loss: 0.0453\n",
      "Epoch 63/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9844 - loss: 0.0723 - val_accuracy: 1.0000 - val_loss: 0.0426\n",
      "Epoch 64/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9897 - loss: 0.0556 - val_accuracy: 1.0000 - val_loss: 0.0363\n",
      "Epoch 65/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9858 - loss: 0.0803 - val_accuracy: 1.0000 - val_loss: 0.0369\n",
      "Epoch 66/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9652 - loss: 0.0626 - val_accuracy: 1.0000 - val_loss: 0.0413\n",
      "Epoch 67/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9655 - loss: 0.0737 - val_accuracy: 1.0000 - val_loss: 0.0416\n",
      "Epoch 68/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9820 - loss: 0.0651 - val_accuracy: 1.0000 - val_loss: 0.0444\n",
      "Epoch 69/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9853 - loss: 0.0718 - val_accuracy: 1.0000 - val_loss: 0.0335\n",
      "Epoch 70/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9921 - loss: 0.0608 - val_accuracy: 1.0000 - val_loss: 0.0305\n",
      "Epoch 71/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9743 - loss: 0.0512 - val_accuracy: 1.0000 - val_loss: 0.0312\n",
      "Epoch 72/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9824 - loss: 0.0539 - val_accuracy: 1.0000 - val_loss: 0.0317\n",
      "Epoch 73/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9921 - loss: 0.0347 - val_accuracy: 1.0000 - val_loss: 0.0314\n",
      "Epoch 74/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9872 - loss: 0.0496 - val_accuracy: 1.0000 - val_loss: 0.0305\n",
      "Epoch 75/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9635 - loss: 0.0799 - val_accuracy: 1.0000 - val_loss: 0.0289\n",
      "Epoch 76/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9891 - loss: 0.0527 - val_accuracy: 1.0000 - val_loss: 0.0332\n",
      "Epoch 77/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9862 - loss: 0.0398 - val_accuracy: 1.0000 - val_loss: 0.0297\n",
      "Epoch 78/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9976 - loss: 0.0325 - val_accuracy: 1.0000 - val_loss: 0.0266\n",
      "Epoch 79/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9840 - loss: 0.0461 - val_accuracy: 1.0000 - val_loss: 0.0255\n",
      "Epoch 80/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9853 - loss: 0.0517 - val_accuracy: 1.0000 - val_loss: 0.0227\n",
      "Epoch 81/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9976 - loss: 0.0297 - val_accuracy: 1.0000 - val_loss: 0.0218\n",
      "Epoch 82/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9726 - loss: 0.0646 - val_accuracy: 1.0000 - val_loss: 0.0208\n",
      "Epoch 83/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9923 - loss: 0.0392 - val_accuracy: 1.0000 - val_loss: 0.0206\n",
      "Epoch 84/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9897 - loss: 0.0371 - val_accuracy: 1.0000 - val_loss: 0.0208\n",
      "Epoch 85/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9961 - loss: 0.0366 - val_accuracy: 1.0000 - val_loss: 0.0191\n",
      "Epoch 86/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9921 - loss: 0.0319 - val_accuracy: 1.0000 - val_loss: 0.0186\n",
      "Epoch 87/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9869 - loss: 0.0306 - val_accuracy: 1.0000 - val_loss: 0.0189\n",
      "Epoch 88/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9877 - loss: 0.0353 - val_accuracy: 1.0000 - val_loss: 0.0189\n",
      "Epoch 89/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9976 - loss: 0.0192 - val_accuracy: 1.0000 - val_loss: 0.0177\n",
      "Epoch 90/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9926 - loss: 0.0282 - val_accuracy: 1.0000 - val_loss: 0.0166\n",
      "Epoch 91/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9844 - loss: 0.0403 - val_accuracy: 1.0000 - val_loss: 0.0167\n",
      "Epoch 92/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9723 - loss: 0.0436 - val_accuracy: 1.0000 - val_loss: 0.0164\n",
      "Epoch 93/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9877 - loss: 0.0393 - val_accuracy: 1.0000 - val_loss: 0.0157\n",
      "Epoch 94/94\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0251 - val_accuracy: 1.0000 - val_loss: 0.0154\n",
      "WARNING:tensorflow:5 out of the last 15 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000022A2AA45B20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 15 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000022A2AA45B20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Evaluation =====\n",
      "Accuracy: 0.8967\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.5385    0.7000        13\n",
      "           1     0.9091    0.8000    0.8511        50\n",
      "           2     0.9593    0.9752    0.9672       121\n",
      "           3     0.0000    0.0000    0.0000         0\n",
      "\n",
      "    accuracy                         0.8967       184\n",
      "   macro avg     0.7171    0.5784    0.6296       184\n",
      "weighted avg     0.9486    0.8967    0.9168       184\n",
      "\n",
      "Confusion Matrix:\n",
      " [[  7   4   0   2]\n",
      " [  0  40   5   5]\n",
      " [  0   0 118   3]\n",
      " [  0   0   0   0]]\n",
      "\n",
      "===== Model Metrics =====\n",
      "Training Time: 13.76s\n",
      "Inference/sample: 0.001908s\n",
      "Testing Time: 0.35s\n",
      "Model Size: 0.13 MB\n",
      "Trainable Params: 26,064\n",
      "RAM Usage: 1404.55 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import psutil\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from bayes_opt import BayesianOptimization\n",
    "from scipy.fftpack import fft\n",
    "import warnings\n",
    "\n",
    "# Set seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================\n",
    "# 1. Data Loading and Preprocessing\n",
    "# ============================================\n",
    "def load_and_preprocess_data(filepath):\n",
    "    df = pd.read_csv(filepath, delimiter=\",\", low_memory=False)\n",
    "    df.columns = df.columns.str.replace('\"', '').str.strip()\n",
    "\n",
    "    if 'TIME' not in df.columns:\n",
    "        raise KeyError(\"The column 'TIME' does not exist in the dataset.\")\n",
    "\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'], errors='coerce')\n",
    "    df.dropna(subset=['TIME'], inplace=True)\n",
    "    df.set_index('TIME', inplace=True)\n",
    "\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "    target_col = 'fenologia_h1'\n",
    "    if target_col not in df.columns:\n",
    "        raise KeyError(f\"The target column '{target_col}' does not exist.\")\n",
    "\n",
    "    df[target_col].interpolate(method='linear', inplace=True)\n",
    "    df[target_col] = df[target_col] - 1\n",
    "\n",
    "    for i in range(1, 4):\n",
    "        df[f'{target_col}_lag{i}'] = df[target_col].shift(i)\n",
    "\n",
    "    for window in [3, 6]:\n",
    "        df[f'{target_col}_roll_mean_{window}'] = df[target_col].rolling(window).mean()\n",
    "        df[f'{target_col}_roll_std_{window}'] = df[target_col].rolling(window).std()\n",
    "\n",
    "    # df['month'] = df.index.month\n",
    "    # df['weekofyear'] = df.index.isocalendar().week\n",
    "    # df['year'] = df.index.isocalendar().year\n",
    "    # df['sin_week'] = np.sin(2 * np.pi * df['weekofyear'] / 52)\n",
    "    # df['cos_week'] = np.cos(2 * np.pi * df['weekofyear'] / 52)\n",
    "    # df['EMA_3'] = df[target_col].ewm(span=3, adjust=False).mean()\n",
    "    # df['EMA_6'] = df[target_col].ewm(span=6, adjust=False).mean()\n",
    "    # df['correlation_target_month'] = df[target_col].rolling(window=6).corr(df['month'])\n",
    "    # df['correlation_target_week'] = df[target_col].rolling(window=6).corr(df['weekofyear'])\n",
    "\n",
    "    # fft_values = fft(df[target_col].dropna().values)\n",
    "    # fft_real = np.real(fft_values)[:len(df[target_col])]\n",
    "    # fft_imag = np.imag(fft_values)[:len(df[target_col])]\n",
    "    # df['fft_real'] = np.concatenate([fft_real, np.nan * np.ones(len(df) - len(fft_real))])\n",
    "    # df['fft_imag'] = np.concatenate([fft_imag, np.nan * np.ones(len(df) - len(fft_imag))])\n",
    "\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "    return df, target_col\n",
    "\n",
    "# ============================================\n",
    "# 2. Feature Scaling\n",
    "# ============================================\n",
    "def feature_scaling(df, target_col):\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))  # 3D\n",
    "    return X_scaled, y, scaler\n",
    "\n",
    "# ============================================\n",
    "# 3. CNN + Gate Attention Model\n",
    "# ============================================\n",
    "def gated_attention_block(inputs, units):\n",
    "    conv_output = layers.Conv1D(units, kernel_size=1, activation='relu')(inputs)\n",
    "    attention_scores = layers.Dense(units, activation='sigmoid')(conv_output)\n",
    "    gated_output = layers.Multiply()([conv_output, attention_scores])\n",
    "    return gated_output\n",
    "\n",
    "def build_cnn_gatt_model(input_shape, num_classes=4, num_units=64, dropout_rate=0.2):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = gated_attention_block(inputs, units=num_units)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    x = layers.Dense(num_units // 2, activation='relu')(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# ============================================\n",
    "# 4. Bayesian Optimization Objective\n",
    "# ============================================\n",
    "def objective_function(num_units, dropout_rate, epochs, batch_size, X_train, y_train):\n",
    "    model = build_cnn_gatt_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        num_classes=4,\n",
    "        num_units=int(num_units),\n",
    "        dropout_rate=dropout_rate\n",
    "    )\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=int(epochs),\n",
    "        batch_size=int(batch_size),\n",
    "        validation_split=0.2,\n",
    "        verbose=0\n",
    "    )\n",
    "    return np.max(history.history['val_accuracy'])\n",
    "\n",
    "# ============================================\n",
    "# 5. Optimize Hyperparameters\n",
    "# ============================================\n",
    "def optimize_hyperparameters(X_train, y_train):\n",
    "    pbounds = {\n",
    "        'num_units': (32, 128),\n",
    "        'dropout_rate': (0.1, 0.5),\n",
    "        'epochs': (50, 100),\n",
    "        'batch_size': (16, 64)\n",
    "    }\n",
    "\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=lambda num_units, dropout_rate, epochs, batch_size: objective_function(\n",
    "            num_units, dropout_rate, epochs, batch_size, X_train, y_train\n",
    "        ),\n",
    "        pbounds=pbounds,\n",
    "        random_state=42,\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    optimizer.maximize(init_points=5, n_iter=15)\n",
    "    return optimizer.max['params']\n",
    "\n",
    "# ============================================\n",
    "# 6. Train & Evaluate Model\n",
    "# ============================================\n",
    "def train_and_evaluate_model(X_train, X_test, y_train, y_test, best_params):\n",
    "    model = build_cnn_gatt_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        num_classes=4,\n",
    "        num_units=int(best_params['num_units']),\n",
    "        dropout_rate=float(best_params['dropout_rate'])\n",
    "    )\n",
    "\n",
    "    start_train = time.time()\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=int(best_params['epochs']),\n",
    "        batch_size=int(best_params['batch_size']),\n",
    "        validation_split=0.2,\n",
    "        verbose=1\n",
    "    )\n",
    "    training_time = time.time() - start_train\n",
    "\n",
    "    start_infer = time.time()\n",
    "    y_pred = model.predict(X_test).argmax(axis=1)\n",
    "    inference_time = (time.time() - start_infer) / len(X_test)\n",
    "    testing_time = time.time() - start_infer\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(\"\\n===== Evaluation =====\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(\"Report:\\n\", classification_report(y_test, y_pred, digits=4))\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "    model.save(\"temp_model.h5\", include_optimizer=False)\n",
    "    model_size = os.path.getsize(\"temp_model.h5\") / (1024 ** 2)\n",
    "    os.remove(\"temp_model.h5\")\n",
    "\n",
    "    total_params = np.sum([np.prod(v.shape) for v in model.trainable_weights])\n",
    "    ram_usage = psutil.Process(os.getpid()).memory_info().rss / (1024 ** 2)\n",
    "\n",
    "    print(\"\\n===== Model Metrics =====\")\n",
    "    print(f\"Training Time: {training_time:.2f}s\")\n",
    "    print(f\"Inference/sample: {inference_time:.6f}s\")\n",
    "    print(f\"Testing Time: {testing_time:.2f}s\")\n",
    "    print(f\"Model Size: {model_size:.2f} MB\")\n",
    "    print(f\"Trainable Params: {total_params:,}\")\n",
    "    print(f\"RAM Usage: {ram_usage:.2f} MB\")\n",
    "\n",
    "    return model, history\n",
    "\n",
    "# ============================================\n",
    "# 7. Main\n",
    "# ============================================\n",
    "def main():\n",
    "    train_path = r\"E:\\Abroad period research\\Phenology datasets\\PHENOLOGY_H1\\jaen_train.csv\"\n",
    "    test_path = r\"E:\\Abroad period research\\Phenology datasets\\PHENOLOGY_H1\\jaen_test.csv\"\n",
    "\n",
    "    df_train, target_col = load_and_preprocess_data(train_path)\n",
    "    X_train, y_train, _ = feature_scaling(df_train, target_col)\n",
    "\n",
    "    df_test, _ = load_and_preprocess_data(test_path)\n",
    "    X_test, y_test, _ = feature_scaling(df_test, target_col)\n",
    "\n",
    "    print(\"\\n Optimizing Hyperparameters...\")\n",
    "    best_params = optimize_hyperparameters(X_train, y_train)\n",
    "    print(\"\\n Best Params:\", best_params)\n",
    "\n",
    "    print(\"\\n Training Final Gate Attention CNN Model...\")\n",
    "    model, history = train_and_evaluate_model(X_train, X_test, y_train, y_test, best_params)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4127b69b",
   "metadata": {},
   "source": [
    "Sevilla station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "854864ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Optimizing Hyperparameters...\n",
      "|   iter    |  target   | batch_... | dropou... |  epochs   | num_units |\n",
      "-------------------------------------------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.9714   \u001b[39m | \u001b[39m33.98    \u001b[39m | \u001b[39m0.4803   \u001b[39m | \u001b[39m86.6     \u001b[39m | \u001b[39m89.47    \u001b[39m |\n",
      "| \u001b[35m2        \u001b[39m | \u001b[35m1.0      \u001b[39m | \u001b[35m23.49    \u001b[39m | \u001b[35m0.1624   \u001b[39m | \u001b[35m52.9     \u001b[39m | \u001b[35m115.2    \u001b[39m |\n",
      "| \u001b[39m3        \u001b[39m | \u001b[39m0.9714   \u001b[39m | \u001b[39m44.85    \u001b[39m | \u001b[39m0.3832   \u001b[39m | \u001b[39m51.03    \u001b[39m | \u001b[39m125.1    \u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m0.9429   \u001b[39m | \u001b[39m55.96    \u001b[39m | \u001b[39m0.1849   \u001b[39m | \u001b[39m59.09    \u001b[39m | \u001b[39m49.61    \u001b[39m |\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m0.9571   \u001b[39m | \u001b[39m30.6     \u001b[39m | \u001b[39m0.3099   \u001b[39m | \u001b[39m71.6     \u001b[39m | \u001b[39m59.96    \u001b[39m |\n",
      "| \u001b[39m6        \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m16.58    \u001b[39m | \u001b[39m0.1036   \u001b[39m | \u001b[39m67.85    \u001b[39m | \u001b[39m120.9    \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m16.13    \u001b[39m | \u001b[39m0.227    \u001b[39m | \u001b[39m54.36    \u001b[39m | \u001b[39m97.41    \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m23.86    \u001b[39m | \u001b[39m0.2556   \u001b[39m | \u001b[39m51.96    \u001b[39m | \u001b[39m114.0    \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m16.41    \u001b[39m | \u001b[39m0.4657   \u001b[39m | \u001b[39m98.87    \u001b[39m | \u001b[39m127.3    \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m0.9857   \u001b[39m | \u001b[39m16.77    \u001b[39m | \u001b[39m0.3563   \u001b[39m | \u001b[39m52.13    \u001b[39m | \u001b[39m127.0    \u001b[39m |\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m62.65    \u001b[39m | \u001b[39m0.3616   \u001b[39m | \u001b[39m99.98    \u001b[39m | \u001b[39m127.8    \u001b[39m |\n",
      "| \u001b[39m12       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m38.83    \u001b[39m | \u001b[39m0.1855   \u001b[39m | \u001b[39m96.97    \u001b[39m | \u001b[39m126.9    \u001b[39m |\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m0.9714   \u001b[39m | \u001b[39m17.81    \u001b[39m | \u001b[39m0.2956   \u001b[39m | \u001b[39m98.12    \u001b[39m | \u001b[39m32.18    \u001b[39m |\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m63.14    \u001b[39m | \u001b[39m0.194    \u001b[39m | \u001b[39m99.69    \u001b[39m | \u001b[39m106.0    \u001b[39m |\n",
      "| \u001b[39m15       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m16.63    \u001b[39m | \u001b[39m0.3739   \u001b[39m | \u001b[39m99.5     \u001b[39m | \u001b[39m107.7    \u001b[39m |\n",
      "| \u001b[39m16       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m16.13    \u001b[39m | \u001b[39m0.155    \u001b[39m | \u001b[39m69.8     \u001b[39m | \u001b[39m106.4    \u001b[39m |\n",
      "| \u001b[39m17       \u001b[39m | \u001b[39m0.9857   \u001b[39m | \u001b[39m63.9     \u001b[39m | \u001b[39m0.3816   \u001b[39m | \u001b[39m80.67    \u001b[39m | \u001b[39m118.9    \u001b[39m |\n",
      "| \u001b[39m18       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m47.87    \u001b[39m | \u001b[39m0.4527   \u001b[39m | \u001b[39m99.36    \u001b[39m | \u001b[39m114.1    \u001b[39m |\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m26.12    \u001b[39m | \u001b[39m0.1964   \u001b[39m | \u001b[39m84.98    \u001b[39m | \u001b[39m119.2    \u001b[39m |\n",
      "| \u001b[39m20       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m16.13    \u001b[39m | \u001b[39m0.4645   \u001b[39m | \u001b[39m54.27    \u001b[39m | \u001b[39m109.8    \u001b[39m |\n",
      "=========================================================================\n",
      "\n",
      " Best Params: {'batch_size': 23.488894741236955, 'dropout_rate': 0.16239780813448107, 'epochs': 52.90418060840997, 'num_units': 115.15290999439378}\n",
      "\n",
      " Training Final Gate Attention CNN Model...\n",
      "Epoch 1/52\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.1130 - loss: 1.4843 - val_accuracy: 0.5429 - val_loss: 1.2531\n",
      "Epoch 2/52\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5233 - loss: 1.2276 - val_accuracy: 0.5143 - val_loss: 1.0952\n",
      "Epoch 3/52\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4800 - loss: 1.0932 - val_accuracy: 0.5143 - val_loss: 0.9751\n",
      "Epoch 4/52\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5537 - loss: 0.9956 - val_accuracy: 0.7429 - val_loss: 0.8635\n",
      "Epoch 5/52\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7303 - loss: 0.8720 - val_accuracy: 0.8571 - val_loss: 0.7286\n",
      "Epoch 6/52\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8339 - loss: 0.7500 - val_accuracy: 0.9000 - val_loss: 0.6016\n",
      "Epoch 7/52\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8419 - loss: 0.6296 - val_accuracy: 0.8857 - val_loss: 0.4836\n",
      "Epoch 8/52\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8379 - loss: 0.5348 - val_accuracy: 0.9000 - val_loss: 0.4069\n",
      "Epoch 9/52\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8544 - loss: 0.4775 - val_accuracy: 0.9143 - val_loss: 0.3450\n",
      "Epoch 10/52\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8629 - loss: 0.4045 - val_accuracy: 0.9000 - val_loss: 0.3112\n",
      "Epoch 11/52\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8384 - loss: 0.4117 - val_accuracy: 0.9000 - val_loss: 0.2863\n",
      "Epoch 12/52\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8796 - loss: 0.3734 - val_accuracy: 0.9143 - val_loss: 0.2668\n",
      "Epoch 13/52\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8500 - loss: 0.3588 - val_accuracy: 0.9000 - val_loss: 0.2495\n",
      "Epoch 14/52\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8625 - loss: 0.3425 - val_accuracy: 0.9143 - val_loss: 0.2296\n",
      "Epoch 15/52\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8623 - loss: 0.3480 - val_accuracy: 0.9286 - val_loss: 0.2239\n",
      "Epoch 16/52\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8868 - loss: 0.3082 - val_accuracy: 0.9143 - val_loss: 0.2078\n",
      "Epoch 17/52\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9008 - loss: 0.2860 - val_accuracy: 0.9286 - val_loss: 0.1961\n",
      "Epoch 18/52\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8834 - loss: 0.3085 - val_accuracy: 0.9143 - val_loss: 0.1970\n",
      "Epoch 19/52\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8952 - loss: 0.2729 - val_accuracy: 0.9571 - val_loss: 0.1750\n",
      "Epoch 20/52\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8706 - loss: 0.2867 - val_accuracy: 0.9286 - val_loss: 0.1694\n",
      "Epoch 21/52\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8975 - loss: 0.2520 - val_accuracy: 0.9429 - val_loss: 0.1551\n",
      "Epoch 22/52\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9120 - loss: 0.2330 - val_accuracy: 0.9429 - val_loss: 0.1475\n",
      "Epoch 23/52\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8911 - loss: 0.2351 - val_accuracy: 0.9571 - val_loss: 0.1347\n",
      "Epoch 24/52\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9168 - loss: 0.2273 - val_accuracy: 0.9571 - val_loss: 0.1309\n",
      "Epoch 25/52\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9223 - loss: 0.2126 - val_accuracy: 0.9714 - val_loss: 0.1200\n",
      "Epoch 26/52\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9299 - loss: 0.2028 - val_accuracy: 0.9571 - val_loss: 0.1187\n",
      "Epoch 27/52\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9240 - loss: 0.2109 - val_accuracy: 0.9571 - val_loss: 0.1235\n",
      "Epoch 28/52\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9298 - loss: 0.2073 - val_accuracy: 0.9714 - val_loss: 0.1011\n",
      "Epoch 29/52\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9401 - loss: 0.1824 - val_accuracy: 0.9714 - val_loss: 0.1014\n",
      "Epoch 30/52\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9312 - loss: 0.1932 - val_accuracy: 0.9714 - val_loss: 0.0945\n",
      "Epoch 31/52\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9352 - loss: 0.1820 - val_accuracy: 0.9714 - val_loss: 0.0936\n",
      "Epoch 32/52\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9284 - loss: 0.1694 - val_accuracy: 0.9714 - val_loss: 0.0850\n",
      "Epoch 33/52\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9312 - loss: 0.1516 - val_accuracy: 0.9714 - val_loss: 0.0808\n",
      "Epoch 34/52\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9388 - loss: 0.1701 - val_accuracy: 0.9714 - val_loss: 0.0837\n",
      "Epoch 35/52\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9542 - loss: 0.1531 - val_accuracy: 0.9857 - val_loss: 0.0671\n",
      "Epoch 36/52\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9534 - loss: 0.1452 - val_accuracy: 1.0000 - val_loss: 0.0649\n",
      "Epoch 37/52\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9556 - loss: 0.1298 - val_accuracy: 0.9857 - val_loss: 0.0629\n",
      "Epoch 38/52\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9479 - loss: 0.1320 - val_accuracy: 0.9714 - val_loss: 0.0628\n",
      "Epoch 39/52\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9472 - loss: 0.1502 - val_accuracy: 1.0000 - val_loss: 0.0564\n",
      "Epoch 40/52\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9617 - loss: 0.1286 - val_accuracy: 0.9857 - val_loss: 0.0566\n",
      "Epoch 41/52\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9605 - loss: 0.1261 - val_accuracy: 0.9857 - val_loss: 0.0618\n",
      "Epoch 42/52\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9575 - loss: 0.1227 - val_accuracy: 1.0000 - val_loss: 0.0481\n",
      "Epoch 43/52\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9500 - loss: 0.1150 - val_accuracy: 0.9857 - val_loss: 0.0614\n",
      "Epoch 44/52\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9565 - loss: 0.1171 - val_accuracy: 0.9857 - val_loss: 0.0481\n",
      "Epoch 45/52\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9630 - loss: 0.1022 - val_accuracy: 0.9857 - val_loss: 0.0518\n",
      "Epoch 46/52\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9586 - loss: 0.0884 - val_accuracy: 1.0000 - val_loss: 0.0444\n",
      "Epoch 47/52\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9648 - loss: 0.0835 - val_accuracy: 1.0000 - val_loss: 0.0391\n",
      "Epoch 48/52\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9573 - loss: 0.0974 - val_accuracy: 1.0000 - val_loss: 0.0379\n",
      "Epoch 49/52\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9654 - loss: 0.0955 - val_accuracy: 1.0000 - val_loss: 0.0437\n",
      "Epoch 50/52\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9456 - loss: 0.1182 - val_accuracy: 1.0000 - val_loss: 0.0418\n",
      "Epoch 51/52\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9662 - loss: 0.0983 - val_accuracy: 1.0000 - val_loss: 0.0329\n",
      "Epoch 52/52\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9669 - loss: 0.0953 - val_accuracy: 1.0000 - val_loss: 0.0375\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000022A93DA5EE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 13 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000022A93DA5EE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Evaluation =====\n",
      "Accuracy: 0.8969\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5385    0.7000    0.6087        10\n",
      "           1     0.9583    0.9079    0.9324        76\n",
      "           2     0.9832    0.9141    0.9474       128\n",
      "           3     0.3684    0.7778    0.5000         9\n",
      "\n",
      "    accuracy                         0.8969       223\n",
      "   macro avg     0.7121    0.8249    0.7471       223\n",
      "weighted avg     0.9300    0.8969    0.9090       223\n",
      "\n",
      "Confusion Matrix:\n",
      " [[  7   3   0   0]\n",
      " [  6  69   0   1]\n",
      " [  0   0 117  11]\n",
      " [  0   0   2   7]]\n",
      "\n",
      "===== Model Metrics =====\n",
      "Training Time: 9.26s\n",
      "Inference/sample: 0.001169s\n",
      "Testing Time: 0.26s\n",
      "Model Size: 0.11 MB\n",
      "Trainable Params: 22,254\n",
      "RAM Usage: 1654.73 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import psutil\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from bayes_opt import BayesianOptimization\n",
    "from scipy.fftpack import fft\n",
    "import warnings\n",
    "\n",
    "# Set seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================\n",
    "# 1. Data Loading and Preprocessing\n",
    "# ============================================\n",
    "def load_and_preprocess_data(filepath):\n",
    "    df = pd.read_csv(filepath, delimiter=\",\", low_memory=False)\n",
    "    df.columns = df.columns.str.replace('\"', '').str.strip()\n",
    "\n",
    "    if 'TIME' not in df.columns:\n",
    "        raise KeyError(\"The column 'TIME' does not exist in the dataset.\")\n",
    "\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'], errors='coerce')\n",
    "    df.dropna(subset=['TIME'], inplace=True)\n",
    "    df.set_index('TIME', inplace=True)\n",
    "\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "    target_col = 'fenologia_h1'\n",
    "    if target_col not in df.columns:\n",
    "        raise KeyError(f\"The target column '{target_col}' does not exist.\")\n",
    "\n",
    "    df[target_col].interpolate(method='linear', inplace=True)\n",
    "    df[target_col] = df[target_col] - 1\n",
    "\n",
    "    for i in range(1, 4):\n",
    "        df[f'{target_col}_lag{i}'] = df[target_col].shift(i)\n",
    "\n",
    "    for window in [3, 6]:\n",
    "        df[f'{target_col}_roll_mean_{window}'] = df[target_col].rolling(window).mean()\n",
    "        df[f'{target_col}_roll_std_{window}'] = df[target_col].rolling(window).std()\n",
    "\n",
    "    # df['month'] = df.index.month\n",
    "    # df['weekofyear'] = df.index.isocalendar().week\n",
    "    # df['year'] = df.index.isocalendar().year\n",
    "    # df['sin_week'] = np.sin(2 * np.pi * df['weekofyear'] / 52)\n",
    "    # df['cos_week'] = np.cos(2 * np.pi * df['weekofyear'] / 52)\n",
    "    # df['EMA_3'] = df[target_col].ewm(span=3, adjust=False).mean()\n",
    "    # df['EMA_6'] = df[target_col].ewm(span=6, adjust=False).mean()\n",
    "    # df['correlation_target_month'] = df[target_col].rolling(window=6).corr(df['month'])\n",
    "    # df['correlation_target_week'] = df[target_col].rolling(window=6).corr(df['weekofyear'])\n",
    "\n",
    "    # fft_values = fft(df[target_col].dropna().values)\n",
    "    # fft_real = np.real(fft_values)[:len(df[target_col])]\n",
    "    # fft_imag = np.imag(fft_values)[:len(df[target_col])]\n",
    "    # df['fft_real'] = np.concatenate([fft_real, np.nan * np.ones(len(df) - len(fft_real))])\n",
    "    # df['fft_imag'] = np.concatenate([fft_imag, np.nan * np.ones(len(df) - len(fft_imag))])\n",
    "\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "    return df, target_col\n",
    "\n",
    "# ============================================\n",
    "# 2. Feature Scaling\n",
    "# ============================================\n",
    "def feature_scaling(df, target_col):\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))  # 3D\n",
    "    return X_scaled, y, scaler\n",
    "\n",
    "# ============================================\n",
    "# 3. CNN + Gate Attention Model\n",
    "# ============================================\n",
    "def gated_attention_block(inputs, units):\n",
    "    conv_output = layers.Conv1D(units, kernel_size=1, activation='relu')(inputs)\n",
    "    attention_scores = layers.Dense(units, activation='sigmoid')(conv_output)\n",
    "    gated_output = layers.Multiply()([conv_output, attention_scores])\n",
    "    return gated_output\n",
    "\n",
    "def build_cnn_gatt_model(input_shape, num_classes=4, num_units=64, dropout_rate=0.2):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = gated_attention_block(inputs, units=num_units)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    x = layers.Dense(num_units // 2, activation='relu')(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# ============================================\n",
    "# 4. Bayesian Optimization Objective\n",
    "# ============================================\n",
    "def objective_function(num_units, dropout_rate, epochs, batch_size, X_train, y_train):\n",
    "    model = build_cnn_gatt_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        num_classes=4,\n",
    "        num_units=int(num_units),\n",
    "        dropout_rate=dropout_rate\n",
    "    )\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=int(epochs),\n",
    "        batch_size=int(batch_size),\n",
    "        validation_split=0.2,\n",
    "        verbose=0\n",
    "    )\n",
    "    return np.max(history.history['val_accuracy'])\n",
    "\n",
    "# ============================================\n",
    "# 5. Optimize Hyperparameters\n",
    "# ============================================\n",
    "def optimize_hyperparameters(X_train, y_train):\n",
    "    pbounds = {\n",
    "        'num_units': (32, 128),\n",
    "        'dropout_rate': (0.1, 0.5),\n",
    "        'epochs': (50, 100),\n",
    "        'batch_size': (16, 64)\n",
    "    }\n",
    "\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=lambda num_units, dropout_rate, epochs, batch_size: objective_function(\n",
    "            num_units, dropout_rate, epochs, batch_size, X_train, y_train\n",
    "        ),\n",
    "        pbounds=pbounds,\n",
    "        random_state=42,\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    optimizer.maximize(init_points=5, n_iter=15)\n",
    "    return optimizer.max['params']\n",
    "\n",
    "# ============================================\n",
    "# 6. Train & Evaluate Model\n",
    "# ============================================\n",
    "def train_and_evaluate_model(X_train, X_test, y_train, y_test, best_params):\n",
    "    model = build_cnn_gatt_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        num_classes=4,\n",
    "        num_units=int(best_params['num_units']),\n",
    "        dropout_rate=float(best_params['dropout_rate'])\n",
    "    )\n",
    "\n",
    "    start_train = time.time()\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=int(best_params['epochs']),\n",
    "        batch_size=int(best_params['batch_size']),\n",
    "        validation_split=0.2,\n",
    "        verbose=1\n",
    "    )\n",
    "    training_time = time.time() - start_train\n",
    "\n",
    "    start_infer = time.time()\n",
    "    y_pred = model.predict(X_test).argmax(axis=1)\n",
    "    inference_time = (time.time() - start_infer) / len(X_test)\n",
    "    testing_time = time.time() - start_infer\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(\"\\n===== Evaluation =====\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(\"Report:\\n\", classification_report(y_test, y_pred, digits=4))\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "    model.save(\"temp_model.h5\", include_optimizer=False)\n",
    "    model_size = os.path.getsize(\"temp_model.h5\") / (1024 ** 2)\n",
    "    os.remove(\"temp_model.h5\")\n",
    "\n",
    "    total_params = np.sum([np.prod(v.shape) for v in model.trainable_weights])\n",
    "    ram_usage = psutil.Process(os.getpid()).memory_info().rss / (1024 ** 2)\n",
    "\n",
    "    print(\"\\n===== Model Metrics =====\")\n",
    "    print(f\"Training Time: {training_time:.2f}s\")\n",
    "    print(f\"Inference/sample: {inference_time:.6f}s\")\n",
    "    print(f\"Testing Time: {testing_time:.2f}s\")\n",
    "    print(f\"Model Size: {model_size:.2f} MB\")\n",
    "    print(f\"Trainable Params: {total_params:,}\")\n",
    "    print(f\"RAM Usage: {ram_usage:.2f} MB\")\n",
    "\n",
    "    return model, history\n",
    "\n",
    "# ============================================\n",
    "# 7. Main\n",
    "# ============================================\n",
    "def main():\n",
    "    train_path = r\"E:\\Abroad period research\\Phenology datasets\\PHENOLOGY_H1\\sevilla_train.csv\"\n",
    "    test_path = r\"E:\\Abroad period research\\Phenology datasets\\PHENOLOGY_H1\\sevilla_test.csv\"\n",
    "\n",
    "    df_train, target_col = load_and_preprocess_data(train_path)\n",
    "    X_train, y_train, _ = feature_scaling(df_train, target_col)\n",
    "\n",
    "    df_test, _ = load_and_preprocess_data(test_path)\n",
    "    X_test, y_test, _ = feature_scaling(df_test, target_col)\n",
    "\n",
    "    print(\"\\n Optimizing Hyperparameters...\")\n",
    "    best_params = optimize_hyperparameters(X_train, y_train)\n",
    "    print(\"\\n Best Params:\", best_params)\n",
    "\n",
    "    print(\"\\n Training Final Gate Attention CNN Model...\")\n",
    "    model, history = train_and_evaluate_model(X_train, X_test, y_train, y_test, best_params)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
