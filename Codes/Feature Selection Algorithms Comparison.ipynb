{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991697e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Mutual Info...\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\n",
      "‚úÖ Accuracy: 0.8664\n",
      "üìä Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.44      0.61        16\n",
      "           1       0.88      1.00      0.94        69\n",
      "           2       1.00      0.84      0.91       137\n",
      "           3       0.31      1.00      0.48        10\n",
      "\n",
      "    accuracy                           0.87       232\n",
      "   macro avg       0.80      0.82      0.73       232\n",
      "weighted avg       0.94      0.87      0.88       232\n",
      "\n",
      "üßæ Confusion Matrix:\n",
      " [[  7   9   0   0]\n",
      " [  0  69   0   0]\n",
      " [  0   0 115  22]\n",
      " [  0   0   0  10]]\n",
      "üîç Pearson Corr...\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\n",
      "‚úÖ Accuracy: 0.8621\n",
      "üìä Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.50      0.67        16\n",
      "           1       0.90      1.00      0.95        69\n",
      "           2       1.00      0.82      0.90       137\n",
      "           3       0.29      1.00      0.45        10\n",
      "\n",
      "    accuracy                           0.86       232\n",
      "   macro avg       0.80      0.83      0.74       232\n",
      "weighted avg       0.94      0.86      0.88       232\n",
      "\n",
      "üßæ Confusion Matrix:\n",
      " [[  8   8   0   0]\n",
      " [  0  69   0   0]\n",
      " [  0   0 113  24]\n",
      " [  0   0   0  10]]\n",
      "üîç Variance Threshold...\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\n",
      "‚úÖ Accuracy: 0.8362\n",
      "üìä Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.56      0.72        16\n",
      "           1       0.90      0.93      0.91        69\n",
      "           2       0.96      0.81      0.88       137\n",
      "           3       0.28      1.00      0.43        10\n",
      "\n",
      "    accuracy                           0.84       232\n",
      "   macro avg       0.78      0.83      0.74       232\n",
      "weighted avg       0.91      0.84      0.86       232\n",
      "\n",
      "üßæ Confusion Matrix:\n",
      " [[  9   7   0   0]\n",
      " [  0  64   5   0]\n",
      " [  0   0 111  26]\n",
      " [  0   0   0  10]]\n",
      "üîç RFE...\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\n",
      "‚úÖ Accuracy: 0.8276\n",
      "üìä Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        16\n",
      "           1       0.80      0.94      0.87        69\n",
      "           2       0.97      0.85      0.91       137\n",
      "           3       0.33      1.00      0.50        10\n",
      "\n",
      "    accuracy                           0.83       232\n",
      "   macro avg       0.53      0.70      0.57       232\n",
      "weighted avg       0.82      0.83      0.81       232\n",
      "\n",
      "üßæ Confusion Matrix:\n",
      " [[  0  16   0   0]\n",
      " [  0  65   4   0]\n",
      " [  0   0 117  20]\n",
      " [  0   0   0  10]]\n",
      "üîç PSO...\n",
      "Stopping search: maximum iterations reached --> 3\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\n",
      "‚úÖ Accuracy: 0.8233\n",
      "üìä Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.50      0.67        16\n",
      "           1       0.88      0.94      0.91        69\n",
      "           2       0.96      0.79      0.87       137\n",
      "           3       0.26      1.00      0.42        10\n",
      "\n",
      "    accuracy                           0.82       232\n",
      "   macro avg       0.78      0.81      0.71       232\n",
      "weighted avg       0.91      0.82      0.85       232\n",
      "\n",
      "üßæ Confusion Matrix:\n",
      " [[  8   8   0   0]\n",
      " [  0  65   4   0]\n",
      " [  0   1 108  28]\n",
      " [  0   0   0  10]]\n",
      "üîç Lasso...\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "\n",
      "‚úÖ Accuracy: 0.7586\n",
      "üìä Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        16\n",
      "           1       0.81      0.97      0.88        69\n",
      "           2       0.98      0.72      0.83       137\n",
      "           3       0.21      1.00      0.34        10\n",
      "\n",
      "    accuracy                           0.76       232\n",
      "   macro avg       0.50      0.67      0.51       232\n",
      "weighted avg       0.83      0.76      0.77       232\n",
      "\n",
      "üßæ Confusion Matrix:\n",
      " [[ 0 16  0  0]\n",
      " [ 0 67  2  0]\n",
      " [ 0  0 99 38]\n",
      " [ 0  0  0 10]]\n",
      "üîç Tree-Based...\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "\n",
      "‚úÖ Accuracy: 0.7241\n",
      "üìä Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.25      0.36        16\n",
      "           1       0.84      0.91      0.88        69\n",
      "           2       0.96      0.66      0.78       137\n",
      "           3       0.18      1.00      0.30        10\n",
      "\n",
      "    accuracy                           0.72       232\n",
      "   macro avg       0.66      0.71      0.58       232\n",
      "weighted avg       0.87      0.72      0.76       232\n",
      "\n",
      "üßæ Confusion Matrix:\n",
      " [[ 4 12  0  0]\n",
      " [ 2 63  4  0]\n",
      " [ 0  0 91 46]\n",
      " [ 0  0  0 10]]\n",
      "\n",
      "‚úÖ FINAL COMPARISON:\n",
      "MI        : 0.8664\n",
      "Pearson   : 0.8621\n",
      "Variance  : 0.8362\n",
      "RFE       : 0.8276\n",
      "PSO       : 0.8233\n",
      "Lasso     : 0.7586\n",
      "Tree      : 0.7241\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "#  IMPORTS\n",
    "# ===========================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_selection import mutual_info_classif, RFE, VarianceThreshold\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# ===========================================\n",
    "#  DATA LOADING + PREPROCESSING\n",
    "# ===========================================\n",
    "def load_and_preprocess_data(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df.columns = df.columns.str.replace('\"', '').str.strip()\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'], errors='coerce')\n",
    "    df.dropna(subset=['TIME'], inplace=True)\n",
    "    df.set_index('TIME', inplace=True)\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "    target_col = 'fenologia_h1'\n",
    "    df[target_col].interpolate(method='linear', inplace=True)\n",
    "    df[target_col] = df[target_col] - 1\n",
    "\n",
    "    for i in range(1, 4):\n",
    "        df[f'{target_col}_lag{i}'] = df[target_col].shift(i)\n",
    "\n",
    "    for window in [3, 6]:\n",
    "        df[f'{target_col}_roll_mean_{window}'] = df[target_col].rolling(window).mean()\n",
    "        df[f'{target_col}_roll_std_{window}'] = df[target_col].rolling(window).std()\n",
    "\n",
    "    df['month'] = df.index.month\n",
    "    df['weekofyear'] = df.index.isocalendar().week\n",
    "    df['year'] = df.index.isocalendar().year\n",
    "    df['sin_week'] = np.sin(2 * np.pi * df['weekofyear'] / 52)\n",
    "    df['cos_week'] = np.cos(2 * np.pi * df['weekofyear'] / 52)\n",
    "\n",
    "    df['EMA_3'] = df[target_col].ewm(span=3, adjust=False).mean()\n",
    "    df['EMA_6'] = df[target_col].ewm(span=6, adjust=False).mean()\n",
    "\n",
    "    df['correlation_target_month'] = df[target_col].rolling(window=6).corr(df['month'])\n",
    "    df['correlation_target_week'] = df[target_col].rolling(window=6).corr(df['weekofyear'])\n",
    "\n",
    "    fft_values = np.fft.fft(df[target_col].dropna().values)\n",
    "    fft_real = np.real(fft_values)[:len(df[target_col])]\n",
    "    fft_imag = np.imag(fft_values)[:len(df[target_col])]\n",
    "    df['fft_real'] = np.concatenate([fft_real, np.nan * np.ones(len(df) - len(fft_real))])\n",
    "    df['fft_imag'] = np.concatenate([fft_imag, np.nan * np.ones(len(df) - len(fft_imag))])\n",
    "\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "    return df, target_col\n",
    "\n",
    "# ===========================================\n",
    "#  SCALING\n",
    "# ===========================================\n",
    "def feature_scaling(df, target_col):\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    return pd.DataFrame(X_scaled, columns=X.columns), y, scaler\n",
    "\n",
    "# ===========================================\n",
    "#  MODEL\n",
    "# ===========================================\n",
    "def build_mlp_model(input_dim, num_classes=4, num_units=64, dropout_rate=0.2, activation='relu'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_units, activation=activation, input_dim=input_dim))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(num_units // 2, activation=activation))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# ===========================================\n",
    "#  FEATURE SELECTION\n",
    "# ===========================================\n",
    "def mutual_information_feature_selection(X, y):\n",
    "    scores = mutual_info_classif(X, y)\n",
    "    return scores > 0.01\n",
    "\n",
    "def pearson_correlation_feature_selection(X, y):\n",
    "    scores = np.array([abs(pearsonr(X[col], y)[0]) for col in X.columns])\n",
    "    return scores > 0.1\n",
    "\n",
    "def variance_threshold_feature_selection(X, threshold=0.01):\n",
    "    selector = VarianceThreshold(threshold)\n",
    "    selector.fit(X)\n",
    "    return selector.get_support()\n",
    "\n",
    "def rfe_feature_selection(X, y, n_features_to_select=10):\n",
    "    rfe = RFE(SVC(kernel=\"linear\"), n_features_to_select=n_features_to_select)\n",
    "    rfe.fit(X, y)\n",
    "    return rfe.support_\n",
    "\n",
    "def pso_feature_selection(X, y):\n",
    "    def fitness_function(features):\n",
    "        mask = features > 0.5\n",
    "        if mask.sum() == 0:\n",
    "            return 1.0\n",
    "        clf = RandomForestClassifier()\n",
    "        clf.fit(X.iloc[:, mask], y)\n",
    "        acc = clf.score(X.iloc[:, mask], y)\n",
    "        return 1 - acc\n",
    "\n",
    "    lb = np.zeros(X.shape[1])\n",
    "    ub = np.ones(X.shape[1])\n",
    "    best_pos, _ = pso(fitness_function, lb, ub, swarmsize=10, maxiter=3)\n",
    "    return best_pos > 0.5\n",
    "\n",
    "def lasso_feature_selection(X, y):\n",
    "    model = Lasso(alpha=0.01)\n",
    "    model.fit(X, y)\n",
    "    return np.abs(model.coef_) > 0.001\n",
    "\n",
    "def tree_based_feature_selection(X, y):\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(X, y)\n",
    "    return model.feature_importances_ > np.mean(model.feature_importances_)\n",
    "\n",
    "# ===========================================\n",
    "#  TRAINING AND EVALUATION\n",
    "# ===========================================\n",
    "def train_and_evaluate_model(X_train, X_test, y_train, y_test, selected_features):\n",
    "    selected_columns = X_train.columns[selected_features]\n",
    "    X_train_sel = X_train[selected_columns]\n",
    "    X_test_sel = X_test[selected_columns]\n",
    "\n",
    "    model = build_mlp_model(input_dim=X_train_sel.shape[1])\n",
    "    model.fit(X_train_sel, y_train, epochs=30, batch_size=32, validation_split=0.2, verbose=0)\n",
    "    y_pred = model.predict(X_test_sel).argmax(axis=1)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"\\n Accuracy: {acc:.4f}\")\n",
    "    print(\" Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    print(\" Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    return acc, model\n",
    "\n",
    "# ===========================================\n",
    "#  MAIN\n",
    "# ===========================================\n",
    "def main():\n",
    "    train_fp = r\"E:\\\\Abroad period research\\\\Phenology datasets\\\\PHENOLOGY_H1\\\\Cadiz_train.csv\"\n",
    "    test_fp  = r\"E:\\\\Abroad period research\\\\Phenology datasets\\\\PHENOLOGY_H1\\\\Cadiz_test.csv\"\n",
    "\n",
    "    df_train, target_col = load_and_preprocess_data(train_fp)\n",
    "    X_train, y_train, scaler = feature_scaling(df_train, target_col)\n",
    "\n",
    "    df_test, _ = load_and_preprocess_data(test_fp)\n",
    "    X_test, y_test, _ = feature_scaling(df_test, target_col)\n",
    "\n",
    "    print(\" Mutual Info...\")\n",
    "    acc1, _ = train_and_evaluate_model(X_train, X_test, y_train, y_test, mutual_information_feature_selection(X_train, y_train))\n",
    "\n",
    "    print(\" Pearson Corr...\")\n",
    "    acc2, _ = train_and_evaluate_model(X_train, X_test, y_train, y_test, pearson_correlation_feature_selection(X_train, y_train))\n",
    "\n",
    "    print(\" Variance Threshold...\")\n",
    "    acc3, _ = train_and_evaluate_model(X_train, X_test, y_train, y_test, variance_threshold_feature_selection(X_train))\n",
    "\n",
    "    print(\" RFE...\")\n",
    "    acc4, _ = train_and_evaluate_model(X_train, X_test, y_train, y_test, rfe_feature_selection(X_train, y_train))\n",
    "\n",
    "    print(\" PSO...\")\n",
    "    acc5, _ = train_and_evaluate_model(X_train, X_test, y_train, y_test, pso_feature_selection(X_train, y_train))\n",
    "\n",
    "    print(\" Lasso...\")\n",
    "    acc6, _ = train_and_evaluate_model(X_train, X_test, y_train, y_test, lasso_feature_selection(X_train, y_train))\n",
    "\n",
    "    print(\" Tree-Based...\")\n",
    "    acc7, _ = train_and_evaluate_model(X_train, X_test, y_train, y_test, tree_based_feature_selection(X_train, y_train))\n",
    "\n",
    "    print(\"\\n FINAL COMPARISON:\")\n",
    "    for name, acc in zip([\"MI\", \"Pearson\", \"Variance\", \"RFE\", \"PSO\", \"Lasso\", \"Tree\"],\n",
    "                         [acc1, acc2, acc3, acc4, acc5, acc6, acc7]):\n",
    "        print(f\"{name:10s}: {acc:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54319753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Mutual Info (Filter)...\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "\n",
      "‚úÖ Accuracy: 0.8491\n",
      "üìä Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.50      0.67        16\n",
      "           1       0.86      0.94      0.90        69\n",
      "           2       0.97      0.83      0.89       137\n",
      "           3       0.33      1.00      0.50        10\n",
      "\n",
      "    accuracy                           0.85       232\n",
      "   macro avg       0.79      0.82      0.74       232\n",
      "weighted avg       0.91      0.85      0.86       232\n",
      "\n",
      "üßæ Confusion Matrix:\n",
      " [[  8   8   0   0]\n",
      " [  0  65   4   0]\n",
      " [  0   3 114  20]\n",
      " [  0   0   0  10]]\n",
      "üîç F-ANOVA (Filter)...\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "‚úÖ Accuracy: 0.7672\n",
      "üìä Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.25      0.36        16\n",
      "           1       0.82      0.91      0.86        69\n",
      "           2       0.96      0.74      0.83       137\n",
      "           3       0.23      1.00      0.37        10\n",
      "\n",
      "    accuracy                           0.77       232\n",
      "   macro avg       0.67      0.73      0.61       232\n",
      "weighted avg       0.87      0.77      0.79       232\n",
      "\n",
      "üßæ Confusion Matrix:\n",
      " [[  4  12   0   0]\n",
      " [  2  63   4   0]\n",
      " [  0   2 101  34]\n",
      " [  0   0   0  10]]\n",
      "üîç RFE SVC (Wrapper)...\n",
      "WARNING:tensorflow:5 out of the last 17 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000002118286A160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\n",
      "‚úÖ Accuracy: 0.8276\n",
      "üìä Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        16\n",
      "           1       0.80      0.94      0.87        69\n",
      "           2       0.97      0.85      0.91       137\n",
      "           3       0.33      1.00      0.50        10\n",
      "\n",
      "    accuracy                           0.83       232\n",
      "   macro avg       0.53      0.70      0.57       232\n",
      "weighted avg       0.82      0.83      0.81       232\n",
      "\n",
      "üßæ Confusion Matrix:\n",
      " [[  0  16   0   0]\n",
      " [  0  65   4   0]\n",
      " [  0   0 117  20]\n",
      " [  0   0   0  10]]\n",
      "üîç Logistic Regression Wrapper...\n",
      "WARNING:tensorflow:5 out of the last 17 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000021184A75A80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "\n",
      "‚úÖ Accuracy: 0.8233\n",
      "üìä Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.44      0.61        16\n",
      "           1       0.88      0.94      0.91        69\n",
      "           2       0.96      0.80      0.87       137\n",
      "           3       0.26      1.00      0.42        10\n",
      "\n",
      "    accuracy                           0.82       232\n",
      "   macro avg       0.78      0.79      0.70       232\n",
      "weighted avg       0.91      0.82      0.85       232\n",
      "\n",
      "üßæ Confusion Matrix:\n",
      " [[  7   9   0   0]\n",
      " [  0  65   4   0]\n",
      " [  0   0 109  28]\n",
      " [  0   0   0  10]]\n",
      "üîç Lasso (Embedded)...\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\n",
      "‚úÖ Accuracy: 0.7586\n",
      "üìä Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        16\n",
      "           1       0.80      0.94      0.87        69\n",
      "           2       0.96      0.74      0.83       137\n",
      "           3       0.22      1.00      0.36        10\n",
      "\n",
      "    accuracy                           0.76       232\n",
      "   macro avg       0.50      0.67      0.51       232\n",
      "weighted avg       0.82      0.76      0.77       232\n",
      "\n",
      "üßæ Confusion Matrix:\n",
      " [[  0  16   0   0]\n",
      " [  0  65   4   0]\n",
      " [  0   0 101  36]\n",
      " [  0   0   0  10]]\n",
      "üîç Tree-Based (Embedded)...\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\n",
      "‚úÖ Accuracy: 0.6293\n",
      "üìä Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        16\n",
      "           1       0.80      0.94      0.87        69\n",
      "           2       0.95      0.52      0.67       137\n",
      "           3       0.13      1.00      0.23        10\n",
      "\n",
      "    accuracy                           0.63       232\n",
      "   macro avg       0.47      0.62      0.44       232\n",
      "weighted avg       0.80      0.63      0.66       232\n",
      "\n",
      "üßæ Confusion Matrix:\n",
      " [[ 0 16  0  0]\n",
      " [ 0 65  4  0]\n",
      " [ 0  0 71 66]\n",
      " [ 0  0  0 10]]\n",
      "\n",
      "‚úÖ FINAL COMPARISON:\n",
      "MI             : 0.8491\n",
      "F-Classif      : 0.7672\n",
      "RFE            : 0.8276\n",
      "LogRegWrapper  : 0.8233\n",
      "Lasso          : 0.7586\n",
      "Tree           : 0.6293\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "#  IMPORTS\n",
    "# ===========================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_selection import mutual_info_classif, RFE, VarianceThreshold, SelectKBest, f_classif\n",
    "from sklearn.linear_model import Lasso, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# ===========================================\n",
    "#  DATA LOADING + PREPROCESSING\n",
    "# ===========================================\n",
    "def load_and_preprocess_data(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df.columns = df.columns.str.replace('\"', '').str.strip()\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'], errors='coerce')\n",
    "    df.dropna(subset=['TIME'], inplace=True)\n",
    "    df.set_index('TIME', inplace=True)\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "    target_col = 'fenologia_h1'\n",
    "    df[target_col].interpolate(method='linear', inplace=True)\n",
    "    df[target_col] = df[target_col] - 1\n",
    "\n",
    "    for i in range(1, 4):\n",
    "        df[f'{target_col}_lag{i}'] = df[target_col].shift(i)\n",
    "\n",
    "    for window in [3, 6]:\n",
    "        df[f'{target_col}_roll_mean_{window}'] = df[target_col].rolling(window).mean()\n",
    "        df[f'{target_col}_roll_std_{window}'] = df[target_col].rolling(window).std()\n",
    "\n",
    "    df['month'] = df.index.month\n",
    "    df['weekofyear'] = df.index.isocalendar().week\n",
    "    df['year'] = df.index.isocalendar().year\n",
    "    df['sin_week'] = np.sin(2 * np.pi * df['weekofyear'] / 52)\n",
    "    df['cos_week'] = np.cos(2 * np.pi * df['weekofyear'] / 52)\n",
    "\n",
    "    df['EMA_3'] = df[target_col].ewm(span=3, adjust=False).mean()\n",
    "    df['EMA_6'] = df[target_col].ewm(span=6, adjust=False).mean()\n",
    "\n",
    "    df['correlation_target_month'] = df[target_col].rolling(window=6).corr(df['month'])\n",
    "    df['correlation_target_week'] = df[target_col].rolling(window=6).corr(df['weekofyear'])\n",
    "\n",
    "    fft_values = np.fft.fft(df[target_col].dropna().values)\n",
    "    fft_real = np.real(fft_values)[:len(df[target_col])]\n",
    "    fft_imag = np.imag(fft_values)[:len(df[target_col])]\n",
    "    df['fft_real'] = np.concatenate([fft_real, np.nan * np.ones(len(df) - len(fft_real))])\n",
    "    df['fft_imag'] = np.concatenate([fft_imag, np.nan * np.ones(len(df) - len(fft_imag))])\n",
    "\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "    return df, target_col\n",
    "\n",
    "# ===========================================\n",
    "#  SCALING\n",
    "# ===========================================\n",
    "def feature_scaling(df, target_col):\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    return pd.DataFrame(X_scaled, columns=X.columns), y, scaler\n",
    "\n",
    "# ===========================================\n",
    "#  MODEL\n",
    "# ===========================================\n",
    "def build_mlp_model(input_dim, num_classes=4, num_units=64, dropout_rate=0.2, activation='relu'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_units, activation=activation, input_dim=input_dim))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(num_units // 2, activation=activation))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# ===========================================\n",
    "#  FEATURE SELECTION\n",
    "# ===========================================\n",
    "def mutual_information_feature_selection(X, y):\n",
    "    scores = mutual_info_classif(X, y)\n",
    "    return scores > 0.01\n",
    "\n",
    "def pearson_correlation_feature_selection(X, y):\n",
    "    scores = np.array([abs(pearsonr(X[col], y)[0]) for col in X.columns])\n",
    "    return scores > 0.1\n",
    "\n",
    "def variance_threshold_feature_selection(X, threshold=0.01):\n",
    "    selector = VarianceThreshold(threshold)\n",
    "    selector.fit(X)\n",
    "    return selector.get_support()\n",
    "\n",
    "def rfe_feature_selection(X, y, n_features_to_select=10):\n",
    "    rfe = RFE(SVC(kernel=\"linear\"), n_features_to_select=n_features_to_select)\n",
    "    rfe.fit(X, y)\n",
    "    return rfe.support_\n",
    "\n",
    "def logistic_regression_wrapper_selection(X, y):\n",
    "    model = LogisticRegression(solver='liblinear')\n",
    "    rfe = RFE(model, n_features_to_select=10)\n",
    "    rfe.fit(X, y)\n",
    "    return rfe.support_\n",
    "\n",
    "def f_classif_filter_selection(X, y):\n",
    "    selector = SelectKBest(score_func=f_classif, k=10)\n",
    "    selector.fit(X, y)\n",
    "    return selector.get_support()\n",
    "\n",
    "def lasso_feature_selection(X, y):\n",
    "    model = Lasso(alpha=0.01)\n",
    "    model.fit(X, y)\n",
    "    return np.abs(model.coef_) > 0.001\n",
    "\n",
    "def tree_based_feature_selection(X, y):\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(X, y)\n",
    "    return model.feature_importances_ > np.mean(model.feature_importances_)\n",
    "\n",
    "# ===========================================\n",
    "#  TRAINING AND EVALUATION\n",
    "# ===========================================\n",
    "def train_and_evaluate_model(X_train, X_test, y_train, y_test, selected_features):\n",
    "    selected_columns = X_train.columns[selected_features]\n",
    "    X_train_sel = X_train[selected_columns]\n",
    "    X_test_sel = X_test[selected_columns]\n",
    "\n",
    "    model = build_mlp_model(input_dim=X_train_sel.shape[1])\n",
    "    model.fit(X_train_sel, y_train, epochs=30, batch_size=32, validation_split=0.2, verbose=0)\n",
    "    y_pred = model.predict(X_test_sel).argmax(axis=1)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"\\n Accuracy: {acc:.4f}\")\n",
    "    print(\" Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    print(\" Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    return acc, model\n",
    "\n",
    "# ===========================================\n",
    "#  MAIN\n",
    "# ===========================================\n",
    "def main():\n",
    "    train_fp = r\"E:\\\\Abroad period research\\\\Phenology datasets\\\\PHENOLOGY_H1\\\\Cadiz_train.csv\"\n",
    "    test_fp  = r\"E:\\\\Abroad period research\\\\Phenology datasets\\\\PHENOLOGY_H1\\\\Cadiz_test.csv\"\n",
    "\n",
    "    df_train, target_col = load_and_preprocess_data(train_fp)\n",
    "    X_train, y_train, scaler = feature_scaling(df_train, target_col)\n",
    "\n",
    "    df_test, _ = load_and_preprocess_data(test_fp)\n",
    "    X_test, y_test, _ = feature_scaling(df_test, target_col)\n",
    "\n",
    "    print(\" Mutual Info (Filter)...\")\n",
    "    acc1, _ = train_and_evaluate_model(X_train, X_test, y_train, y_test, mutual_information_feature_selection(X_train, y_train))\n",
    "\n",
    "    print(\" F-ANOVA (Filter)...\")\n",
    "    acc2, _ = train_and_evaluate_model(X_train, X_test, y_train, y_test, f_classif_filter_selection(X_train, y_train))\n",
    "\n",
    "    print(\" RFE SVC (Wrapper)...\")\n",
    "    acc3, _ = train_and_evaluate_model(X_train, X_test, y_train, y_test, rfe_feature_selection(X_train, y_train))\n",
    "\n",
    "    print(\" Logistic Regression Wrapper...\")\n",
    "    acc4, _ = train_and_evaluate_model(X_train, X_test, y_train, y_test, logistic_regression_wrapper_selection(X_train, y_train))\n",
    "\n",
    "    print(\" Lasso (Embedded)...\")\n",
    "    acc5, _ = train_and_evaluate_model(X_train, X_test, y_train, y_test, lasso_feature_selection(X_train, y_train))\n",
    "\n",
    "    print(\" Tree-Based (Embedded)...\")\n",
    "    acc6, _ = train_and_evaluate_model(X_train, X_test, y_train, y_test, tree_based_feature_selection(X_train, y_train))\n",
    "\n",
    "    print(\"\\n FINAL COMPARISON:\")\n",
    "    for name, acc in zip([\"MI\", \"F-Classif\", \"RFE\", \"LogRegWrapper\", \"Lasso\", \"Tree\"],\n",
    "                         [acc1, acc2, acc3, acc4, acc5, acc6]):\n",
    "        print(f\"{name:15s}: {acc:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b293173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Mutual Info (Filter)...\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\n",
      "‚úÖ Accuracy: 0.8793\n",
      "üìä Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.50      0.67        16\n",
      "           1       0.90      1.00      0.95        69\n",
      "           2       1.00      0.85      0.92       137\n",
      "           3       0.33      1.00      0.50        10\n",
      "\n",
      "    accuracy                           0.88       232\n",
      "   macro avg       0.81      0.84      0.76       232\n",
      "weighted avg       0.94      0.88      0.89       232\n",
      "\n",
      "üßæ Confusion Matrix:\n",
      " [[  8   8   0   0]\n",
      " [  0  69   0   0]\n",
      " [  0   0 117  20]\n",
      " [  0   0   0  10]]\n",
      "üîç F-ANOVA (Filter)...\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "\n",
      "‚úÖ Accuracy: 0.7198\n",
      "üìä Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.12      0.22        16\n",
      "           1       0.80      0.93      0.86        69\n",
      "           2       0.95      0.66      0.78       137\n",
      "           3       0.19      1.00      0.31        10\n",
      "\n",
      "    accuracy                           0.72       232\n",
      "   macro avg       0.73      0.68      0.54       232\n",
      "weighted avg       0.87      0.72      0.75       232\n",
      "\n",
      "üßæ Confusion Matrix:\n",
      " [[ 2 14  0  0]\n",
      " [ 0 64  5  0]\n",
      " [ 0  2 91 44]\n",
      " [ 0  0  0 10]]\n",
      "üîç Variance Threshold (Filter)...\n",
      "WARNING:tensorflow:5 out of the last 17 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000018FFFD0E7A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\n",
      "‚úÖ Accuracy: 0.8836\n",
      "üìä Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.56      0.72        16\n",
      "           1       0.91      1.00      0.95        69\n",
      "           2       1.00      0.85      0.92       137\n",
      "           3       0.33      1.00      0.50        10\n",
      "\n",
      "    accuracy                           0.88       232\n",
      "   macro avg       0.81      0.85      0.77       232\n",
      "weighted avg       0.94      0.88      0.90       232\n",
      "\n",
      "üßæ Confusion Matrix:\n",
      " [[  9   7   0   0]\n",
      " [  0  69   0   0]\n",
      " [  0   0 117  20]\n",
      " [  0   0   0  10]]\n",
      "üîç RFE SVC (Wrapper)...\n",
      "WARNING:tensorflow:5 out of the last 17 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000018F8576E2A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\n",
      "‚úÖ Accuracy: 0.8276\n",
      "üìä Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        16\n",
      "           1       0.80      0.94      0.87        69\n",
      "           2       0.97      0.85      0.91       137\n",
      "           3       0.33      1.00      0.50        10\n",
      "\n",
      "    accuracy                           0.83       232\n",
      "   macro avg       0.53      0.70      0.57       232\n",
      "weighted avg       0.82      0.83      0.81       232\n",
      "\n",
      "üßæ Confusion Matrix:\n",
      " [[  0  16   0   0]\n",
      " [  0  65   4   0]\n",
      " [  0   0 117  20]\n",
      " [  0   0   0  10]]\n",
      "üîç Logistic Regression Wrapper...\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "\n",
      "‚úÖ Accuracy: 0.7974\n",
      "üìä Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.31      0.48        16\n",
      "           1       0.86      0.94      0.90        69\n",
      "           2       0.96      0.77      0.85       137\n",
      "           3       0.24      1.00      0.38        10\n",
      "\n",
      "    accuracy                           0.80       232\n",
      "   macro avg       0.76      0.76      0.65       232\n",
      "weighted avg       0.90      0.80      0.82       232\n",
      "\n",
      "üßæ Confusion Matrix:\n",
      " [[  5  11   0   0]\n",
      " [  0  65   4   0]\n",
      " [  0   0 105  32]\n",
      " [  0   0   0  10]]\n",
      "üîç Forward Selection (Wrapper)...\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "\n",
      "‚úÖ Accuracy: 0.6940\n",
      "üìä Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        16\n",
      "           1       0.78      0.93      0.85        69\n",
      "           2       0.95      0.64      0.76       137\n",
      "           3       0.17      1.00      0.29        10\n",
      "\n",
      "    accuracy                           0.69       232\n",
      "   macro avg       0.47      0.64      0.48       232\n",
      "weighted avg       0.80      0.69      0.71       232\n",
      "\n",
      "üßæ Confusion Matrix:\n",
      " [[ 0 16  0  0]\n",
      " [ 0 64  5  0]\n",
      " [ 0  2 87 48]\n",
      " [ 0  0  0 10]]\n",
      "üîç Lasso (Embedded)...\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "\n",
      "‚úÖ Accuracy: 0.7586\n",
      "üìä Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        16\n",
      "           1       0.80      0.94      0.87        69\n",
      "           2       0.96      0.74      0.83       137\n",
      "           3       0.22      1.00      0.36        10\n",
      "\n",
      "    accuracy                           0.76       232\n",
      "   macro avg       0.50      0.67      0.51       232\n",
      "weighted avg       0.82      0.76      0.77       232\n",
      "\n",
      "üßæ Confusion Matrix:\n",
      " [[  0  16   0   0]\n",
      " [  0  65   4   0]\n",
      " [  0   0 101  36]\n",
      " [  0   0   0  10]]\n",
      "üîç Tree-Based (Embedded)...\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\n",
      "‚úÖ Accuracy: 0.7543\n",
      "üìä Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        16\n",
      "           1       0.77      0.94      0.85        69\n",
      "           2       0.96      0.73      0.83       137\n",
      "           3       0.23      1.00      0.37        10\n",
      "\n",
      "    accuracy                           0.75       232\n",
      "   macro avg       0.49      0.67      0.51       232\n",
      "weighted avg       0.81      0.75      0.76       232\n",
      "\n",
      "üßæ Confusion Matrix:\n",
      " [[  0  16   0   0]\n",
      " [  0  65   4   0]\n",
      " [  0   3 100  34]\n",
      " [  0   0   0  10]]\n",
      "üîç Linear SVC L1 (Embedded)...\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\n",
      "‚úÖ Accuracy: 0.7888\n",
      "üìä Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.50      0.67        16\n",
      "           1       0.86      0.94      0.90        69\n",
      "           2       0.96      0.73      0.83       137\n",
      "           3       0.23      1.00      0.37        10\n",
      "\n",
      "    accuracy                           0.79       232\n",
      "   macro avg       0.76      0.79      0.69       232\n",
      "weighted avg       0.90      0.79      0.82       232\n",
      "\n",
      "üßæ Confusion Matrix:\n",
      " [[  8   8   0   0]\n",
      " [  0  65   4   0]\n",
      " [  0   3 100  34]\n",
      " [  0   0   0  10]]\n",
      "\n",
      "‚úÖ FINAL COMPARISON:\n",
      "MI             : 0.8793\n",
      "F-Classif      : 0.7198\n",
      "Variance       : 0.8836\n",
      "RFE            : 0.8276\n",
      "LogRegWrapper  : 0.7974\n",
      "ForwardSel     : 0.6940\n",
      "Lasso          : 0.7586\n",
      "Tree           : 0.7543\n",
      "LinearSVC-L1   : 0.7888\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "#  IMPORTS\n",
    "# ===========================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_selection import mutual_info_classif, RFE, VarianceThreshold, SelectKBest, f_classif\n",
    "from sklearn.linear_model import Lasso, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_selection import (mutual_info_classif, RFE, VarianceThreshold,\n",
    "                                       SelectFromModel, f_classif)\n",
    "from sklearn.base import clone\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# ===========================================\n",
    "#  DATA LOADING + PREPROCESSING\n",
    "# ===========================================\n",
    "def load_and_preprocess_data(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df.columns = df.columns.str.replace('\"', '').str.strip()\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'], errors='coerce')\n",
    "    df.dropna(subset=['TIME'], inplace=True)\n",
    "    df.set_index('TIME', inplace=True)\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "    target_col = 'fenologia_h1'\n",
    "    df[target_col].interpolate(method='linear', inplace=True)\n",
    "    df[target_col] = df[target_col] - 1\n",
    "\n",
    "    for i in range(1, 4):\n",
    "        df[f'{target_col}_lag{i}'] = df[target_col].shift(i)\n",
    "\n",
    "    for window in [3, 6]:\n",
    "        df[f'{target_col}_roll_mean_{window}'] = df[target_col].rolling(window).mean()\n",
    "        df[f'{target_col}_roll_std_{window}'] = df[target_col].rolling(window).std()\n",
    "\n",
    "    df['month'] = df.index.month\n",
    "    df['weekofyear'] = df.index.isocalendar().week\n",
    "    df['year'] = df.index.isocalendar().year\n",
    "    df['sin_week'] = np.sin(2 * np.pi * df['weekofyear'] / 52)\n",
    "    df['cos_week'] = np.cos(2 * np.pi * df['weekofyear'] / 52)\n",
    "\n",
    "    df['EMA_3'] = df[target_col].ewm(span=3, adjust=False).mean()\n",
    "    df['EMA_6'] = df[target_col].ewm(span=6, adjust=False).mean()\n",
    "\n",
    "    df['correlation_target_month'] = df[target_col].rolling(window=6).corr(df['month'])\n",
    "    df['correlation_target_week'] = df[target_col].rolling(window=6).corr(df['weekofyear'])\n",
    "\n",
    "    fft_values = np.fft.fft(df[target_col].dropna().values)\n",
    "    fft_real = np.real(fft_values)[:len(df[target_col])]\n",
    "    fft_imag = np.imag(fft_values)[:len(df[target_col])]\n",
    "    df['fft_real'] = np.concatenate([fft_real, np.nan * np.ones(len(df) - len(fft_real))])\n",
    "    df['fft_imag'] = np.concatenate([fft_imag, np.nan * np.ones(len(df) - len(fft_imag))])\n",
    "\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "    return df, target_col\n",
    "\n",
    "# ===========================================\n",
    "#  SCALING\n",
    "# ===========================================\n",
    "def feature_scaling(df, target_col):\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    return pd.DataFrame(X_scaled, columns=X.columns), y, scaler\n",
    "\n",
    "# ===========================================\n",
    "#  MODEL\n",
    "# ===========================================\n",
    "def build_mlp_model(input_dim, num_classes=4, num_units=64, dropout_rate=0.2, activation='relu'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_units, activation=activation, input_dim=input_dim))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(num_units // 2, activation=activation))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# ===========================================\n",
    "#  FEATURE SELECTION\n",
    "# ===========================================\n",
    "def mutual_information_feature_selection(X, y):\n",
    "    scores = mutual_info_classif(X, y)\n",
    "    return scores > 0.01\n",
    "\n",
    "def pearson_correlation_feature_selection(X, y):\n",
    "    scores = np.array([abs(pearsonr(X[col], y)[0]) for col in X.columns])\n",
    "    return scores > 0.1\n",
    "\n",
    "def variance_threshold_feature_selection(X, threshold=0.01):\n",
    "    selector = VarianceThreshold(threshold)\n",
    "    selector.fit(X)\n",
    "    return selector.get_support()\n",
    "\n",
    "def rfe_feature_selection(X, y, n_features_to_select=10):\n",
    "    rfe = RFE(SVC(kernel=\"linear\"), n_features_to_select=n_features_to_select)\n",
    "    rfe.fit(X, y)\n",
    "    return rfe.support_\n",
    "\n",
    "def logistic_regression_wrapper_selection(X, y):\n",
    "    model = LogisticRegression(solver='liblinear')\n",
    "    rfe = RFE(model, n_features_to_select=10)\n",
    "    rfe.fit(X, y)\n",
    "    return rfe.support_\n",
    "\n",
    "def f_classif_filter_selection(X, y):\n",
    "    selector = SelectKBest(score_func=f_classif, k=10)\n",
    "    selector.fit(X, y)\n",
    "    return selector.get_support()\n",
    "\n",
    "def lasso_feature_selection(X, y):\n",
    "    model = Lasso(alpha=0.01)\n",
    "    model.fit(X, y)\n",
    "    return np.abs(model.coef_) > 0.001\n",
    "\n",
    "def tree_based_feature_selection(X, y):\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(X, y)\n",
    "    return model.feature_importances_ > np.mean(model.feature_importances_)\n",
    "\n",
    "def variance_threshold_feature_selection(X, threshold=0.01):\n",
    "    selector = VarianceThreshold(threshold)\n",
    "    selector.fit(X)\n",
    "    return selector.get_support()\n",
    "\n",
    "#  New Filter: Variance Threshold\n",
    "\n",
    "#  New Wrapper: Forward Selection (simple custom greedy implementation)\n",
    "def forward_selection(X, y, n_features=10):\n",
    "    from sklearn.base import clone\n",
    "    model = RandomForestClassifier()\n",
    "    selected = []\n",
    "    remaining = list(X.columns)\n",
    "    while len(selected) < n_features:\n",
    "        scores = []\n",
    "        for feature in remaining:\n",
    "            temp_features = selected + [feature]\n",
    "            model_clone = clone(model)\n",
    "            model_clone.fit(X[temp_features], y)\n",
    "            acc = model_clone.score(X[temp_features], y)\n",
    "            scores.append((acc, feature))\n",
    "        scores.sort(reverse=True)\n",
    "        best_feature = scores[0][1]\n",
    "        selected.append(best_feature)\n",
    "        remaining.remove(best_feature)\n",
    "    return X.columns.isin(selected)\n",
    "\n",
    "#  New Embedded: Linear SVM with L1 penalty\n",
    "\n",
    "def linear_svc_l1_feature_selection(X, y):\n",
    "    svc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(X, y)\n",
    "    selector = SelectFromModel(svc, prefit=True)\n",
    "    return selector.get_support()\n",
    "\n",
    "\n",
    "# ===========================================\n",
    "#  TRAINING AND EVALUATION\n",
    "# ===========================================\n",
    "def train_and_evaluate_model(X_train, X_test, y_train, y_test, selected_features):\n",
    "    selected_columns = X_train.columns[selected_features]\n",
    "    X_train_sel = X_train[selected_columns]\n",
    "    X_test_sel = X_test[selected_columns]\n",
    "\n",
    "    model = build_mlp_model(input_dim=X_train_sel.shape[1])\n",
    "    model.fit(X_train_sel, y_train, epochs=30, batch_size=32, validation_split=0.2, verbose=0)\n",
    "    y_pred = model.predict(X_test_sel).argmax(axis=1)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"\\n Accuracy: {acc:.4f}\")\n",
    "    print(\" Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    print(\" Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    return acc, model\n",
    "\n",
    "# ===========================================\n",
    "#  MAIN\n",
    "# ===========================================\n",
    "def main():\n",
    "    train_fp = r\"E:\\\\Abroad period research\\\\Phenology datasets\\\\PHENOLOGY_H1\\\\Cadiz_train.csv\"\n",
    "    test_fp  = r\"E:\\\\Abroad period research\\\\Phenology datasets\\\\PHENOLOGY_H1\\\\Cadiz_test.csv\"\n",
    "\n",
    "    df_train, target_col = load_and_preprocess_data(train_fp)\n",
    "    X_train, y_train, scaler = feature_scaling(df_train, target_col)\n",
    "\n",
    "    df_test, _ = load_and_preprocess_data(test_fp)\n",
    "    X_test, y_test, _ = feature_scaling(df_test, target_col)\n",
    "\n",
    "    print(\" Mutual Info (Filter)...\")\n",
    "    acc1, _ = train_and_evaluate_model(X_train, X_test, y_train, y_test, mutual_information_feature_selection(X_train, y_train))\n",
    "\n",
    "    print(\" F-ANOVA (Filter)...\")\n",
    "    acc2, _ = train_and_evaluate_model(X_train, X_test, y_train, y_test, f_classif_filter_selection(X_train, y_train))\n",
    "\n",
    "    print(\" Variance Threshold (Filter)...\")\n",
    "    acc3, _ = train_and_evaluate_model(X_train, X_test, y_train, y_test, variance_threshold_feature_selection(X_train))\n",
    "\n",
    "    print(\" RFE SVC (Wrapper)...\")\n",
    "    acc4, _ = train_and_evaluate_model(X_train, X_test, y_train, y_test, rfe_feature_selection(X_train, y_train))\n",
    "\n",
    "    print(\" Logistic Regression Wrapper...\")\n",
    "    acc5, _ = train_and_evaluate_model(X_train, X_test, y_train, y_test, logistic_regression_wrapper_selection(X_train, y_train))\n",
    "\n",
    "    print(\" Forward Selection (Wrapper)...\")\n",
    "    acc6, _ = train_and_evaluate_model(X_train, X_test, y_train, y_test, forward_selection(X_train, y_train))\n",
    "\n",
    "    print(\" Lasso (Embedded)...\")\n",
    "    acc7, _ = train_and_evaluate_model(X_train, X_test, y_train, y_test, lasso_feature_selection(X_train, y_train))\n",
    "\n",
    "    print(\" Tree-Based (Embedded)...\")\n",
    "    acc8, _ = train_and_evaluate_model(X_train, X_test, y_train, y_test, tree_based_feature_selection(X_train, y_train))\n",
    "\n",
    "    print(\" Linear SVC L1 (Embedded)...\")\n",
    "    acc9, _ = train_and_evaluate_model(X_train, X_test, y_train, y_test, linear_svc_l1_feature_selection(X_train, y_train))\n",
    "\n",
    "    print(\"\\n FINAL COMPARISON:\")\n",
    "    methods = [\"MI\", \"F-Classif\", \"Variance\", \"RFE\", \"LogRegWrapper\", \"ForwardSel\", \"Lasso\", \"Tree\", \"LinearSVC-L1\"]\n",
    "    accuracies = [acc1, acc2, acc3, acc4, acc5, acc6, acc7, acc8, acc9]\n",
    "    for name, acc in zip(methods, accuracies):\n",
    "        print(f\"{name:15s}: {acc:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cef1db",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9acc9c2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13510ae4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3887f0f",
   "metadata": {},
   "source": [
    "Feature selection algorithms with proper results for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4aa087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Mutual Info...\n",
      "WARNING:tensorflow:5 out of the last 17 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000280FD89E2A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "üîç Pearson...\n",
      "WARNING:tensorflow:5 out of the last 17 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000280824660C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "üîç Variance...\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "üîç RFE...\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "üîç PSO...\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "üîç Lasso...\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "üîç Tree-Based...\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\n",
      "üìä FINAL COMPARISON TABLE:\n",
      "     Method  Accuracy  Precision  Recall  F1 Score  #Features  Feature Reduction (%)  FS Time (s)  Train Time (s)  Test Time (s)\n",
      "Mutual Info    0.8793     0.9280  0.8793    0.8873         27                   3.57         0.13            3.91           0.17\n",
      "    Pearson    0.8707     0.9395  0.8707    0.8866         24                  14.29         0.05            3.49           0.16\n",
      "   Variance    0.8534     0.9179  0.8534    0.8693         24                  14.29         0.00            3.42           0.15\n",
      "        RFE    0.8276     0.8978  0.8276    0.8276         10                  64.29         0.05            3.49           0.15\n",
      "        PSO    0.8362     0.9191  0.8362    0.8587         14                  50.00         0.00            3.43           0.15\n",
      "      Lasso    0.7069     0.7989  0.7069    0.7256          7                  75.00         0.00            3.39           0.16\n",
      " Tree-Based    0.7457     0.8856  0.7457    0.7739         10                  64.29         0.18            4.68           0.22\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_selection import mutual_info_classif, RFE, VarianceThreshold\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import pearsonr\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Dummy PSO (replace with your real PSO function)\n",
    "def pso(fitness_func, lb, ub, swarmsize=10, maxiter=3):\n",
    "    dim = len(lb)\n",
    "    best = np.random.rand(dim)\n",
    "    return best, 0.5\n",
    "\n",
    "# ======================\n",
    "# Load & Preprocess\n",
    "# ======================\n",
    "def load_and_preprocess_data(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df.columns = df.columns.str.replace('\"', '').str.strip()\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'], errors='coerce')\n",
    "    df.dropna(subset=['TIME'], inplace=True)\n",
    "    df.set_index('TIME', inplace=True)\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "    target_col = 'fenologia_h1'\n",
    "    df[target_col].interpolate(method='linear', inplace=True)\n",
    "    df[target_col] = df[target_col] - 1\n",
    "\n",
    "    for i in range(1, 4):\n",
    "        df[f'{target_col}_lag{i}'] = df[target_col].shift(i)\n",
    "\n",
    "    for window in [3, 6]:\n",
    "        df[f'{target_col}_roll_mean_{window}'] = df[target_col].rolling(window).mean()\n",
    "        df[f'{target_col}_roll_std_{window}'] = df[target_col].rolling(window).std()\n",
    "\n",
    "    df['month'] = df.index.month\n",
    "    df['weekofyear'] = df.index.isocalendar().week\n",
    "    df['year'] = df.index.isocalendar().year\n",
    "    df['sin_week'] = np.sin(2 * np.pi * df['weekofyear'] / 52)\n",
    "    df['cos_week'] = np.cos(2 * np.pi * df['weekofyear'] / 52)\n",
    "\n",
    "    df['EMA_3'] = df[target_col].ewm(span=3, adjust=False).mean()\n",
    "    df['EMA_6'] = df[target_col].ewm(span=6, adjust=False).mean()\n",
    "    df['correlation_target_month'] = df[target_col].rolling(window=6).corr(df['month'])\n",
    "    df['correlation_target_week'] = df[target_col].rolling(window=6).corr(df['weekofyear'])\n",
    "\n",
    "    fft_values = np.fft.fft(df[target_col].dropna().values)\n",
    "    fft_real = np.real(fft_values)[:len(df[target_col])]\n",
    "    fft_imag = np.imag(fft_values)[:len(df[target_col])]\n",
    "    df['fft_real'] = np.concatenate([fft_real, np.nan * np.ones(len(df) - len(fft_real))])\n",
    "    df['fft_imag'] = np.concatenate([fft_imag, np.nan * np.ones(len(df) - len(fft_imag))])\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "    return df, target_col\n",
    "\n",
    "# ======================\n",
    "# Scaling\n",
    "# ======================\n",
    "def feature_scaling(df, target_col):\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    return pd.DataFrame(X_scaled, columns=X.columns), y\n",
    "\n",
    "# ======================\n",
    "# MLP Model\n",
    "# ======================\n",
    "def build_mlp_model(input_dim, num_classes=4, num_units=64, dropout_rate=0.2, activation='relu'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_units, activation=activation, input_dim=input_dim))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(num_units // 2, activation=activation))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# ======================\n",
    "# Feature Selection Methods\n",
    "# ======================\n",
    "def mutual_information_feature_selection(X, y):\n",
    "    scores = mutual_info_classif(X, y)\n",
    "    return scores > 0.01\n",
    "\n",
    "def pearson_correlation_feature_selection(X, y):\n",
    "    scores = np.array([abs(pearsonr(X[col], y)[0]) for col in X.columns])\n",
    "    return scores > 0.1\n",
    "\n",
    "def variance_threshold_feature_selection(X):\n",
    "    selector = VarianceThreshold(threshold=0.01)\n",
    "    selector.fit(X)\n",
    "    return selector.get_support()\n",
    "\n",
    "def rfe_feature_selection(X, y, n_features_to_select=10):\n",
    "    rfe = RFE(SVC(kernel=\"linear\"), n_features_to_select=n_features_to_select)\n",
    "    rfe.fit(X, y)\n",
    "    return rfe.support_\n",
    "\n",
    "def pso_feature_selection(X, y):\n",
    "    def fitness_function(features):\n",
    "        mask = features > 0.5\n",
    "        if mask.sum() == 0:\n",
    "            return 1.0\n",
    "        clf = RandomForestClassifier()\n",
    "        clf.fit(X.iloc[:, mask], y)\n",
    "        acc = clf.score(X.iloc[:, mask], y)\n",
    "        return 1 - acc\n",
    "\n",
    "    lb = np.zeros(X.shape[1])\n",
    "    ub = np.ones(X.shape[1])\n",
    "    best_pos, _ = pso(fitness_function, lb, ub)\n",
    "    return best_pos > 0.5\n",
    "\n",
    "def lasso_feature_selection(X, y):\n",
    "    model = Lasso(alpha=0.01)\n",
    "    model.fit(X, y)\n",
    "    return np.abs(model.coef_) > 0.001\n",
    "\n",
    "def tree_based_feature_selection(X, y):\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(X, y)\n",
    "    return model.feature_importances_ > np.mean(model.feature_importances_)\n",
    "\n",
    "# ======================\n",
    "# Train & Evaluate\n",
    "# ======================\n",
    "def train_and_evaluate(X_train, X_test, y_train, y_test, selector_func, method_name):\n",
    "    start_fs = time.time()\n",
    "    if method_name == \"Variance\":\n",
    "        mask = selector_func(X_train)\n",
    "    else:\n",
    "        mask = selector_func(X_train, y_train)\n",
    "    fs_time = time.time() - start_fs\n",
    "\n",
    "    selected_cols = X_train.columns[mask]\n",
    "    X_train_sel = X_train[selected_cols]\n",
    "    X_test_sel = X_test[selected_cols]\n",
    "\n",
    "    start_train = time.time()\n",
    "    model = build_mlp_model(input_dim=X_train_sel.shape[1])\n",
    "    model.fit(X_train_sel, y_train, epochs=30, batch_size=32, validation_split=0.2, verbose=0)\n",
    "    train_time = time.time() - start_train\n",
    "\n",
    "    start_test = time.time()\n",
    "    y_pred = model.predict(X_test_sel).argmax(axis=1)\n",
    "    test_time = time.time() - start_test\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    n_features = sum(mask)\n",
    "    reduction_pct = 100 * (1 - n_features / X_train.shape[1])\n",
    "\n",
    "    return {\n",
    "        \"Method\": method_name,\n",
    "        \"Accuracy\": round(acc, 4),\n",
    "        \"Precision\": round(prec, 4),\n",
    "        \"Recall\": round(rec, 4),\n",
    "        \"F1 Score\": round(f1, 4),\n",
    "        \"#Features\": n_features,\n",
    "        \"Feature Reduction (%)\": round(reduction_pct, 2),\n",
    "        \"FS Time (s)\": round(fs_time, 2),\n",
    "        \"Train Time (s)\": round(train_time, 2),\n",
    "        \"Test Time (s)\": round(test_time, 2)\n",
    "    }\n",
    "\n",
    "# ======================\n",
    "# Main\n",
    "# ======================\n",
    "def main():\n",
    "    train_fp = r\"E:\\\\Abroad period research\\\\Phenology datasets\\\\PHENOLOGY_H1\\\\Cadiz_train.csv\"\n",
    "    test_fp = r\"E:\\\\Abroad period research\\\\Phenology datasets\\\\PHENOLOGY_H1\\\\Cadiz_test.csv\"\n",
    "\n",
    "    df_train, target_col = load_and_preprocess_data(train_fp)\n",
    "    X_train, y_train = feature_scaling(df_train, target_col)\n",
    "\n",
    "    df_test, _ = load_and_preprocess_data(test_fp)\n",
    "    X_test, y_test = feature_scaling(df_test, target_col)\n",
    "\n",
    "    methods = [\n",
    "        (\"Mutual Info\", mutual_information_feature_selection),\n",
    "        (\"Pearson\", pearson_correlation_feature_selection),\n",
    "        (\"Variance\", variance_threshold_feature_selection),\n",
    "        (\"RFE\", rfe_feature_selection),\n",
    "        (\"PSO\", pso_feature_selection),\n",
    "        (\"Lasso\", lasso_feature_selection),\n",
    "        (\"Tree-Based\", tree_based_feature_selection),\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "    for name, func in methods:\n",
    "        print(f\" {name}...\")\n",
    "        result = train_and_evaluate(X_train, X_test, y_train, y_test, func, name)\n",
    "        results.append(result)\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "    print(\"\\n FINAL COMPARISON TABLE:\")\n",
    "    print(df_results.to_string(index=False))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219cc1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Running Feature Selection: Mutual Info\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "\n",
      "üîç Running Feature Selection: F-Classif\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\n",
      "üîç Running Feature Selection: RFE\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\n",
      "üîç Running Feature Selection: LogRegWrapper\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "\n",
      "üîç Running Feature Selection: Lasso\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\n",
      "üîç Running Feature Selection: Tree-Based\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\n",
      "üìä Final Comparison Table:\n",
      "       Method  Accuracy  Precision  Recall  F1 Score  #Features  Feature Reduction (%)  FS Time (s)  Train Time (s)  Test Time (s)\n",
      "  Mutual Info    0.8405     0.9163  0.8405    0.8602         27                   3.57         0.16            3.61           0.17\n",
      "    F-Classif    0.7672     0.8116  0.7672    0.7695         10                  64.29         0.00            3.70           0.16\n",
      "          RFE    0.8491     0.9087  0.8491    0.8566         10                  64.29         0.04            3.27           0.14\n",
      "LogRegWrapper    0.8103     0.8889  0.8103    0.8334         10                  64.29         0.07            3.24           0.15\n",
      "        Lasso    0.7586     0.8279  0.7586    0.7683          7                  75.00         0.00            3.28           0.15\n",
      "   Tree-Based    0.5172     0.7768  0.5172    0.5463          8                  71.43         0.26            5.14           0.18\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# üì¶ IMPORTS\n",
    "# ===========================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_selection import mutual_info_classif, RFE, VarianceThreshold, SelectKBest, f_classif\n",
    "from sklearn.linear_model import Lasso, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# ===========================================\n",
    "#  DATA LOADING + PREPROCESSING\n",
    "# ===========================================\n",
    "def load_and_preprocess_data(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df.columns = df.columns.str.replace('\"', '').str.strip()\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'], errors='coerce')\n",
    "    df.dropna(subset=['TIME'], inplace=True)\n",
    "    df.set_index('TIME', inplace=True)\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "    target_col = 'fenologia_h1'\n",
    "    df[target_col].interpolate(method='linear', inplace=True)\n",
    "    df[target_col] = df[target_col] - 1\n",
    "\n",
    "    for i in range(1, 4):\n",
    "        df[f'{target_col}_lag{i}'] = df[target_col].shift(i)\n",
    "\n",
    "    for window in [3, 6]:\n",
    "        df[f'{target_col}_roll_mean_{window}'] = df[target_col].rolling(window).mean()\n",
    "        df[f'{target_col}_roll_std_{window}'] = df[target_col].rolling(window).std()\n",
    "\n",
    "    df['month'] = df.index.month\n",
    "    df['weekofyear'] = df.index.isocalendar().week\n",
    "    df['year'] = df.index.isocalendar().year\n",
    "    df['sin_week'] = np.sin(2 * np.pi * df['weekofyear'] / 52)\n",
    "    df['cos_week'] = np.cos(2 * np.pi * df['weekofyear'] / 52)\n",
    "\n",
    "    df['EMA_3'] = df[target_col].ewm(span=3, adjust=False).mean()\n",
    "    df['EMA_6'] = df[target_col].ewm(span=6, adjust=False).mean()\n",
    "\n",
    "    df['correlation_target_month'] = df[target_col].rolling(window=6).corr(df['month'])\n",
    "    df['correlation_target_week'] = df[target_col].rolling(window=6).corr(df['weekofyear'])\n",
    "\n",
    "    fft_values = np.fft.fft(df[target_col].dropna().values)\n",
    "    fft_real = np.real(fft_values)[:len(df[target_col])]\n",
    "    fft_imag = np.imag(fft_values)[:len(df[target_col])]\n",
    "    df['fft_real'] = np.concatenate([fft_real, np.nan * np.ones(len(df) - len(fft_real))])\n",
    "    df['fft_imag'] = np.concatenate([fft_imag, np.nan * np.ones(len(df) - len(fft_imag))])\n",
    "\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "    return df, target_col\n",
    "\n",
    "# ===========================================\n",
    "#  SCALING\n",
    "# ===========================================\n",
    "def feature_scaling(df, target_col):\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    return pd.DataFrame(X_scaled, columns=X.columns), y, scaler\n",
    "\n",
    "# ===========================================\n",
    "#  MODEL\n",
    "# ===========================================\n",
    "def build_mlp_model(input_dim, num_classes=4, num_units=64, dropout_rate=0.2, activation='relu'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_units, activation=activation, input_dim=input_dim))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(num_units // 2, activation=activation))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# ===========================================\n",
    "#  FEATURE SELECTION FUNCTIONS\n",
    "# ===========================================\n",
    "def timed_feature_selection(method, X, y):\n",
    "    start = time.time()\n",
    "    mask = method(X, y)\n",
    "    duration = time.time() - start\n",
    "    return mask, duration\n",
    "\n",
    "def mutual_information_feature_selection(X, y):\n",
    "    return mutual_info_classif(X, y) > 0.01\n",
    "\n",
    "def pearson_correlation_feature_selection(X, y):\n",
    "    scores = np.array([abs(pearsonr(X[col], y)[0]) for col in X.columns])\n",
    "    return scores > 0.1\n",
    "\n",
    "def variance_threshold_feature_selection(X, y, threshold=0.01):\n",
    "    selector = VarianceThreshold(threshold)\n",
    "    selector.fit(X)\n",
    "    return selector.get_support()\n",
    "\n",
    "def rfe_feature_selection(X, y, n_features_to_select=10):\n",
    "    rfe = RFE(SVC(kernel=\"linear\"), n_features_to_select=n_features_to_select)\n",
    "    rfe.fit(X, y)\n",
    "    return rfe.support_\n",
    "\n",
    "def logistic_regression_wrapper_selection(X, y):\n",
    "    model = LogisticRegression(solver='liblinear')\n",
    "    rfe = RFE(model, n_features_to_select=10)\n",
    "    rfe.fit(X, y)\n",
    "    return rfe.support_\n",
    "\n",
    "def f_classif_filter_selection(X, y):\n",
    "    selector = SelectKBest(score_func=f_classif, k=10)\n",
    "    selector.fit(X, y)\n",
    "    return selector.get_support()\n",
    "\n",
    "def lasso_feature_selection(X, y):\n",
    "    model = Lasso(alpha=0.01)\n",
    "    model.fit(X, y)\n",
    "    return np.abs(model.coef_) > 0.001\n",
    "\n",
    "def tree_based_feature_selection(X, y):\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(X, y)\n",
    "    return model.feature_importances_ > np.mean(model.feature_importances_)\n",
    "\n",
    "# ===========================================\n",
    "#  TRAINING AND EVALUATION\n",
    "# ===========================================\n",
    "def train_and_evaluate_model(X_train, X_test, y_train, y_test, selected_features):\n",
    "    X_train_sel = X_train.loc[:, selected_features]\n",
    "    X_test_sel = X_test.loc[:, selected_features]\n",
    "\n",
    "    model = build_mlp_model(input_dim=X_train_sel.shape[1])\n",
    "\n",
    "    start_train = time.time()\n",
    "    model.fit(X_train_sel, y_train, epochs=30, batch_size=32, validation_split=0.2, verbose=0)\n",
    "    train_time = time.time() - start_train\n",
    "\n",
    "    start_test = time.time()\n",
    "    y_pred = model.predict(X_test_sel).argmax(axis=1)\n",
    "    test_time = time.time() - start_test\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    return acc, prec, rec, f1, train_time, test_time\n",
    "\n",
    "# ===========================================\n",
    "#  MAIN\n",
    "# ===========================================\n",
    "def main():\n",
    "    train_fp = r\"E:\\\\Abroad period research\\\\Phenology datasets\\\\PHENOLOGY_H1\\\\Cadiz_train.csv\"\n",
    "    test_fp  = r\"E:\\\\Abroad period research\\\\Phenology datasets\\\\PHENOLOGY_H1\\\\Cadiz_test.csv\"\n",
    "\n",
    "    df_train, target_col = load_and_preprocess_data(train_fp)\n",
    "    X_train, y_train, scaler = feature_scaling(df_train, target_col)\n",
    "\n",
    "    df_test, _ = load_and_preprocess_data(test_fp)\n",
    "    X_test, y_test, _ = feature_scaling(df_test, target_col)\n",
    "\n",
    "    fs_methods = {\n",
    "        \"Mutual Info\": mutual_information_feature_selection,\n",
    "        \"F-Classif\": f_classif_filter_selection,\n",
    "        \"RFE\": rfe_feature_selection,\n",
    "        \"LogRegWrapper\": logistic_regression_wrapper_selection,\n",
    "        \"Lasso\": lasso_feature_selection,\n",
    "        \"Tree-Based\": tree_based_feature_selection\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for name, method in fs_methods.items():\n",
    "        print(f\"\\nüîç Running Feature Selection: {name}\")\n",
    "        mask, fs_time = timed_feature_selection(method, X_train, y_train)\n",
    "        selected_cols = X_train.columns[mask]\n",
    "        n_features = len(selected_cols)\n",
    "        reduction = 100 * (1 - n_features / X_train.shape[1])\n",
    "\n",
    "        acc, prec, rec, f1, train_time, test_time = train_and_evaluate_model(\n",
    "            X_train, X_test, y_train, y_test, selected_cols\n",
    "        )\n",
    "\n",
    "        results.append({\n",
    "            \"Method\": name,\n",
    "            \"Accuracy\": round(acc, 4),\n",
    "            \"Precision\": round(prec, 4),\n",
    "            \"Recall\": round(rec, 4),\n",
    "            \"F1 Score\": round(f1, 4),\n",
    "            \"#Features\": n_features,\n",
    "            \"Feature Reduction (%)\": round(reduction, 2),\n",
    "            \"FS Time (s)\": round(fs_time, 2),\n",
    "            \"Train Time (s)\": round(train_time, 2),\n",
    "            \"Test Time (s)\": round(test_time, 2)\n",
    "        })\n",
    "\n",
    "    print(\"\\n Final Comparison Table:\")\n",
    "    df_results = pd.DataFrame(results)\n",
    "    print(df_results.to_string(index=False))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "591fc148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Running: MI...\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\n",
      " Running: F-Classif...\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\n",
      " Running: Variance...\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\n",
      " Running: RFE...\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\n",
      " Running: LogRegWrapper...\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\n",
      " Running: ForwardSel...\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\n",
      " Running: Lasso...\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\n",
      " Running: Tree...\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\n",
      " Running: LinearSVC-L1...\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\n",
      " Final Comparison Table:\n",
      "       Method  FS Time (s)  #Features  Feature Reduction (%)  Train Time (s)  Test Time (s)  Accuracy  Precision  Recall  F1 Score\n",
      "           MI       0.2555         27                   3.57          5.4397         0.2004    0.9385     0.9473  0.9385    0.9255\n",
      "    F-Classif       0.0052         10                  64.29          4.6711         0.1729    0.9237     0.9323  0.9237    0.9242\n",
      "     Variance       0.0000         20                  28.57          4.5742         0.1742    0.9299     0.9257  0.9299    0.9213\n",
      "          RFE       0.1476         10                  64.29          4.5606         0.2312    0.9569     0.9608  0.9569    0.9528\n",
      "LogRegWrapper       0.2376         10                  64.29          4.3407         0.1588    0.9459     0.9480  0.9459    0.9422\n",
      "   ForwardSel      59.2357         10                  64.29          6.1525         0.1633    0.7651     0.8884  0.7651    0.8018\n",
      "        Lasso       0.0050          7                  75.00          4.3285         0.1519    0.9938     0.9938  0.9938    0.9938\n",
      "         Tree       0.3275          7                  75.00          4.4328         0.1503    0.9545     0.9556  0.9545    0.9543\n",
      " LinearSVC-L1       0.0202         10                  64.29          4.0284         0.1448    0.9225     0.9342  0.9225    0.9225\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "#  IMPORTS\n",
    "# ===========================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_selection import mutual_info_classif, RFE, VarianceThreshold, SelectKBest, f_classif\n",
    "from sklearn.linear_model import Lasso, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import time\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.base import clone\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# ===========================================\n",
    "#  DATA LOADING + PREPROCESSING\n",
    "# ===========================================\n",
    "def load_and_preprocess_data(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df.columns = df.columns.str.replace('\"', '').str.strip()\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'], errors='coerce')\n",
    "    df.dropna(subset=['TIME'], inplace=True)\n",
    "    df.set_index('TIME', inplace=True)\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "    target_col = 'fenologia_h2'\n",
    "    df[target_col].interpolate(method='linear', inplace=True)\n",
    "    df[target_col] = df[target_col] - 1\n",
    "\n",
    "    for i in range(1, 4):\n",
    "        df[f'{target_col}_lag{i}'] = df[target_col].shift(i)\n",
    "\n",
    "    for window in [3, 6]:\n",
    "        df[f'{target_col}_roll_mean_{window}'] = df[target_col].rolling(window).mean()\n",
    "        df[f'{target_col}_roll_std_{window}'] = df[target_col].rolling(window).std()\n",
    "\n",
    "    df['month'] = df.index.month\n",
    "    df['weekofyear'] = df.index.isocalendar().week\n",
    "    df['year'] = df.index.isocalendar().year\n",
    "    df['sin_week'] = np.sin(2 * np.pi * df['weekofyear'] / 52)\n",
    "    df['cos_week'] = np.cos(2 * np.pi * df['weekofyear'] / 52)\n",
    "\n",
    "    df['EMA_3'] = df[target_col].ewm(span=3, adjust=False).mean()\n",
    "    df['EMA_6'] = df[target_col].ewm(span=6, adjust=False).mean()\n",
    "\n",
    "    df['correlation_target_month'] = df[target_col].rolling(window=6).corr(df['month'])\n",
    "    df['correlation_target_week'] = df[target_col].rolling(window=6).corr(df['weekofyear'])\n",
    "\n",
    "    fft_values = np.fft.fft(df[target_col].dropna().values)\n",
    "    fft_real = np.real(fft_values)[:len(df[target_col])]\n",
    "    fft_imag = np.imag(fft_values)[:len(df[target_col])]\n",
    "    df['fft_real'] = np.concatenate([fft_real, np.nan * np.ones(len(df) - len(fft_real))])\n",
    "    df['fft_imag'] = np.concatenate([fft_imag, np.nan * np.ones(len(df) - len(fft_imag))])\n",
    "\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "    return df, target_col\n",
    "\n",
    "# ===========================================\n",
    "#  SCALING\n",
    "# ===========================================\n",
    "def feature_scaling(df, target_col):\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    return pd.DataFrame(X_scaled, columns=X.columns), y, scaler\n",
    "\n",
    "# ===========================================\n",
    "#  MODEL\n",
    "# ===========================================\n",
    "def build_mlp_model(input_dim, num_classes=4, num_units=64, dropout_rate=0.2, activation='relu'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_units, activation=activation, input_dim=input_dim))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(num_units // 2, activation=activation))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# ===========================================\n",
    "#  FEATURE SELECTION FUNCTIONS\n",
    "# ===========================================\n",
    "def mutual_information_feature_selection(X, y):\n",
    "    return mutual_info_classif(X, y) > 0.01\n",
    "\n",
    "def pearson_correlation_feature_selection(X, y):\n",
    "    scores = np.array([abs(pearsonr(X[col], y)[0]) for col in X.columns])\n",
    "    return scores > 0.1\n",
    "\n",
    "def variance_threshold_feature_selection(X, threshold=0.01):\n",
    "    return VarianceThreshold(threshold).fit(X).get_support()\n",
    "\n",
    "def rfe_feature_selection(X, y, n_features_to_select=10):\n",
    "    return RFE(SVC(kernel=\"linear\"), n_features_to_select=n_features_to_select).fit(X, y).support_\n",
    "\n",
    "def logistic_regression_wrapper_selection(X, y):\n",
    "    return RFE(LogisticRegression(solver='liblinear'), n_features_to_select=10).fit(X, y).support_\n",
    "\n",
    "def f_classif_filter_selection(X, y):\n",
    "    return SelectKBest(score_func=f_classif, k=10).fit(X, y).get_support()\n",
    "\n",
    "def lasso_feature_selection(X, y):\n",
    "    return np.abs(Lasso(alpha=0.01).fit(X, y).coef_) > 0.001\n",
    "\n",
    "def tree_based_feature_selection(X, y):\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(X, y)\n",
    "    return model.feature_importances_ > np.mean(model.feature_importances_)\n",
    "\n",
    "def forward_selection(X, y, n_features=10):\n",
    "    model = RandomForestClassifier()\n",
    "    selected, remaining = [], list(X.columns)\n",
    "    while len(selected) < n_features:\n",
    "        best_feature = max(\n",
    "            ((clone(model).fit(X[selected + [f]], y).score(X[selected + [f]], y), f) for f in remaining),\n",
    "            key=lambda x: x[0]\n",
    "        )[1]\n",
    "        selected.append(best_feature)\n",
    "        remaining.remove(best_feature)\n",
    "    return X.columns.isin(selected)\n",
    "\n",
    "def linear_svc_l1_feature_selection(X, y):\n",
    "    svc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(X, y)\n",
    "    return SelectFromModel(svc, prefit=True).get_support()\n",
    "\n",
    "# ===========================================\n",
    "#  TRAINING AND EVALUATION\n",
    "# ===========================================\n",
    "def train_evaluate_record(X_train, X_test, y_train, y_test, fs_func, method_name):\n",
    "    record = {'Method': method_name}\n",
    "    fs_start = time.time()\n",
    "    selected_features = fs_func(X_train, y_train) if 'y' in fs_func.__code__.co_varnames else fs_func(X_train)\n",
    "    fs_time = time.time() - fs_start\n",
    "    record['FS Time (s)'] = round(fs_time, 4)\n",
    "\n",
    "    X_train_sel = X_train.loc[:, selected_features]\n",
    "    X_test_sel = X_test.loc[:, selected_features]\n",
    "    record['#Features'] = X_train_sel.shape[1]\n",
    "    record['Feature Reduction (%)'] = round(100 * (1 - X_train_sel.shape[1] / X_train.shape[1]), 2)\n",
    "\n",
    "    train_start = time.time()\n",
    "    model = build_mlp_model(X_train_sel.shape[1])\n",
    "    model.fit(X_train_sel, y_train, epochs=30, batch_size=32, validation_split=0.2, verbose=0)\n",
    "    \n",
    "    train_time = time.time() - train_start\n",
    "    record['Train Time (s)'] = round(train_time, 4)\n",
    "\n",
    "    test_start = time.time()\n",
    "    y_pred = model.predict(X_test_sel).argmax(axis=1)\n",
    "    test_time = time.time() - test_start\n",
    "    record['Test Time (s)'] = round(test_time, 4)\n",
    "\n",
    "    record['Accuracy'] = round(accuracy_score(y_test, y_pred), 4)\n",
    "    record['Precision'] = round(precision_score(y_test, y_pred, average='weighted'), 4)\n",
    "    record['Recall'] = round(recall_score(y_test, y_pred, average='weighted'), 4)\n",
    "    record['F1 Score'] = round(f1_score(y_test, y_pred, average='weighted'), 4)\n",
    "\n",
    "    return record\n",
    "\n",
    "# ===========================================\n",
    "#  MAIN\n",
    "# ===========================================\n",
    "def main():\n",
    "    train_fp = r\"E:\\Abroad period research\\Phenology datasets\\PHENOLOGY_H2_train.csv\"\n",
    "    test_fp  = r\"E:\\Abroad period research\\Phenology datasets\\PHENOLOGY_H2_test.csv\"\n",
    "\n",
    "    df_train, target_col = load_and_preprocess_data(train_fp)\n",
    "    X_train, y_train, _ = feature_scaling(df_train, target_col)\n",
    "\n",
    "    df_test, _ = load_and_preprocess_data(test_fp)\n",
    "    X_test, y_test, _ = feature_scaling(df_test, target_col)\n",
    "\n",
    "    fs_methods = [\n",
    "        (\"MI\", mutual_information_feature_selection),\n",
    "        (\"F-Classif\", f_classif_filter_selection),\n",
    "        (\"Variance\", variance_threshold_feature_selection),\n",
    "        (\"RFE\", rfe_feature_selection),\n",
    "        (\"LogRegWrapper\", logistic_regression_wrapper_selection),\n",
    "        (\"ForwardSel\", forward_selection),\n",
    "        (\"Lasso\", lasso_feature_selection),\n",
    "        (\"Tree\", tree_based_feature_selection),\n",
    "        (\"LinearSVC-L1\", linear_svc_l1_feature_selection),\n",
    "    ]\n",
    "\n",
    "    all_results = []\n",
    "    for name, func in fs_methods:\n",
    "        print(f\"\\n Running: {name}...\")\n",
    "        result = train_evaluate_record(X_train, X_test, y_train, y_test, func, name)\n",
    "        all_results.append(result)\n",
    "\n",
    "    df_results = pd.DataFrame(all_results)\n",
    "    print(\"\\n Final Comparison Table:\")\n",
    "    print(df_results.to_string(index=False))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59bd53db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Running Feature Selection: PSO\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\n",
      "üîç Running Feature Selection: Pearson\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\n",
      " FINAL COMPARISON TABLE:\n",
      " Method  Accuracy  Precision  Recall  F1 Score  #Features  Feature Reduction (%)  FS Time (s)  Train Time (s)  Test Time (s)\n",
      "    PSO    0.8389     0.8633  0.8389    0.8402         12                  57.14       0.0006          4.0008         0.1591\n",
      "Pearson    0.9459     0.9457  0.9459    0.9360         23                  17.86       0.0364          3.9413         0.1408\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import pearsonr\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Dummy PSO (replace with your real PSO logic if needed)\n",
    "def pso(fitness_func, lb, ub, swarmsize=10, maxiter=3):\n",
    "    dim = len(lb)\n",
    "    best = np.random.rand(dim)\n",
    "    return best, 0.5\n",
    "\n",
    "# ======================\n",
    "# Load & Preprocess\n",
    "# ======================\n",
    "def load_and_preprocess_data(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df.columns = df.columns.str.replace('\"', '').str.strip()\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'], errors='coerce')\n",
    "    df.dropna(subset=['TIME'], inplace=True)\n",
    "    df.set_index('TIME', inplace=True)\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "    target_col = 'fenologia_h2'\n",
    "    df[target_col].interpolate(method='linear', inplace=True)\n",
    "    df[target_col] = df[target_col] - 1\n",
    "\n",
    "    for i in range(1, 4):\n",
    "        df[f'{target_col}_lag{i}'] = df[target_col].shift(i)\n",
    "\n",
    "    for window in [3, 6]:\n",
    "        df[f'{target_col}_roll_mean_{window}'] = df[target_col].rolling(window).mean()\n",
    "        df[f'{target_col}_roll_std_{window}'] = df[target_col].rolling(window).std()\n",
    "\n",
    "    df['month'] = df.index.month\n",
    "    df['weekofyear'] = df.index.isocalendar().week\n",
    "    df['year'] = df.index.isocalendar().year\n",
    "    df['sin_week'] = np.sin(2 * np.pi * df['weekofyear'] / 52)\n",
    "    df['cos_week'] = np.cos(2 * np.pi * df['weekofyear'] / 52)\n",
    "\n",
    "    df['EMA_3'] = df[target_col].ewm(span=3, adjust=False).mean()\n",
    "    df['EMA_6'] = df[target_col].ewm(span=6, adjust=False).mean()\n",
    "    df['correlation_target_month'] = df[target_col].rolling(window=6).corr(df['month'])\n",
    "    df['correlation_target_week'] = df[target_col].rolling(window=6).corr(df['weekofyear'])\n",
    "\n",
    "    fft_values = np.fft.fft(df[target_col].dropna().values)\n",
    "    fft_real = np.real(fft_values)[:len(df[target_col])]\n",
    "    fft_imag = np.imag(fft_values)[:len(df[target_col])]\n",
    "    df['fft_real'] = np.concatenate([fft_real, np.nan * np.ones(len(df) - len(fft_real))])\n",
    "    df['fft_imag'] = np.concatenate([fft_imag, np.nan * np.ones(len(df) - len(fft_imag))])\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "    return df, target_col\n",
    "\n",
    "# ======================\n",
    "# Scaling\n",
    "# ======================\n",
    "def feature_scaling(df, target_col):\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    return pd.DataFrame(X_scaled, columns=X.columns), y\n",
    "\n",
    "# ======================\n",
    "# MLP Model\n",
    "# ======================\n",
    "def build_mlp_model(input_dim, num_classes=4, num_units=64, dropout_rate=0.2, activation='relu'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_units, activation=activation, input_dim=input_dim))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(num_units // 2, activation=activation))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# ======================\n",
    "# Feature Selection Methods\n",
    "# ======================\n",
    "def pso_feature_selection(X, y):\n",
    "    def fitness_function(features):\n",
    "        mask = features > 0.5\n",
    "        if mask.sum() == 0:\n",
    "            return 1.0\n",
    "        clf = RandomForestClassifier()\n",
    "        clf.fit(X.iloc[:, mask], y)\n",
    "        acc = clf.score(X.iloc[:, mask], y)\n",
    "        return 1 - acc\n",
    "\n",
    "    lb = np.zeros(X.shape[1])\n",
    "    ub = np.ones(X.shape[1])\n",
    "    best_pos, _ = pso(fitness_function, lb, ub)\n",
    "    return best_pos > 0.5\n",
    "\n",
    "def pearson_feature_selection(X, y, threshold=0.1):\n",
    "    scores = np.array([abs(pearsonr(X[col], y)[0]) for col in X.columns])\n",
    "    return scores > threshold\n",
    "\n",
    "# ======================\n",
    "# Train & Evaluate\n",
    "# ======================\n",
    "def train_and_evaluate(X_train, X_test, y_train, y_test, selector_func, method_name):\n",
    "    start_fs = time.time()\n",
    "    mask = selector_func(X_train, y_train)\n",
    "    fs_time = time.time() - start_fs\n",
    "\n",
    "    selected_cols = X_train.columns[mask]\n",
    "    X_train_sel = X_train[selected_cols]\n",
    "    X_test_sel = X_test[selected_cols]\n",
    "\n",
    "    start_train = time.time()\n",
    "    model = build_mlp_model(input_dim=X_train_sel.shape[1])\n",
    "    model.fit(X_train_sel, y_train, epochs=30, batch_size=32, validation_split=0.2, verbose=0)\n",
    "    train_time = time.time() - start_train\n",
    "\n",
    "    start_test = time.time()\n",
    "    y_pred = model.predict(X_test_sel).argmax(axis=1)\n",
    "    test_time = time.time() - start_test\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    n_features = sum(mask)\n",
    "    reduction_pct = 100 * (1 - n_features / X_train.shape[1])\n",
    "\n",
    "    return {\n",
    "        \"Method\": method_name,\n",
    "        \"Accuracy\": round(acc, 4),\n",
    "        \"Precision\": round(prec, 4),\n",
    "        \"Recall\": round(rec, 4),\n",
    "        \"F1 Score\": round(f1, 4),\n",
    "        \"#Features\": n_features,\n",
    "        \"Feature Reduction (%)\": round(reduction_pct, 2),\n",
    "        \"FS Time (s)\": round(fs_time, 4),\n",
    "        \"Train Time (s)\": round(train_time, 4),\n",
    "        \"Test Time (s)\": round(test_time, 4)\n",
    "    }\n",
    "\n",
    "# ======================\n",
    "# Main\n",
    "# ======================\n",
    "def main():\n",
    "    train_fp = r\"E:\\Abroad period research\\Phenology datasets\\PHENOLOGY_H2_train.csv\"\n",
    "    test_fp  = r\"E:\\Abroad period research\\Phenology datasets\\PHENOLOGY_H2_test.csv\"\n",
    "\n",
    "    df_train, target_col = load_and_preprocess_data(train_fp)\n",
    "    X_train, y_train = feature_scaling(df_train, target_col)\n",
    "\n",
    "    df_test, _ = load_and_preprocess_data(test_fp)\n",
    "    X_test, y_test = feature_scaling(df_test, target_col)\n",
    "\n",
    "    methods = [\n",
    "        (\"PSO\", pso_feature_selection),\n",
    "        (\"Pearson\", pearson_feature_selection)\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "    for name, func in methods:\n",
    "        print(f\"\\nüîç Running Feature Selection: {name}\")\n",
    "        result = train_and_evaluate(X_train, X_test, y_train, y_test, func, name)\n",
    "        results.append(result)\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "    print(\"\\n FINAL COMPARISON TABLE:\")\n",
    "    print(df_results.to_string(index=False))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f5b801",
   "metadata": {},
   "source": [
    "Final feature selection code with all features selection algorithms for comparison"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
