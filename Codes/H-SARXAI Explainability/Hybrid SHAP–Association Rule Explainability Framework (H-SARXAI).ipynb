{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d2b57ea",
   "metadata": {},
   "source": [
    "Final Code for Hybrid SHAP–Association Rule Explainability Framework (H-SARXAI) with trust scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147eb9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import shap\n",
    "import warnings\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout\n",
    "from bayes_opt import BayesianOptimization\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(42)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# ---------------------------\n",
    "# SHAP-Compatible Wrapper\n",
    "# ---------------------------\n",
    "class GRUModelWrapper:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def __call__(self, data):\n",
    "        if len(data.shape) == 2:\n",
    "            data = data[:, np.newaxis, :]  # Add time step dimension\n",
    "        return self.model(data).numpy()\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Load and Preprocess Data\n",
    "# ---------------------------\n",
    "def load_and_preprocess_data(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df.columns = df.columns.str.strip().str.replace('\"', '')\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'], errors='coerce')\n",
    "    df.dropna(subset=['TIME'], inplace=True)\n",
    "    df.set_index('TIME', inplace=True)\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "    target_col = 'fenologia_h2'\n",
    "    df[target_col].interpolate(method='linear', inplace=True)\n",
    "    df[target_col] = df[target_col] - 1\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "    return df, target_col\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Feature Scaling\n",
    "# ---------------------------\n",
    "def feature_scaling(df, target_col):\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled_3d = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))\n",
    "    return X_scaled, X_scaled_3d, y, scaler\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Build GRU Model\n",
    "# ---------------------------\n",
    "def build_gru_model(input_shape, num_classes=4, num_units=64, dropout_rate=0.2):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(num_units, input_shape=input_shape))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(num_units // 2, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Bayesian Optimization\n",
    "# ---------------------------\n",
    "def objective_function(num_units, dropout_rate, epochs, batch_size, X_train, y_train):\n",
    "    model = build_gru_model((X_train.shape[1], X_train.shape[2]), 4, int(num_units), dropout_rate)\n",
    "    history = model.fit(X_train, y_train, epochs=int(epochs), batch_size=int(batch_size), validation_split=0.2, verbose=0)\n",
    "    return np.max(history.history['val_accuracy'])\n",
    "\n",
    "def optimize_hyperparameters(X_train, y_train):\n",
    "    pbounds = {\n",
    "        'num_units': (32, 128),\n",
    "        'dropout_rate': (0.1, 0.5),\n",
    "        'epochs': (10, 30),\n",
    "        'batch_size': (16, 64)\n",
    "    }\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=lambda num_units, dropout_rate, epochs, batch_size: objective_function(\n",
    "            num_units, dropout_rate, epochs, batch_size, X_train, y_train\n",
    "        ),\n",
    "        pbounds=pbounds,\n",
    "        random_state=42,\n",
    "        verbose=2\n",
    "    )\n",
    "    optimizer.maximize(init_points=2, n_iter=3)\n",
    "    return optimizer.max['params']\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Train & Evaluate Model\n",
    "# ---------------------------\n",
    "def train_and_evaluate_model(X_train, X_test, y_train, y_test, best_params):\n",
    "    model = build_gru_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        num_classes=4,\n",
    "        num_units=int(best_params['num_units']),\n",
    "        dropout_rate=float(best_params['dropout_rate'])\n",
    "    )\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=int(best_params['epochs']),\n",
    "        batch_size=int(best_params['batch_size']),\n",
    "        validation_split=0.2,\n",
    "        verbose=1\n",
    "    )\n",
    "    y_pred = model.predict(X_test).argmax(axis=1)\n",
    "    print(f\"\\nAccuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    return model\n",
    "\n",
    "# ---------------------------\n",
    "# 6. Trust Score Computation\n",
    "# ---------------------------\n",
    "def compute_trust_metrics(shap_feats, rule_feats, k=10):\n",
    "    shap_top_k = set(shap_feats[:k])\n",
    "    rule_top = set(rule_feats)\n",
    "    intersection = shap_top_k.intersection(rule_top)\n",
    "    \n",
    "    precision_at_k = len(intersection) / k\n",
    "    coverage = len(intersection) / len(rule_top) if rule_top else 0\n",
    "    jaccard = len(intersection) / len(shap_top_k.union(rule_top)) if shap_top_k.union(rule_top) else 0\n",
    "    \n",
    "    return precision_at_k, coverage, jaccard\n",
    "\n",
    "# ---------------------------\n",
    "# 7. Hybrid XAI with Trust Score\n",
    "# ---------------------------\n",
    "def hybrid_xai_analysis(model, X_2d, y, df, target_col):\n",
    "    print(\"\\n===== XAI Analysis (SHAP + Rules + Trust Score) =====\")\n",
    "\n",
    "    # --- SHAP ---\n",
    "    wrapped_model = GRUModelWrapper(model)\n",
    "    explainer = shap.Explainer(wrapped_model, X_2d)\n",
    "    shap_vals = explainer(X_2d)\n",
    "\n",
    "    # Ensure 1D indexing\n",
    "    mean_abs_shap = np.abs(shap_vals.values).mean(axis=0)\n",
    "    if mean_abs_shap.ndim > 1:\n",
    "        mean_abs_shap = mean_abs_shap.mean(axis=0)  # Flatten if needed\n",
    "\n",
    "    shap_top_idx = np.argsort(mean_abs_shap)[::-1]\n",
    "    shap_top_feats = df.drop(columns=[target_col]).columns.to_numpy()[shap_top_idx].tolist()\n",
    "    print(\"\\nTop SHAP Features:\", shap_top_feats[:10])\n",
    "\n",
    "    # --- Association Rules ---\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y_str = y.astype(str)\n",
    "    df_disc = X.apply(lambda col: pd.qcut(col, q=4, duplicates='drop').astype(str))\n",
    "    df_disc['target'] = y_str\n",
    "    transactions = [[f\"{col}={val}\" for col, val in row.items()] for _, row in df_disc.iterrows()]\n",
    "    te = TransactionEncoder()\n",
    "    df_encoded = pd.DataFrame(te.fit(transactions).transform(transactions), columns=te.columns_)\n",
    "    freq_items = apriori(df_encoded, min_support=0.1, use_colnames=True)\n",
    "    rules = association_rules(freq_items, metric=\"lift\", min_threshold=1.2)\n",
    "\n",
    "    rule_top_feats = {}\n",
    "    for cls in y.unique():\n",
    "        class_rules = rules[rules['consequents'].apply(lambda x: f'target={cls}' in x)]\n",
    "        feat_counts = {}\n",
    "        for _, row in class_rules.iterrows():\n",
    "            for antecedent in row['antecedents']:\n",
    "                if not antecedent.startswith('target='):\n",
    "                    feat = antecedent.split('=')[0]\n",
    "                    feat_counts[feat] = feat_counts.get(feat, 0) + 1\n",
    "        sorted_feats = sorted(feat_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "        rule_top_feats[cls] = sorted_feats\n",
    "        print(f\"\\nClass {cls} Rule Features: {[f for f, _ in sorted_feats]}\")\n",
    "\n",
    "    # --- Overlap & Trust Metrics ---\n",
    "    print(\"\\n--- Trust Metrics per Class ---\")\n",
    "    for cls, feats in rule_top_feats.items():\n",
    "        rule_feat_names = [f for f, _ in feats]\n",
    "        precision, coverage, jaccard = compute_trust_metrics(shap_top_feats, rule_feat_names)\n",
    "        print(f\"Class {cls} → Precision@10: {precision:.2f}, Coverage: {coverage:.2f}, Jaccard: {jaccard:.2f}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 8. Main\n",
    "# ---------------------------\n",
    "def main():\n",
    "    train_path = r\"E:\\Abroad period research\\Phenology datasets\\Lasso_Selected_Features_H2\\train_lasso_selected.csv\"\n",
    "    test_path = r\"E:\\Abroad period research\\Phenology datasets\\Lasso_Selected_Features_H2\\test_lasso_selected.csv\"\n",
    "\n",
    "    df_train, target_col = load_and_preprocess_data(train_path)\n",
    "    X_train_2d, X_train_3d, y_train, _ = feature_scaling(df_train, target_col)\n",
    "\n",
    "    df_test, _ = load_and_preprocess_data(test_path)\n",
    "    X_test_2d, X_test_3d, y_test, _ = feature_scaling(df_test, target_col)\n",
    "\n",
    "    print(\"\\nOptimizing Hyperparameters...\")\n",
    "    best_params = optimize_hyperparameters(X_train_3d, y_train)\n",
    "\n",
    "    print(\"\\nTraining Model...\")\n",
    "    model = train_and_evaluate_model(X_train_3d, X_test_3d, y_train, y_test, best_params)\n",
    "\n",
    "    print(\"\\nRunning Hybrid XAI...\")\n",
    "    hybrid_xai_analysis(model, X_train_2d, y_train, df_train, target_col)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e86b32",
   "metadata": {},
   "source": [
    "Printing number of samples per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034bd042",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "train_path = r\"E:\\Abroad period research\\Phenology datasets\\Lasso_Selected_Features_H2\\train_lasso_selected.csv\"\n",
    "test_path = r\"E:\\Abroad period research\\Phenology datasets\\Lasso_Selected_Features_H2\\test_lasso_selected.csv\"\n",
    "\n",
    "# Load the data\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "# Count instances per class in the target column 'fenologia_h2'\n",
    "train_class_counts = train_df['fenologia_h2'].value_counts().sort_index()\n",
    "test_class_counts = test_df['fenologia_h2'].value_counts().sort_index()\n",
    "\n",
    "# Print results\n",
    "print(\"Training Set Class Distribution:\")\n",
    "print(train_class_counts)\n",
    "print(\"\\nTesting Set Class Distribution:\")\n",
    "print(test_class_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dae0da",
   "metadata": {},
   "source": [
    "trust metrics for:\n",
    "\n",
    "SHAP top features\n",
    "\n",
    "Association rule features\n",
    "\n",
    "Overlapping features (intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f9c94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import shap\n",
    "import warnings\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout\n",
    "from bayes_opt import BayesianOptimization\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(42)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# ---------------------------\n",
    "# SHAP-Compatible Wrapper\n",
    "# ---------------------------\n",
    "class GRUModelWrapper:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def __call__(self, data):\n",
    "        if len(data.shape) == 2:\n",
    "            data = data[:, np.newaxis, :]  # Add time step dimension\n",
    "        return self.model(data).numpy()\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Load and Preprocess Data\n",
    "# ---------------------------\n",
    "def load_and_preprocess_data(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df.columns = df.columns.str.strip().str.replace('\"', '')\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'], errors='coerce')\n",
    "    df.dropna(subset=['TIME'], inplace=True)\n",
    "    df.set_index('TIME', inplace=True)\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "    target_col = 'fenologia_h2'\n",
    "    df[target_col].interpolate(method='linear', inplace=True)\n",
    "    df[target_col] = df[target_col] - 1\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "    return df, target_col\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Feature Scaling\n",
    "# ---------------------------\n",
    "def feature_scaling(df, target_col):\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled_3d = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))\n",
    "    return X_scaled, X_scaled_3d, y, scaler\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Build GRU Model\n",
    "# ---------------------------\n",
    "def build_gru_model(input_shape, num_classes=4, num_units=64, dropout_rate=0.2):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(num_units, input_shape=input_shape))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(num_units // 2, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Bayesian Optimization\n",
    "# ---------------------------\n",
    "def objective_function(num_units, dropout_rate, epochs, batch_size, X_train, y_train):\n",
    "    model = build_gru_model((X_train.shape[1], X_train.shape[2]), 4, int(num_units), dropout_rate)\n",
    "    history = model.fit(X_train, y_train, epochs=int(epochs), batch_size=int(batch_size), validation_split=0.2, verbose=0)\n",
    "    return np.max(history.history['val_accuracy'])\n",
    "\n",
    "def optimize_hyperparameters(X_train, y_train):\n",
    "    pbounds = {\n",
    "        'num_units': (32, 128),\n",
    "        'dropout_rate': (0.1, 0.5),\n",
    "        'epochs': (10, 30),\n",
    "        'batch_size': (16, 64)\n",
    "    }\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=lambda num_units, dropout_rate, epochs, batch_size: objective_function(\n",
    "            num_units, dropout_rate, epochs, batch_size, X_train, y_train\n",
    "        ),\n",
    "        pbounds=pbounds,\n",
    "        random_state=42,\n",
    "        verbose=2\n",
    "    )\n",
    "    optimizer.maximize(init_points=2, n_iter=3)\n",
    "    return optimizer.max['params']\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Train & Evaluate Model\n",
    "# ---------------------------\n",
    "def train_and_evaluate_model(X_train, X_test, y_train, y_test, best_params):\n",
    "    model = build_gru_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        num_classes=4,\n",
    "        num_units=int(best_params['num_units']),\n",
    "        dropout_rate=float(best_params['dropout_rate'])\n",
    "    )\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=int(best_params['epochs']),\n",
    "        batch_size=int(best_params['batch_size']),\n",
    "        validation_split=0.2,\n",
    "        verbose=1\n",
    "    )\n",
    "    y_pred = model.predict(X_test).argmax(axis=1)\n",
    "    print(f\"\\nAccuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    return model\n",
    "\n",
    "# ---------------------------\n",
    "# 6. Trust Score Computation\n",
    "# ---------------------------\n",
    "def compute_trust_metrics(shap_feats, rule_feats, k=10):\n",
    "    shap_top_k = set(shap_feats[:k])\n",
    "    rule_top = set(rule_feats)\n",
    "    intersection = shap_top_k.intersection(rule_top)\n",
    "    \n",
    "    precision_at_k = len(intersection) / k\n",
    "    coverage = len(intersection) / len(rule_top) if rule_top else 0\n",
    "    jaccard = len(intersection) / len(shap_top_k.union(rule_top)) if shap_top_k.union(rule_top) else 0\n",
    "    \n",
    "    return precision_at_k, coverage, jaccard\n",
    "\n",
    "# ---------------------------\n",
    "# 7. Hybrid XAI with Trust Score\n",
    "# ---------------------------\n",
    "# All previous imports remain the same...\n",
    "# Additions already included for SHAP, mlxtend, BayesianOptimization, etc.\n",
    "\n",
    "# Replace your hybrid_xai_analysis() function with this updated version:\n",
    "\n",
    "def hybrid_xai_analysis(model, X_2d, y, df, target_col):\n",
    "    print(\"\\n===== XAI Analysis (SHAP + Rules + Trust Score) =====\")\n",
    "\n",
    "    # --- SHAP ---\n",
    "    wrapped_model = GRUModelWrapper(model)\n",
    "    explainer = shap.Explainer(wrapped_model, X_2d)\n",
    "    shap_vals = explainer(X_2d)\n",
    "    mean_abs_shap = np.abs(shap_vals.values).mean(axis=0)\n",
    "    if mean_abs_shap.ndim > 1:\n",
    "        mean_abs_shap = mean_abs_shap.mean(axis=0)\n",
    "    shap_top_idx = np.argsort(mean_abs_shap)[::-1]\n",
    "    shap_top_feats = df.drop(columns=[target_col]).columns.to_numpy()[shap_top_idx].tolist()\n",
    "    print(\"\\nTop SHAP Features:\", shap_top_feats[:10])\n",
    "\n",
    "    # --- Association Rules ---\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y_str = y.astype(str)\n",
    "    df_disc = X.apply(lambda col: pd.qcut(col, q=4, duplicates='drop').astype(str))\n",
    "    df_disc['target'] = y_str\n",
    "    transactions = [[f\"{col}={val}\" for col, val in row.items()] for _, row in df_disc.iterrows()]\n",
    "    te = TransactionEncoder()\n",
    "    df_encoded = pd.DataFrame(te.fit(transactions).transform(transactions), columns=te.columns_)\n",
    "    freq_items = apriori(df_encoded, min_support=0.1, use_colnames=True)\n",
    "    rules = association_rules(freq_items, metric=\"lift\", min_threshold=1.2)\n",
    "\n",
    "    rule_top_feats = {}\n",
    "    all_rule_feats_set = set()\n",
    "\n",
    "    for cls in y.unique():\n",
    "        class_rules = rules[rules['consequents'].apply(lambda x: f'target={cls}' in x)]\n",
    "        feat_counts = {}\n",
    "        for _, row in class_rules.iterrows():\n",
    "            for antecedent in row['antecedents']:\n",
    "                if not antecedent.startswith('target='):\n",
    "                    feat = antecedent.split('=')[0]\n",
    "                    feat_counts[feat] = feat_counts.get(feat, 0) + 1\n",
    "                    all_rule_feats_set.add(feat)\n",
    "        sorted_feats = sorted(feat_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "        rule_top_feats[cls] = sorted_feats\n",
    "        print(f\"\\nClass {cls} Rule Features: {[f for f, _ in sorted_feats]}\")\n",
    "\n",
    "    # --- Overlap ---\n",
    "    overlapping_features = set(shap_top_feats[:10]).intersection(all_rule_feats_set)\n",
    "    print(\"\\nOverlapping Features:\", list(overlapping_features))\n",
    "\n",
    "    # --- Compute and Print Trust Metrics ---\n",
    "    def trust_report(name, shap_list, rule_list):\n",
    "        precision, coverage, jaccard = compute_trust_metrics(shap_list, rule_list)\n",
    "        print(f\"{name} → Precision@10: {precision:.2f}, Coverage: {coverage:.2f}, Jaccard: {jaccard:.2f}\")\n",
    "\n",
    "    print(\"\\n--- Trust Metrics Summary ---\")\n",
    "    trust_report(\"SHAP vs Rules\", shap_top_feats, list(all_rule_feats_set))\n",
    "    trust_report(\"Overlap vs SHAP\", list(overlapping_features), shap_top_feats)\n",
    "    trust_report(\"Overlap vs Rules\", list(overlapping_features), list(all_rule_feats_set))\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 8. Main\n",
    "# ---------------------------\n",
    "def main():\n",
    "    train_path = r\"E:\\Abroad period research\\Phenology datasets\\Lasso_Selected_Features_H2\\train_lasso_selected.csv\"\n",
    "    test_path = r\"E:\\Abroad period research\\Phenology datasets\\Lasso_Selected_Features_H2\\test_lasso_selected.csv\"\n",
    "\n",
    "    df_train, target_col = load_and_preprocess_data(train_path)\n",
    "    X_train_2d, X_train_3d, y_train, _ = feature_scaling(df_train, target_col)\n",
    "\n",
    "    df_test, _ = load_and_preprocess_data(test_path)\n",
    "    X_test_2d, X_test_3d, y_test, _ = feature_scaling(df_test, target_col)\n",
    "\n",
    "    print(\"\\nOptimizing Hyperparameters...\")\n",
    "    best_params = optimize_hyperparameters(X_train_3d, y_train)\n",
    "\n",
    "    print(\"\\nTraining Model...\")\n",
    "    model = train_and_evaluate_model(X_train_3d, X_test_3d, y_train, y_test, best_params)\n",
    "\n",
    "    print(\"\\nRunning Hybrid XAI...\")\n",
    "    hybrid_xai_analysis(model, X_train_2d, y_train, df_train, target_col)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dbe7ce",
   "metadata": {},
   "source": [
    "Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b342e4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import shap\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout\n",
    "from bayes_opt import BayesianOptimization\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(42)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# ---------------------------\n",
    "# SHAP-Compatible Wrapper\n",
    "# ---------------------------\n",
    "class GRUModelWrapper:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    def __call__(self, data):\n",
    "        if len(data.shape) == 2:\n",
    "            data = data[:, np.newaxis, :]\n",
    "        return self.model(data).numpy()\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Load and Preprocess Data\n",
    "# ---------------------------\n",
    "def load_and_preprocess_data(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df.columns = df.columns.str.strip().str.replace('\"', '')\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'], errors='coerce')\n",
    "    df.dropna(subset=['TIME'], inplace=True)\n",
    "    df.set_index('TIME', inplace=True)\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "    target_col = 'fenologia_h2'\n",
    "    df[target_col].interpolate(method='linear', inplace=True)\n",
    "    df[target_col] = df[target_col] - 1\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "    return df, target_col\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Feature Scaling\n",
    "# ---------------------------\n",
    "def feature_scaling(df, target_col):\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled_3d = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))\n",
    "    return X_scaled, X_scaled_3d, y, scaler\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Build GRU Model\n",
    "# ---------------------------\n",
    "def build_gru_model(input_shape, num_classes=4, num_units=64, dropout_rate=0.2):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(num_units, input_shape=input_shape))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(num_units // 2, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Bayesian Optimization\n",
    "# ---------------------------\n",
    "def objective_function(num_units, dropout_rate, epochs, batch_size, X_train, y_train):\n",
    "    model = build_gru_model((X_train.shape[1], X_train.shape[2]), 4, int(num_units), dropout_rate)\n",
    "    history = model.fit(X_train, y_train, epochs=int(epochs), batch_size=int(batch_size), validation_split=0.2, verbose=0)\n",
    "    return np.max(history.history['val_accuracy'])\n",
    "\n",
    "def optimize_hyperparameters(X_train, y_train):\n",
    "    pbounds = {\n",
    "        'num_units': (32, 128),\n",
    "        'dropout_rate': (0.1, 0.5),\n",
    "        'epochs': (10, 30),\n",
    "        'batch_size': (16, 64)\n",
    "    }\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=lambda num_units, dropout_rate, epochs, batch_size: objective_function(\n",
    "            num_units, dropout_rate, epochs, batch_size, X_train, y_train\n",
    "        ),\n",
    "        pbounds=pbounds,\n",
    "        random_state=42,\n",
    "        verbose=2\n",
    "    )\n",
    "    optimizer.maximize(init_points=2, n_iter=3)\n",
    "    return optimizer.max['params']\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Train & Evaluate Model\n",
    "# ---------------------------\n",
    "def train_and_evaluate_model(X_train, X_test, y_train, y_test, best_params):\n",
    "    model = build_gru_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        num_classes=4,\n",
    "        num_units=int(best_params['num_units']),\n",
    "        dropout_rate=float(best_params['dropout_rate'])\n",
    "    )\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=int(best_params['epochs']),\n",
    "        batch_size=int(best_params['batch_size']),\n",
    "        validation_split=0.2,\n",
    "        verbose=1\n",
    "    )\n",
    "    y_pred = model.predict(X_test).argmax(axis=1)\n",
    "    print(f\"\\nAccuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    return model\n",
    "\n",
    "# ---------------------------\n",
    "# 6. Trust Score Computation\n",
    "# ---------------------------\n",
    "def compute_trust_metrics(shap_feats, rule_feats, k=10):\n",
    "    shap_top_k = set(shap_feats[:k])\n",
    "    rule_top = set(rule_feats)\n",
    "    intersection = shap_top_k.intersection(rule_top)\n",
    "    precision_at_k = len(intersection) / k\n",
    "    coverage = len(intersection) / len(rule_top) if rule_top else 0\n",
    "    jaccard = len(intersection) / len(shap_top_k.union(rule_top)) if shap_top_k.union(rule_top) else 0\n",
    "    return precision_at_k, coverage, jaccard\n",
    "\n",
    "# ---------------------------\n",
    "# 7. Hybrid XAI with Trust Score + Heatmap\n",
    "# ---------------------------\n",
    "def hybrid_xai_analysis(model, X_2d, y, df, target_col):\n",
    "    print(\"\\n===== XAI Analysis (SHAP + Rules + Trust Score) =====\")\n",
    "\n",
    "    # --- SHAP ---\n",
    "    wrapped_model = GRUModelWrapper(model)\n",
    "    explainer = shap.Explainer(wrapped_model, X_2d)\n",
    "    shap_vals = explainer(X_2d)\n",
    "    mean_abs_shap = np.abs(shap_vals.values).mean(axis=0)\n",
    "    if mean_abs_shap.ndim > 1:\n",
    "        mean_abs_shap = mean_abs_shap.mean(axis=0)\n",
    "    shap_top_idx = np.argsort(mean_abs_shap)[::-1]\n",
    "    shap_top_feats = df.drop(columns=[target_col]).columns.to_numpy()[shap_top_idx].tolist()\n",
    "    print(\"\\nTop SHAP Features:\", shap_top_feats[:10])\n",
    "\n",
    "    # --- Association Rules ---\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y_str = y.astype(str)\n",
    "    df_disc = X.apply(lambda col: pd.qcut(col, q=4, duplicates='drop').astype(str))\n",
    "    df_disc['target'] = y_str\n",
    "    transactions = [[f\"{col}={val}\" for col, val in row.items()] for _, row in df_disc.iterrows()]\n",
    "    te = TransactionEncoder()\n",
    "    df_encoded = pd.DataFrame(te.fit(transactions).transform(transactions), columns=te.columns_)\n",
    "    freq_items = apriori(df_encoded, min_support=0.1, use_colnames=True)\n",
    "    rules = association_rules(freq_items, metric=\"lift\", min_threshold=1.2)\n",
    "\n",
    "    rule_feat_scores = {}\n",
    "    all_rule_feats_set = set()\n",
    "\n",
    "    for cls in y.unique():\n",
    "        class_rules = rules[rules['consequents'].apply(lambda x: f'target={cls}' in x)]\n",
    "        feat_counts = {}\n",
    "        for _, row in class_rules.iterrows():\n",
    "            for antecedent in row['antecedents']:\n",
    "                if not antecedent.startswith('target='):\n",
    "                    feat = antecedent.split('=')[0]\n",
    "                    feat_counts[feat] = feat_counts.get(feat, 0) + 1\n",
    "                    all_rule_feats_set.add(feat)\n",
    "        rule_feat_scores.update(feat_counts)\n",
    "        sorted_feats = sorted(feat_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "        print(f\"\\nClass {cls} Rule Features: {[f for f, _ in sorted_feats]}\")\n",
    "\n",
    "    # --- Overlap ---\n",
    "    overlapping_features = set(shap_top_feats[:10]).intersection(all_rule_feats_set)\n",
    "    print(\"\\nOverlapping Features:\", list(overlapping_features))\n",
    "\n",
    "    # --- Trust Metrics ---\n",
    "    def trust_report(name, shap_list, rule_list):\n",
    "        precision, coverage, jaccard = compute_trust_metrics(shap_list, rule_list)\n",
    "        print(f\"{name} → Precision@10: {precision:.2f}, Coverage: {coverage:.2f}, Jaccard: {jaccard:.2f}\")\n",
    "    print(\"\\n--- Trust Metrics Summary ---\")\n",
    "    trust_report(\"SHAP vs Rules\", shap_top_feats, list(all_rule_feats_set))\n",
    "    trust_report(\"Overlap vs SHAP\", list(overlapping_features), shap_top_feats)\n",
    "    trust_report(\"Overlap vs Rules\", list(overlapping_features), list(all_rule_feats_set))\n",
    "\n",
    "    # --- Visualization: Heatmap of SHAP vs Rule-based Scores ---\n",
    "    top_feats = shap_top_feats[:10]\n",
    "    shap_dict = {feat: mean_abs_shap[df.columns.get_loc(feat)] for feat in top_feats}\n",
    "    rule_dict = {feat: rule_feat_scores.get(feat, 0) for feat in top_feats}\n",
    "    heatmap_df = pd.DataFrame([shap_dict, rule_dict], index=['SHAP', 'Rule-based'])\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    sns.heatmap(heatmap_df, annot=True, cmap=\"YlGnBu\", fmt=\".2f\")\n",
    "    plt.title(\"Heatmap of SHAP vs Rule-based Feature Importance\")\n",
    "    plt.xlabel(\"Top Features\")\n",
    "    plt.ylabel(\"Method\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ---------------------------\n",
    "# 8. Main\n",
    "# ---------------------------\n",
    "def main():\n",
    "    train_path = r\"E:\\Abroad period research\\Phenology datasets\\Lasso_Selected_Features_H2\\train_lasso_selected.csv\"\n",
    "    test_path = r\"E:\\Abroad period research\\Phenology datasets\\Lasso_Selected_Features_H2\\test_lasso_selected.csv\"\n",
    "    df_train, target_col = load_and_preprocess_data(train_path)\n",
    "    X_train_2d, X_train_3d, y_train, _ = feature_scaling(df_train, target_col)\n",
    "    df_test, _ = load_and_preprocess_data(test_path)\n",
    "    X_test_2d, X_test_3d, y_test, _ = feature_scaling(df_test, target_col)\n",
    "    print(\"\\nOptimizing Hyperparameters...\")\n",
    "    best_params = optimize_hyperparameters(X_train_3d, y_train)\n",
    "    print(\"\\nTraining Model...\")\n",
    "    model = train_and_evaluate_model(X_train_3d, X_test_3d, y_train, y_test, best_params)\n",
    "    print(\"\\nRunning Hybrid XAI...\")\n",
    "    hybrid_xai_analysis(model, X_train_2d, y_train, df_train, target_col)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1f5e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import shap\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout\n",
    "from bayes_opt import BayesianOptimization\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(42)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "class GRUModelWrapper:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def __call__(self, data):\n",
    "        if len(data.shape) == 2:\n",
    "            data = data[:, np.newaxis, :]\n",
    "        return self.model(data).numpy()\n",
    "\n",
    "def load_and_preprocess_data(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df.columns = df.columns.str.strip().str.replace('\"', '')\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'], errors='coerce')\n",
    "    df.dropna(subset=['TIME'], inplace=True)\n",
    "    df.set_index('TIME', inplace=True)\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "    target_col = 'fenologia_h2'\n",
    "    df[target_col].interpolate(method='linear', inplace=True)\n",
    "    df[target_col] = df[target_col] - 1\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "    return df, target_col\n",
    "\n",
    "def feature_scaling(df, target_col):\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled_3d = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))\n",
    "    return X_scaled, X_scaled_3d, y, scaler\n",
    "\n",
    "def build_gru_model(input_shape, num_classes=4, num_units=64, dropout_rate=0.2):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(num_units, input_shape=input_shape))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(num_units // 2, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def objective_function(num_units, dropout_rate, epochs, batch_size, X_train, y_train):\n",
    "    model = build_gru_model((X_train.shape[1], X_train.shape[2]), 4, int(num_units), dropout_rate)\n",
    "    history = model.fit(X_train, y_train, epochs=int(epochs), batch_size=int(batch_size), validation_split=0.2, verbose=0)\n",
    "    return np.max(history.history['val_accuracy'])\n",
    "\n",
    "def optimize_hyperparameters(X_train, y_train):\n",
    "    pbounds = {\n",
    "        'num_units': (32, 128),\n",
    "        'dropout_rate': (0.1, 0.5),\n",
    "        'epochs': (10, 30),\n",
    "        'batch_size': (16, 64)\n",
    "    }\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=lambda num_units, dropout_rate, epochs, batch_size: objective_function(\n",
    "            num_units, dropout_rate, epochs, batch_size, X_train, y_train\n",
    "        ),\n",
    "        pbounds=pbounds,\n",
    "        random_state=42,\n",
    "        verbose=2\n",
    "    )\n",
    "    optimizer.maximize(init_points=2, n_iter=3)\n",
    "    return optimizer.max['params']\n",
    "\n",
    "def train_and_evaluate_model(X_train, X_test, y_train, y_test, best_params):\n",
    "    model = build_gru_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        num_classes=4,\n",
    "        num_units=int(best_params['num_units']),\n",
    "        dropout_rate=float(best_params['dropout_rate'])\n",
    "    )\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=int(best_params['epochs']),\n",
    "        batch_size=int(best_params['batch_size']),\n",
    "        validation_split=0.2,\n",
    "        verbose=1\n",
    "    )\n",
    "    y_pred = model.predict(X_test).argmax(axis=1)\n",
    "    print(f\"\\nAccuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    return model\n",
    "\n",
    "def compute_trust_metrics(shap_feats, rule_feats, k=10):\n",
    "    shap_top_k = set(shap_feats[:k])\n",
    "    rule_top = set(rule_feats)\n",
    "    intersection = shap_top_k.intersection(rule_top)\n",
    "    precision_at_k = len(intersection) / k\n",
    "    coverage = len(intersection) / len(rule_top) if rule_top else 0\n",
    "    jaccard = len(intersection) / len(shap_top_k.union(rule_top)) if shap_top_k.union(rule_top) else 0\n",
    "    return precision_at_k, coverage, jaccard\n",
    "\n",
    "def hybrid_xai_analysis(model, X_2d, y, df, target_col):\n",
    "    print(\"\\n===== XAI Analysis (SHAP + Rules + Trust Score) =====\")\n",
    "\n",
    "    wrapped_model = GRUModelWrapper(model)\n",
    "    explainer = shap.Explainer(wrapped_model, X_2d)\n",
    "    shap_vals = explainer(X_2d)\n",
    "    mean_abs_shap = np.abs(shap_vals.values).mean(axis=0)\n",
    "    if mean_abs_shap.ndim > 1:\n",
    "        mean_abs_shap = mean_abs_shap.mean(axis=0)\n",
    "    shap_top_idx = np.argsort(mean_abs_shap)[::-1]\n",
    "    shap_top_feats = df.drop(columns=[target_col]).columns.to_numpy()[shap_top_idx].tolist()\n",
    "    print(\"\\nTop SHAP Features:\", shap_top_feats[:10])\n",
    "\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y_str = y.astype(str)\n",
    "    df_disc = X.apply(lambda col: pd.qcut(col, q=4, duplicates='drop').astype(str))\n",
    "    df_disc['target'] = y_str\n",
    "    transactions = [[f\"{col}={val}\" for col, val in row.items()] for _, row in df_disc.iterrows()]\n",
    "    te = TransactionEncoder()\n",
    "    df_encoded = pd.DataFrame(te.fit(transactions).transform(transactions), columns=te.columns_)\n",
    "    freq_items = apriori(df_encoded, min_support=0.1, use_colnames=True)\n",
    "    rules = association_rules(freq_items, metric=\"lift\", min_threshold=1.2)\n",
    "\n",
    "    rule_top_feats = {}\n",
    "    all_rule_feats_set = set()\n",
    "\n",
    "    for cls in y.unique():\n",
    "        class_rules = rules[rules['consequents'].apply(lambda x: f'target={cls}' in x)]\n",
    "        feat_counts = {}\n",
    "        for _, row in class_rules.iterrows():\n",
    "            for antecedent in row['antecedents']:\n",
    "                if not antecedent.startswith('target='):\n",
    "                    feat = antecedent.split('=')[0]\n",
    "                    feat_counts[feat] = feat_counts.get(feat, 0) + 1\n",
    "                    all_rule_feats_set.add(feat)\n",
    "        sorted_feats = sorted(feat_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "        rule_top_feats[cls] = sorted_feats\n",
    "        print(f\"\\nClass {cls} Rule Features: {[f for f, _ in sorted_feats]}\")\n",
    "\n",
    "    overlapping_features = set(shap_top_feats[:10]).intersection(all_rule_feats_set)\n",
    "    print(\"\\nOverlapping Features:\", list(overlapping_features))\n",
    "\n",
    "    def trust_report(name, shap_list, rule_list):\n",
    "        precision, coverage, jaccard = compute_trust_metrics(shap_list, rule_list)\n",
    "        print(f\"{name} → Precision@10: {precision:.2f}, Coverage: {coverage:.2f}, Jaccard: {jaccard:.2f}\")\n",
    "\n",
    "    print(\"\\n--- Trust Metrics Summary ---\")\n",
    "    trust_report(\"SHAP vs Rules\", shap_top_feats, list(all_rule_feats_set))\n",
    "    trust_report(\"Overlap vs SHAP\", list(overlapping_features), shap_top_feats)\n",
    "    trust_report(\"Overlap vs Rules\", list(overlapping_features), list(all_rule_feats_set))\n",
    "\n",
    "    # --- Combined Feature Importance Line Plot ---\n",
    "    print(\"\\nGenerating Combined Feature Importance Line Plot...\")\n",
    "    shap_vals_norm = mean_abs_shap / np.sum(mean_abs_shap)\n",
    "    rule_feat_counts = {}\n",
    "    for cls_feats in rule_top_feats.values():\n",
    "        for feat, count in cls_feats:\n",
    "            rule_feat_counts[feat] = rule_feat_counts.get(feat, 0) + count\n",
    "    rule_feats_sorted = sorted(rule_feat_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    rule_feats, rule_counts = zip(*rule_feats_sorted)\n",
    "    rule_freq_norm = np.array(rule_counts) / np.sum(rule_counts)\n",
    "\n",
    "    top_n = 20\n",
    "    combined_features = list(dict.fromkeys(shap_top_feats[:top_n] + list(rule_feats[:top_n])))\n",
    "\n",
    "    shap_dict = dict(zip(df.drop(columns=[target_col]).columns, shap_vals_norm))\n",
    "    rule_dict = dict(zip(rule_feats, rule_freq_norm))\n",
    "    shap_vals_plot = [shap_dict.get(f, 0) for f in combined_features]\n",
    "    rule_vals_plot = [rule_dict.get(f, 0) for f in combined_features]\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 6))\n",
    "    ax1.plot(combined_features, shap_vals_plot, label='Normalized SHAP', color='blue', marker='o')\n",
    "    ax1.set_ylabel('Normalized SHAP Importance', color='blue')\n",
    "    ax1.tick_params(axis='y', labelcolor='blue')\n",
    "    ax1.set_xticks(range(len(combined_features)))\n",
    "    ax1.set_xticklabels(combined_features, rotation=45, ha='right')\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(combined_features, rule_vals_plot, label='Normalized Rule Frequency', color='green', marker='x')\n",
    "    ax2.set_ylabel('Normalized Rule Frequency', color='green')\n",
    "    ax2.tick_params(axis='y', labelcolor='green')\n",
    "\n",
    "    plt.title('Combined Feature Importance: SHAP vs Rule Frequency')\n",
    "    fig.tight_layout()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    train_path = r\"E:\\Abroad period research\\Phenology datasets\\Lasso_Selected_Features_H2\\train_lasso_selected.csv\"\n",
    "    test_path = r\"E:\\Abroad period research\\Phenology datasets\\Lasso_Selected_Features_H2\\test_lasso_selected.csv\"\n",
    "\n",
    "    df_train, target_col = load_and_preprocess_data(train_path)\n",
    "    X_train_2d, X_train_3d, y_train, _ = feature_scaling(df_train, target_col)\n",
    "\n",
    "    df_test, _ = load_and_preprocess_data(test_path)\n",
    "    X_test_2d, X_test_3d, y_test, _ = feature_scaling(df_test, target_col)\n",
    "\n",
    "    print(\"\\nOptimizing Hyperparameters...\")\n",
    "    best_params = optimize_hyperparameters(X_train_3d, y_train)\n",
    "\n",
    "    print(\"\\nTraining Model...\")\n",
    "    model = train_and_evaluate_model(X_train_3d, X_test_3d, y_train, y_test, best_params)\n",
    "\n",
    "    print(\"\\nRunning Hybrid XAI...\")\n",
    "    hybrid_xai_analysis(model, X_train_2d, y_train, df_train, target_col)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47533e2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "232c4baa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dcc73592",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db70790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import time\n",
    "# import shap\n",
    "# import warnings\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import GRU, Dense, Dropout\n",
    "# from bayes_opt import BayesianOptimization\n",
    "# from mlxtend.frequent_patterns import apriori, association_rules\n",
    "# from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "# np.random.seed(42)\n",
    "# import tensorflow as tf\n",
    "# tf.random.set_seed(42)\n",
    "\n",
    "# # ---------------------------\n",
    "# # SHAP-Compatible Wrapper\n",
    "# # ---------------------------\n",
    "# class GRUModelWrapper:\n",
    "#     def __init__(self, model):\n",
    "#         self.model = model\n",
    "\n",
    "#     def __call__(self, data):\n",
    "#         if len(data.shape) == 2:\n",
    "#             data = data[:, np.newaxis, :]  # Add time step dimension\n",
    "#         return self.model(data).numpy()\n",
    "\n",
    "# # ---------------------------\n",
    "# # 1. Load and Preprocess Data\n",
    "# # ---------------------------\n",
    "# def load_and_preprocess_data(filepath):\n",
    "#     df = pd.read_csv(filepath)\n",
    "#     df.columns = df.columns.str.strip().str.replace('\"', '')\n",
    "#     df['TIME'] = pd.to_datetime(df['TIME'], errors='coerce')\n",
    "#     df.dropna(subset=['TIME'], inplace=True)\n",
    "#     df.set_index('TIME', inplace=True)\n",
    "#     df = df.apply(pd.to_numeric, errors='coerce')\n",
    "#     df.dropna(axis=1, how='all', inplace=True)\n",
    "#     target_col = 'fenologia_h2'\n",
    "#     df[target_col].interpolate(method='linear', inplace=True)\n",
    "#     df[target_col] = df[target_col] - 1\n",
    "#     df.fillna(df.median(), inplace=True)\n",
    "#     return df, target_col\n",
    "\n",
    "# # ---------------------------\n",
    "# # 2. Feature Scaling\n",
    "# # ---------------------------\n",
    "# def feature_scaling(df, target_col):\n",
    "#     X = df.drop(columns=[target_col])\n",
    "#     y = df[target_col]\n",
    "#     scaler = MinMaxScaler()\n",
    "#     X_scaled = scaler.fit_transform(X)\n",
    "#     X_scaled_3d = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))\n",
    "#     return X_scaled, X_scaled_3d, y, scaler\n",
    "\n",
    "# # ---------------------------\n",
    "# # 3. Build GRU Model\n",
    "# # ---------------------------\n",
    "# def build_gru_model(input_shape, num_classes=4, num_units=64, dropout_rate=0.2):\n",
    "#     model = Sequential()\n",
    "#     model.add(GRU(num_units, input_shape=input_shape))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "#     model.add(Dense(num_units // 2, activation='relu'))\n",
    "#     model.add(Dense(num_classes, activation='softmax'))\n",
    "#     model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# # ---------------------------\n",
    "# # 4. Bayesian Optimization\n",
    "# # ---------------------------\n",
    "# def objective_function(num_units, dropout_rate, epochs, batch_size, X_train, y_train):\n",
    "#     model = build_gru_model((X_train.shape[1], X_train.shape[2]), 4, int(num_units), dropout_rate)\n",
    "#     history = model.fit(X_train, y_train, epochs=int(epochs), batch_size=int(batch_size), validation_split=0.2, verbose=0)\n",
    "#     return np.max(history.history['val_accuracy'])\n",
    "\n",
    "# def optimize_hyperparameters(X_train, y_train):\n",
    "#     pbounds = {\n",
    "#         'num_units': (32, 128),\n",
    "#         'dropout_rate': (0.1, 0.5),\n",
    "#         'epochs': (10, 30),\n",
    "#         'batch_size': (16, 64)\n",
    "#     }\n",
    "#     optimizer = BayesianOptimization(\n",
    "#         f=lambda num_units, dropout_rate, epochs, batch_size: objective_function(\n",
    "#             num_units, dropout_rate, epochs, batch_size, X_train, y_train\n",
    "#         ),\n",
    "#         pbounds=pbounds,\n",
    "#         random_state=42,\n",
    "#         verbose=2\n",
    "#     )\n",
    "#     optimizer.maximize(init_points=2, n_iter=3)\n",
    "#     return optimizer.max['params']\n",
    "\n",
    "# # ---------------------------\n",
    "# # 5. Train & Evaluate Model\n",
    "# # ---------------------------\n",
    "# def train_and_evaluate_model(X_train, X_test, y_train, y_test, best_params):\n",
    "#     model = build_gru_model(\n",
    "#         input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "#         num_classes=4,\n",
    "#         num_units=int(best_params['num_units']),\n",
    "#         dropout_rate=float(best_params['dropout_rate'])\n",
    "#     )\n",
    "#     model.fit(\n",
    "#         X_train, y_train,\n",
    "#         epochs=int(best_params['epochs']),\n",
    "#         batch_size=int(best_params['batch_size']),\n",
    "#         validation_split=0.2,\n",
    "#         verbose=1\n",
    "#     )\n",
    "#     y_pred = model.predict(X_test).argmax(axis=1)\n",
    "#     print(f\"\\nAccuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "#     print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "#     print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "#     return model\n",
    "\n",
    "# # ---------------------------\n",
    "# # 6. Hybrid XAI: SHAP + Rules\n",
    "# # ---------------------------\n",
    "# def hybrid_xai_analysis(model, X_2d, y, df, target_col, top_k=10):\n",
    "#     print(\"\\n===== XAI Analysis (SHAP + Rules) =====\")\n",
    "\n",
    "#     # --- SHAP ---\n",
    "#     print(\"\\nTop SHAP Features:\")\n",
    "#     wrapped_model = GRUModelWrapper(model)\n",
    "#     explainer = shap.Explainer(wrapped_model, X_2d)\n",
    "#     shap_vals = explainer(X_2d)\n",
    "#     mean_abs_shap = np.abs(shap_vals.values).mean(axis=0)\n",
    "\n",
    "#     shap_top_idx = np.argsort(mean_abs_shap)[-top_k:][::-1]\n",
    "#     shap_top_idx = np.ravel(shap_top_idx)  # Ensure it's 1D\n",
    "#     shap_top_feats = df.drop(columns=[target_col]).columns.to_numpy()[shap_top_idx].tolist()\n",
    "#     print(shap_top_feats)\n",
    "\n",
    "#     # --- Association Rules ---\n",
    "#     print(\"\\nTop Rule-Based Features:\")\n",
    "#     X = df.drop(columns=[target_col])\n",
    "#     y_str = y.astype(str)\n",
    "#     df_disc = X.apply(lambda col: pd.qcut(col, q=4, duplicates='drop').astype(str))\n",
    "#     df_disc['target'] = y_str\n",
    "#     transactions = [[f\"{col}={val}\" for col, val in row.items()] for _, row in df_disc.iterrows()]\n",
    "#     te = TransactionEncoder()\n",
    "#     df_encoded = pd.DataFrame(te.fit(transactions).transform(transactions), columns=te.columns_)\n",
    "#     freq_items = apriori(df_encoded, min_support=0.1, use_colnames=True)\n",
    "#     rules = association_rules(freq_items, metric=\"lift\", min_threshold=1.2)\n",
    "\n",
    "#     rule_top_feats = {}\n",
    "#     for cls in y.unique():\n",
    "#         class_rules = rules[rules['consequents'].apply(lambda x: f'target={cls}' in x)]\n",
    "#         feat_counts = {}\n",
    "#         for _, row in class_rules.iterrows():\n",
    "#             for antecedent in row['antecedents']:\n",
    "#                 if not antecedent.startswith('target='):\n",
    "#                     feat = antecedent.split('=')[0]\n",
    "#                     feat_counts[feat] = feat_counts.get(feat, 0) + 1\n",
    "#         sorted_feats = sorted(feat_counts.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "#         rule_top_feats[cls] = sorted_feats\n",
    "#         print(f\"\\nClass {cls} Features: {[f for f, _ in sorted_feats]}\")\n",
    "\n",
    "#     # --- Overlap Analysis ---\n",
    "#     print(\"\\nOverlap SHAP ↔ Rule-Based Features:\")\n",
    "#     for cls, feats in rule_top_feats.items():\n",
    "#         rule_feats = {f for f, _ in feats}\n",
    "#         overlap = set(shap_top_feats).intersection(rule_feats)\n",
    "#         print(f\"Class {cls} → Overlap: {overlap} (count: {len(overlap)})\")\n",
    "\n",
    "# # ---------------------------\n",
    "# # 7. Main\n",
    "# # ---------------------------\n",
    "# def main():\n",
    "#     train_path = r\"E:\\Abroad period research\\Phenology datasets\\Lasso_Selected_Features_H2\\train_lasso_selected.csv\"\n",
    "#     test_path = r\"E:\\Abroad period research\\Phenology datasets\\Lasso_Selected_Features_H2\\test_lasso_selected.csv\"\n",
    "\n",
    "#     df_train, target_col = load_and_preprocess_data(train_path)\n",
    "#     X_train_2d, X_train_3d, y_train, _ = feature_scaling(df_train, target_col)\n",
    "\n",
    "#     df_test, _ = load_and_preprocess_data(test_path)\n",
    "#     X_test_2d, X_test_3d, y_test, _ = feature_scaling(df_test, target_col)\n",
    "\n",
    "#     print(\"\\nOptimizing Hyperparameters...\")\n",
    "#     best_params = optimize_hyperparameters(X_train_3d, y_train)\n",
    "\n",
    "#     print(\"\\nTraining Model...\")\n",
    "#     model = train_and_evaluate_model(X_train_3d, X_test_3d, y_train, y_test, best_params)\n",
    "\n",
    "#     print(\"\\nRunning Hybrid XAI...\")\n",
    "#     hybrid_xai_analysis(model, X_train_2d, y_train, df_train, target_col)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661328c6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42cd394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import time\n",
    "# import shap\n",
    "# import warnings\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import GRU, Dense, Dropout\n",
    "# from bayes_opt import BayesianOptimization\n",
    "# from mlxtend.frequent_patterns import apriori, association_rules\n",
    "# from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "# np.random.seed(42)\n",
    "# import tensorflow as tf\n",
    "# tf.random.set_seed(42)\n",
    "\n",
    "# # ---------------------------\n",
    "# # SHAP-Compatible Wrapper\n",
    "# # ---------------------------\n",
    "# class GRUModelWrapper:\n",
    "#     def __init__(self, model):\n",
    "#         self.model = model\n",
    "\n",
    "#     def __call__(self, data):\n",
    "#         if len(data.shape) == 2:\n",
    "#             data = data[:, np.newaxis, :]  # Add time step dimension\n",
    "#         return self.model(data).numpy()\n",
    "\n",
    "# # ---------------------------\n",
    "# # 1. Load and Preprocess Data\n",
    "# # ---------------------------\n",
    "# def load_and_preprocess_data(filepath):\n",
    "#     df = pd.read_csv(filepath)\n",
    "#     df.columns = df.columns.str.strip().str.replace('\"', '')\n",
    "#     df['TIME'] = pd.to_datetime(df['TIME'], errors='coerce')\n",
    "#     df.dropna(subset=['TIME'], inplace=True)\n",
    "#     df.set_index('TIME', inplace=True)\n",
    "#     df = df.apply(pd.to_numeric, errors='coerce')\n",
    "#     df.dropna(axis=1, how='all', inplace=True)\n",
    "#     target_col = 'fenologia_h2'\n",
    "#     df[target_col].interpolate(method='linear', inplace=True)\n",
    "#     df[target_col] = df[target_col] - 1\n",
    "#     df.fillna(df.median(), inplace=True)\n",
    "#     return df, target_col\n",
    "\n",
    "# # ---------------------------\n",
    "# # 2. Feature Scaling\n",
    "# # ---------------------------\n",
    "# def feature_scaling(df, target_col):\n",
    "#     X = df.drop(columns=[target_col])\n",
    "#     y = df[target_col]\n",
    "#     scaler = MinMaxScaler()\n",
    "#     X_scaled = scaler.fit_transform(X)\n",
    "#     X_scaled_3d = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))\n",
    "#     return X_scaled, X_scaled_3d, y, scaler\n",
    "\n",
    "# # ---------------------------\n",
    "# # 3. Build GRU Model\n",
    "# # ---------------------------\n",
    "# def build_gru_model(input_shape, num_classes=4, num_units=64, dropout_rate=0.2):\n",
    "#     model = Sequential()\n",
    "#     model.add(GRU(num_units, input_shape=input_shape))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "#     model.add(Dense(num_units // 2, activation='relu'))\n",
    "#     model.add(Dense(num_classes, activation='softmax'))\n",
    "#     model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# # ---------------------------\n",
    "# # 4. Bayesian Optimization\n",
    "# # ---------------------------\n",
    "# def objective_function(num_units, dropout_rate, epochs, batch_size, X_train, y_train):\n",
    "#     model = build_gru_model((X_train.shape[1], X_train.shape[2]), 4, int(num_units), dropout_rate)\n",
    "#     history = model.fit(X_train, y_train, epochs=int(epochs), batch_size=int(batch_size), validation_split=0.2, verbose=0)\n",
    "#     return np.max(history.history['val_accuracy'])\n",
    "\n",
    "# def optimize_hyperparameters(X_train, y_train):\n",
    "#     pbounds = {\n",
    "#         'num_units': (32, 128),\n",
    "#         'dropout_rate': (0.1, 0.5),\n",
    "#         'epochs': (10, 30),\n",
    "#         'batch_size': (16, 64)\n",
    "#     }\n",
    "#     optimizer = BayesianOptimization(\n",
    "#         f=lambda num_units, dropout_rate, epochs, batch_size: objective_function(\n",
    "#             num_units, dropout_rate, epochs, batch_size, X_train, y_train\n",
    "#         ),\n",
    "#         pbounds=pbounds,\n",
    "#         random_state=42,\n",
    "#         verbose=2\n",
    "#     )\n",
    "#     optimizer.maximize(init_points=2, n_iter=3)\n",
    "#     return optimizer.max['params']\n",
    "\n",
    "# # ---------------------------\n",
    "# # 5. Train & Evaluate Model\n",
    "# # ---------------------------\n",
    "# def train_and_evaluate_model(X_train, X_test, y_train, y_test, best_params):\n",
    "#     model = build_gru_model(\n",
    "#         input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "#         num_classes=4,\n",
    "#         num_units=int(best_params['num_units']),\n",
    "#         dropout_rate=float(best_params['dropout_rate'])\n",
    "#     )\n",
    "#     model.fit(\n",
    "#         X_train, y_train,\n",
    "#         epochs=int(best_params['epochs']),\n",
    "#         batch_size=int(best_params['batch_size']),\n",
    "#         validation_split=0.2,\n",
    "#         verbose=1\n",
    "#     )\n",
    "#     y_pred = model.predict(X_test).argmax(axis=1)\n",
    "#     print(f\"\\nAccuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "#     print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "#     print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "#     return model\n",
    "\n",
    "# # ---------------------------\n",
    "# # 6. Hybrid XAI: SHAP + Rules\n",
    "# # ---------------------------\n",
    "# def hybrid_xai_analysis(model, X_2d, y, df, target_col, top_k=10):\n",
    "#     print(\"\\n===== XAI Analysis (SHAP + Rules) =====\")\n",
    "    \n",
    "#     # --- SHAP ---\n",
    "#     print(\"\\nTop SHAP Features:\")\n",
    "#     wrapped_model = GRUModelWrapper(model)\n",
    "#     explainer = shap.Explainer(wrapped_model, X_2d)\n",
    "#     shap_vals = explainer(X_2d)\n",
    "#     mean_abs_shap = np.abs(shap_vals.values).mean(axis=0)\n",
    "#     shap_top_idx = np.argsort(mean_abs_shap)[-top_k:][::-1]\n",
    "#     shap_top_idx = np.array(shap_top_idx).flatten()  # ✅ FIXED HERE\n",
    "#     shap_top_feats = df.drop(columns=[target_col]).columns[shap_top_idx].tolist()\n",
    "#     print(shap_top_feats)\n",
    "\n",
    "#     # --- Association Rules ---\n",
    "#     print(\"\\nTop Rule-Based Features:\")\n",
    "#     X = df.drop(columns=[target_col])\n",
    "#     y_str = y.astype(str)\n",
    "#     df_disc = X.apply(lambda col: pd.qcut(col, q=4, duplicates='drop').astype(str))\n",
    "#     df_disc['target'] = y_str\n",
    "#     transactions = [[f\"{col}={val}\" for col, val in row.items()] for _, row in df_disc.iterrows()]\n",
    "#     te = TransactionEncoder()\n",
    "#     df_encoded = pd.DataFrame(te.fit(transactions).transform(transactions), columns=te.columns_)\n",
    "#     freq_items = apriori(df_encoded, min_support=0.1, use_colnames=True)\n",
    "#     rules = association_rules(freq_items, metric=\"lift\", min_threshold=1.2)\n",
    "    \n",
    "#     rule_top_feats = {}\n",
    "#     for cls in y.unique():\n",
    "#         class_rules = rules[rules['consequents'].apply(lambda x: f'target={cls}' in x)]\n",
    "#         feat_counts = {}\n",
    "#         for _, row in class_rules.iterrows():\n",
    "#             for antecedent in row['antecedents']:\n",
    "#                 if not antecedent.startswith('target='):\n",
    "#                     feat = antecedent.split('=')[0]\n",
    "#                     feat_counts[feat] = feat_counts.get(feat, 0) + 1\n",
    "#         sorted_feats = sorted(feat_counts.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "#         rule_top_feats[cls] = sorted_feats\n",
    "#         print(f\"\\nClass {cls} Features: {[f for f, _ in sorted_feats]}\")\n",
    "\n",
    "#     # --- Overlap Analysis ---\n",
    "#     print(\"\\nOverlap SHAP ↔ Rule-Based Features:\")\n",
    "#     for cls, feats in rule_top_feats.items():\n",
    "#         rule_feats = {f for f, _ in feats}\n",
    "#         overlap = set(shap_top_feats).intersection(rule_feats)\n",
    "#         print(f\"Class {cls} → Overlap: {overlap} (count: {len(overlap)})\")\n",
    "\n",
    "# # ---------------------------\n",
    "# # 7. Main\n",
    "# # ---------------------------\n",
    "# def main():\n",
    "#     train_path = r\"E:\\Abroad period research\\Phenology datasets\\Lasso_Selected_Features_H2\\train_lasso_selected.csv\"\n",
    "#     test_path = r\"E:\\Abroad period research\\Phenology datasets\\Lasso_Selected_Features_H2\\test_lasso_selected.csv\"\n",
    "\n",
    "#     df_train, target_col = load_and_preprocess_data(train_path)\n",
    "#     X_train_2d, X_train_3d, y_train, _ = feature_scaling(df_train, target_col)\n",
    "\n",
    "#     df_test, _ = load_and_preprocess_data(test_path)\n",
    "#     X_test_2d, X_test_3d, y_test, _ = feature_scaling(df_test, target_col)\n",
    "\n",
    "#     print(\"\\nOptimizing Hyperparameters...\")\n",
    "#     best_params = optimize_hyperparameters(X_train_3d, y_train)\n",
    "\n",
    "#     print(\"\\nTraining Model...\")\n",
    "#     model = train_and_evaluate_model(X_train_3d, X_test_3d, y_train, y_test, best_params)\n",
    "\n",
    "#     print(\"\\nRunning Hybrid XAI...\")\n",
    "#     hybrid_xai_analysis(model, X_train_2d, y_train, df_train, target_col)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549e6335",
   "metadata": {},
   "source": [
    "Displaying the association rules without any order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45488fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout\n",
    "from bayes_opt import BayesianOptimization\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "import tensorflow as tf\n",
    "\n",
    "# SHAP-Compatible Wrapper for GRU model\n",
    "class GRUModelWrapper:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def __call__(self, data):\n",
    "        if len(data.shape) == 2:\n",
    "            data = data[:, np.newaxis, :]  # Add time step dimension\n",
    "        return self.model(data).numpy()\n",
    "\n",
    "# Load and preprocess data\n",
    "def load_and_preprocess_data(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df.columns = df.columns.str.strip().str.replace('\"', '')\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'], errors='coerce')\n",
    "    df.dropna(subset=['TIME'], inplace=True)\n",
    "    df.set_index('TIME', inplace=True)\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "    target_col = 'fenologia_h2'\n",
    "    df[target_col].interpolate(method='linear', inplace=True)\n",
    "    df[target_col] = df[target_col] - 1\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "    return df, target_col\n",
    "\n",
    "# Feature scaling\n",
    "def feature_scaling(df, target_col):\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled_3d = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))\n",
    "    return X_scaled, X_scaled_3d, y, scaler\n",
    "\n",
    "# Build GRU model\n",
    "def build_gru_model(input_shape, num_classes=4, num_units=64, dropout_rate=0.2):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(num_units, input_shape=input_shape))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(num_units // 2, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Bayesian Optimization\n",
    "def objective_function(num_units, dropout_rate, epochs, batch_size, X_train, y_train):\n",
    "    model = build_gru_model((X_train.shape[1], X_train.shape[2]), 4, int(num_units), dropout_rate)\n",
    "    history = model.fit(X_train, y_train, epochs=int(epochs), batch_size=int(batch_size), validation_split=0.2, verbose=0)\n",
    "    return np.max(history.history['val_accuracy'])\n",
    "\n",
    "def optimize_hyperparameters(X_train, y_train):\n",
    "    pbounds = {\n",
    "        'num_units': (32, 128),\n",
    "        'dropout_rate': (0.1, 0.5),\n",
    "        'epochs': (10, 30),\n",
    "        'batch_size': (16, 64)\n",
    "    }\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=lambda num_units, dropout_rate, epochs, batch_size: objective_function(\n",
    "            num_units, dropout_rate, epochs, batch_size, X_train, y_train\n",
    "        ),\n",
    "        pbounds=pbounds,\n",
    "        random_state=42,\n",
    "        verbose=2\n",
    "    )\n",
    "    optimizer.maximize(init_points=1, n_iter=1)\n",
    "    return optimizer.max['params']\n",
    "\n",
    "# Train & evaluate model\n",
    "def train_and_evaluate_model(X_train, X_test, y_train, y_test, best_params):\n",
    "    model = build_gru_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        num_classes=4,\n",
    "        num_units=int(best_params['num_units']),\n",
    "        dropout_rate=float(best_params['dropout_rate'])\n",
    "    )\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=int(best_params['epochs']),\n",
    "        batch_size=int(best_params['batch_size']),\n",
    "        validation_split=0.2,\n",
    "        verbose=1\n",
    "    )\n",
    "    y_pred = model.predict(X_test).argmax(axis=1)\n",
    "    print(f\"\\nAccuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    return model\n",
    "\n",
    "# Association Rules Analysis\n",
    "def rules_analysis(df, y, target_col, top_k=10):\n",
    "    print(\"\\n===== Association Rules Analysis =====\")\n",
    "    \n",
    "    # --- Association Rules ---\n",
    "    print(\"\\nTop Rule-Based Features:\")\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y_str = y.astype(str)\n",
    "    df_disc = X.apply(lambda col: pd.qcut(col, q=4, duplicates='drop').astype(str))\n",
    "    df_disc['target'] = y_str\n",
    "    transactions = [[f\"{col}={val}\" for col, val in row.items()] for _, row in df_disc.iterrows()]\n",
    "    te = TransactionEncoder()\n",
    "    df_encoded = pd.DataFrame(te.fit(transactions).transform(transactions), columns=te.columns_)\n",
    "    freq_items = apriori(df_encoded, min_support=0.1, use_colnames=True)\n",
    "    rules = association_rules(freq_items, metric=\"lift\", min_threshold=1.2)\n",
    "\n",
    "    # Print Association Rules\n",
    "    print(\"\\nAssociation Rules:\")\n",
    "    for idx, rule in rules.iterrows():\n",
    "        print(f\"Rule: {rule['antecedents']} => {rule['consequents']}, Support: {rule['support']}, Lift: {rule['lift']}\")\n",
    "\n",
    "    # Visualization of Association Rules\n",
    "    rule_support = rules['support']\n",
    "    rule_lift = rules['lift']\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(rule_support, rule_lift, alpha=0.5)\n",
    "    plt.title(\"Support vs Lift of Association Rules\")\n",
    "    plt.xlabel(\"Support\")\n",
    "    plt.ylabel(\"Lift\")\n",
    "    plt.show()\n",
    "\n",
    "    rule_top_feats = {}\n",
    "    for cls in y.unique():\n",
    "        class_rules = rules[rules['consequents'].apply(lambda x: f'target={cls}' in x)]\n",
    "        feat_counts = {}\n",
    "        for _, row in class_rules.iterrows():\n",
    "            for antecedent in row['antecedents']:\n",
    "                if not antecedent.startswith('target='):\n",
    "                    feat = antecedent.split('=')[0]\n",
    "                    feat_counts[feat] = feat_counts.get(feat, 0) + 1\n",
    "        sorted_feats = sorted(feat_counts.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "        rule_top_feats[cls] = sorted_feats\n",
    "        print(f\"\\nClass {cls} Features: {[f for f, _ in sorted_feats]}\")\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    train_path = r\"E:\\Abroad period research\\Phenology datasets\\Lasso_Selected_Features_H2\\train_lasso_selected.csv\"\n",
    "    test_path = r\"E:\\Abroad period research\\Phenology datasets\\Lasso_Selected_Features_H2\\test_lasso_selected.csv\"\n",
    "\n",
    "    df_train, target_col = load_and_preprocess_data(train_path)\n",
    "    X_train_2d, X_train_3d, y_train, _ = feature_scaling(df_train, target_col)\n",
    "\n",
    "    df_test, _ = load_and_preprocess_data(test_path)\n",
    "    X_test_2d, X_test_3d, y_test, _ = feature_scaling(df_test, target_col)\n",
    "\n",
    "    print(\"\\nOptimizing Hyperparameters...\")\n",
    "    best_params = optimize_hyperparameters(X_train_3d, y_train)\n",
    "\n",
    "    print(\"\\nTraining Model...\")\n",
    "    model = train_and_evaluate_model(X_train_3d, X_test_3d, y_train, y_test, best_params)\n",
    "\n",
    "    # Perform Association Rules Analysis\n",
    "    rules_analysis(df_train, y_train, target_col)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c0918d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce11ac08",
   "metadata": {},
   "source": [
    "Displaying the SHAP feature importance with original feature names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c095c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- SHAP-Compatible Wrapper for GRU Model ---\n",
    "class GRUModelWrapper:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def __call__(self, data):\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            data = data.values  # Ensure it's a NumPy array\n",
    "        if len(data.shape) == 2:\n",
    "            data = data[:, np.newaxis, :]  # Add time step dimension\n",
    "        return self.model(data).numpy()\n",
    "\n",
    "# --- Load and Preprocess Data ---\n",
    "def load_and_preprocess_data(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df.columns = df.columns.str.strip().str.replace('\"', '')\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'], errors='coerce')\n",
    "    df.dropna(subset=['TIME'], inplace=True)\n",
    "    df.set_index('TIME', inplace=True)\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "    target_col = 'fenologia_h2'\n",
    "    df[target_col].interpolate(method='linear', inplace=True)\n",
    "    df[target_col] = df[target_col] - 1  # Normalize target column\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "    return df, target_col\n",
    "\n",
    "# --- Feature Scaling ---\n",
    "def feature_scaling(df, target_col):\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)\n",
    "    X_scaled_3d = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))  # For GRU\n",
    "    return X_scaled_df, X_scaled_3d, y, scaler\n",
    "\n",
    "# --- Build GRU Model ---\n",
    "def build_gru_model(input_shape, num_classes=4, num_units=64, dropout_rate=0.2):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(num_units, input_shape=input_shape))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(num_units // 2, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# --- Train the Model ---\n",
    "def train_and_evaluate_model(X_train, y_train, X_test, y_test):\n",
    "    model = build_gru_model((X_train.shape[1], X_train.shape[2]))\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, verbose=1)\n",
    "    y_pred = model.predict(X_test).argmax(axis=1)\n",
    "    accuracy = np.mean(y_pred == y_test)\n",
    "    print(f\"Model Accuracy: {accuracy:.4f}\")\n",
    "    return model\n",
    "\n",
    "# --- SHAP Analysis ---\n",
    "def shap_analysis(model, X_scaled_df, target_col, top_k=10):\n",
    "    print(\"\\n===== SHAP Analysis =====\")\n",
    "    wrapped_model = GRUModelWrapper(model)\n",
    "    \n",
    "    explainer = shap.Explainer(wrapped_model, X_scaled_df)\n",
    "    shap_vals = explainer(X_scaled_df)\n",
    "\n",
    "    # If SHAP returns multi-class output\n",
    "    if isinstance(shap_vals.values, list) or shap_vals.values.ndim == 3:\n",
    "        shap_values_to_plot = shap_vals[..., 0]\n",
    "    else:\n",
    "        shap_values_to_plot = shap_vals\n",
    "\n",
    "    # Compute mean absolute SHAP values\n",
    "    mean_abs_shap = np.abs(shap_values_to_plot.values).mean(axis=0)\n",
    "    shap_top_idx = np.argsort(mean_abs_shap)[-top_k:][::-1]\n",
    "    shap_top_feats = X_scaled_df.columns[shap_top_idx].tolist()\n",
    "\n",
    "    print(f\"Top SHAP Features: {shap_top_feats}\")\n",
    "\n",
    "    # SHAP Visualizations with original feature names\n",
    "    shap.summary_plot(shap_values_to_plot, X_scaled_df, plot_type=\"bar\")\n",
    "    shap.summary_plot(shap_values_to_plot, X_scaled_df)\n",
    "\n",
    "# --- Main Execution Flow ---\n",
    "def main():\n",
    "    train_path = r\"E:\\Abroad period research\\Phenology datasets\\Lasso_Selected_Features_H2\\train_lasso_selected.csv\"\n",
    "    test_path = r\"E:\\Abroad period research\\Phenology datasets\\Lasso_Selected_Features_H2\\test_lasso_selected.csv\"\n",
    "\n",
    "    df_train, target_col = load_and_preprocess_data(train_path)\n",
    "    X_train_df, X_train_3d, y_train, scaler = feature_scaling(df_train, target_col)\n",
    "\n",
    "    df_test, _ = load_and_preprocess_data(test_path)\n",
    "    X_test_df, X_test_3d, y_test, _ = feature_scaling(df_test, target_col)\n",
    "\n",
    "    model = train_and_evaluate_model(X_train_3d, y_train, X_test_3d, y_test)\n",
    "\n",
    "    # Run SHAP with feature names preserved\n",
    "    shap_analysis(model, X_train_df, target_col)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd68b148",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0bff591",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1463d550",
   "metadata": {},
   "source": [
    "Displaying the Association rules in descending order based on lift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af5d57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout\n",
    "from bayes_opt import BayesianOptimization\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "import tensorflow as tf\n",
    "\n",
    "# SHAP-Compatible Wrapper for GRU model\n",
    "class GRUModelWrapper:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def __call__(self, data):\n",
    "        if len(data.shape) == 2:\n",
    "            data = data[:, np.newaxis, :]  # Add time step dimension\n",
    "        return self.model(data).numpy()\n",
    "\n",
    "# Load and preprocess data\n",
    "def load_and_preprocess_data(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df.columns = df.columns.str.strip().str.replace('\"', '')\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'], errors='coerce')\n",
    "    df.dropna(subset=['TIME'], inplace=True)\n",
    "    df.set_index('TIME', inplace=True)\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "    target_col = 'fenologia_h2'\n",
    "    df[target_col].interpolate(method='linear', inplace=True)\n",
    "    df[target_col] = df[target_col] - 1\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "    return df, target_col\n",
    "\n",
    "# Feature scaling\n",
    "def feature_scaling(df, target_col):\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled_3d = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))\n",
    "    return X_scaled, X_scaled_3d, y, scaler\n",
    "\n",
    "# Build GRU model\n",
    "def build_gru_model(input_shape, num_classes=4, num_units=64, dropout_rate=0.2):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(num_units, input_shape=input_shape))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(num_units // 2, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Bayesian Optimization\n",
    "def objective_function(num_units, dropout_rate, epochs, batch_size, X_train, y_train):\n",
    "    model = build_gru_model((X_train.shape[1], X_train.shape[2]), 4, int(num_units), dropout_rate)\n",
    "    history = model.fit(X_train, y_train, epochs=int(epochs), batch_size=int(batch_size), validation_split=0.2, verbose=0)\n",
    "    return np.max(history.history['val_accuracy'])\n",
    "\n",
    "def optimize_hyperparameters(X_train, y_train):\n",
    "    pbounds = {\n",
    "        'num_units': (32, 128),\n",
    "        'dropout_rate': (0.1, 0.5),\n",
    "        'epochs': (10, 30),\n",
    "        'batch_size': (16, 64)\n",
    "    }\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=lambda num_units, dropout_rate, epochs, batch_size: objective_function(\n",
    "            num_units, dropout_rate, epochs, batch_size, X_train, y_train\n",
    "        ),\n",
    "        pbounds=pbounds,\n",
    "        random_state=42,\n",
    "        verbose=2\n",
    "    )\n",
    "    optimizer.maximize(init_points=1, n_iter=1)\n",
    "    return optimizer.max['params']\n",
    "\n",
    "# Train & evaluate model\n",
    "def train_and_evaluate_model(X_train, X_test, y_train, y_test, best_params):\n",
    "    model = build_gru_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        num_classes=4,\n",
    "        num_units=int(best_params['num_units']),\n",
    "        dropout_rate=float(best_params['dropout_rate'])\n",
    "    )\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=int(best_params['epochs']),\n",
    "        batch_size=int(best_params['batch_size']),\n",
    "        validation_split=0.2,\n",
    "        verbose=1\n",
    "    )\n",
    "    y_pred = model.predict(X_test).argmax(axis=1)\n",
    "    print(f\"\\nAccuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    return model\n",
    "\n",
    "# Association Rules Analysis (sorting by lift)\n",
    "def rules_analysis(df, y, target_col, top_k=10):\n",
    "    print(\"\\n===== Association Rules Analysis =====\")\n",
    "    \n",
    "    # --- Association Rules ---\n",
    "    print(\"\\nTop Rule-Based Features:\")\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y_str = y.astype(str)\n",
    "    df_disc = X.apply(lambda col: pd.qcut(col, q=4, duplicates='drop').astype(str))\n",
    "    df_disc['target'] = y_str\n",
    "    transactions = [[f\"{col}={val}\" for col, val in row.items()] for _, row in df_disc.iterrows()]\n",
    "    te = TransactionEncoder()\n",
    "    df_encoded = pd.DataFrame(te.fit(transactions).transform(transactions), columns=te.columns_)\n",
    "    freq_items = apriori(df_encoded, min_support=0.1, use_colnames=True)\n",
    "    rules = association_rules(freq_items, metric=\"lift\", min_threshold=1.2)\n",
    "\n",
    "    # Sort rules by lift in descending order\n",
    "    rules_sorted_by_lift = rules.sort_values(by='lift', ascending=False)\n",
    "\n",
    "    # Print sorted Association Rules\n",
    "    print(\"\\nSorted Association Rules by Lift:\")\n",
    "    for idx, rule in rules_sorted_by_lift.iterrows():\n",
    "        print(f\"Rule: {rule['antecedents']} => {rule['consequents']}, Support: {rule['support']}, Lift: {rule['lift']}\")\n",
    "\n",
    "    # Visualization of Association Rules\n",
    "    rule_support = rules_sorted_by_lift['support']\n",
    "    rule_lift = rules_sorted_by_lift['lift']\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(rule_support, rule_lift, alpha=0.5)\n",
    "    plt.title(\"Support vs Lift of Association Rules\")\n",
    "    plt.xlabel(\"Support\")\n",
    "    plt.ylabel(\"Lift\")\n",
    "    plt.show()\n",
    "\n",
    "    rule_top_feats = {}\n",
    "    for cls in y.unique():\n",
    "        class_rules = rules_sorted_by_lift[rules_sorted_by_lift['consequents'].apply(lambda x: f'target={cls}' in x)]\n",
    "        feat_counts = {}\n",
    "        for _, row in class_rules.iterrows():\n",
    "            for antecedent in row['antecedents']:\n",
    "                if not antecedent.startswith('target='):\n",
    "                    feat = antecedent.split('=')[0]\n",
    "                    feat_counts[feat] = feat_counts.get(feat, 0) + 1\n",
    "        sorted_feats = sorted(feat_counts.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "        rule_top_feats[cls] = sorted_feats\n",
    "        print(f\"\\nClass {cls} Features: {[f for f, _ in sorted_feats]}\")\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    train_path = r\"E:\\Abroad period research\\Phenology datasets\\Lasso_Selected_Features_H2\\train_lasso_selected.csv\"\n",
    "    test_path = r\"E:\\Abroad period research\\Phenology datasets\\Lasso_Selected_Features_H2\\test_lasso_selected.csv\"\n",
    "\n",
    "    df_train, target_col = load_and_preprocess_data(train_path)\n",
    "    X_train_2d, X_train_3d, y_train, _ = feature_scaling(df_train, target_col)\n",
    "\n",
    "    df_test, _ = load_and_preprocess_data(test_path)\n",
    "    X_test_2d, X_test_3d, y_test, _ = feature_scaling(df_test, target_col)\n",
    "\n",
    "    print(\"\\nOptimizing Hyperparameters...\")\n",
    "    best_params = optimize_hyperparameters(X_train_3d, y_train)\n",
    "\n",
    "    print(\"\\nTraining Model...\")\n",
    "    model = train_and_evaluate_model(X_train_3d, X_test_3d, y_train, y_test, best_params)\n",
    "\n",
    "    # Perform Association Rules Analysis\n",
    "    rules_analysis(df_train, y_train, target_col)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278e1c46",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0f9a70d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2651c2f1",
   "metadata": {},
   "source": [
    "Quantify Trust using:\n",
    "\n",
    "precision@k\n",
    "\n",
    "coverage\n",
    "\n",
    "Jaccard similarity\n",
    "\n",
    "Statistically Test the overlap between SHAP and rule-based features using permutation testing.\n",
    "\n",
    "Retrain a GRU model using only the overlapping features and compare performance to the full-feature model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d566627",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n",
    "import warnings\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout\n",
    "from bayes_opt import BayesianOptimization\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "import tensorflow as tf\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "class GRUModelWrapper:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def __call__(self, data):\n",
    "        if len(data.shape) == 2:\n",
    "            data = data[:, np.newaxis, :]\n",
    "        return self.model(data).numpy()\n",
    "\n",
    "def load_and_preprocess_data(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df.columns = df.columns.str.strip().str.replace('\"', '')\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'], errors='coerce')\n",
    "    df.dropna(subset=['TIME'], inplace=True)\n",
    "    df.set_index('TIME', inplace=True)\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "    target_col = 'fenologia_h2'\n",
    "    df[target_col].interpolate(method='linear', inplace=True)\n",
    "    df[target_col] = df[target_col] - 1\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "    return df, target_col\n",
    "\n",
    "def feature_scaling(df, target_col):\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled_3d = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))\n",
    "    return X_scaled, X_scaled_3d, y, scaler\n",
    "\n",
    "def build_gru_model(input_shape, num_classes=4, num_units=64, dropout_rate=0.2):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(num_units, input_shape=input_shape))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(num_units // 2, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def objective_function(num_units, dropout_rate, epochs, batch_size, X_train, y_train):\n",
    "    model = build_gru_model((X_train.shape[1], X_train.shape[2]), 4, int(num_units), dropout_rate)\n",
    "    history = model.fit(X_train, y_train, epochs=int(epochs), batch_size=int(batch_size), validation_split=0.2, verbose=0)\n",
    "    return np.max(history.history['val_accuracy'])\n",
    "\n",
    "def optimize_hyperparameters(X_train, y_train):\n",
    "    pbounds = {\n",
    "        'num_units': (32, 128),\n",
    "        'dropout_rate': (0.1, 0.5),\n",
    "        'epochs': (10, 30),\n",
    "        'batch_size': (16, 64)\n",
    "    }\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=lambda num_units, dropout_rate, epochs, batch_size: objective_function(\n",
    "            num_units, dropout_rate, epochs, batch_size, X_train, y_train\n",
    "        ),\n",
    "        pbounds=pbounds,\n",
    "        random_state=42,\n",
    "        verbose=2\n",
    "    )\n",
    "    optimizer.maximize(init_points=2, n_iter=3)\n",
    "    return optimizer.max['params']\n",
    "\n",
    "def train_and_evaluate_model(X_train, X_test, y_train, y_test, best_params):\n",
    "    model = build_gru_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        num_classes=4,\n",
    "        num_units=int(best_params['num_units']),\n",
    "        dropout_rate=float(best_params['dropout_rate'])\n",
    "    )\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=int(best_params['epochs']),\n",
    "        batch_size=int(best_params['batch_size']),\n",
    "        validation_split=0.2,\n",
    "        verbose=1\n",
    "    )\n",
    "    y_pred = model.predict(X_test).argmax(axis=1)\n",
    "    print(f\"\\nAccuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    return model\n",
    "\n",
    "def calculate_trust_metrics(shap_top_feats, rule_top_feats):\n",
    "    metrics = {}\n",
    "    precision_list = []\n",
    "    coverage_classes = 0\n",
    "    total_jaccard = 0\n",
    "    for cls, feats in rule_top_feats.items():\n",
    "        rule_feats = set(f for f, _ in feats)\n",
    "        overlap = set(shap_top_feats).intersection(rule_feats)\n",
    "        precision_k = len(overlap) / len(shap_top_feats)\n",
    "        precision_list.append(precision_k)\n",
    "        if len(feats) > 0:\n",
    "            coverage_classes += 1\n",
    "        union_len = len(set(shap_top_feats).union(rule_feats))\n",
    "        intersection_len = len(overlap)\n",
    "        jaccard = intersection_len / union_len if union_len > 0 else 0\n",
    "        total_jaccard += jaccard\n",
    "    metrics[\"Precision@10\"] = np.mean(precision_list)\n",
    "    metrics[\"Jaccard Similarity\"] = total_jaccard / len(rule_top_feats)\n",
    "    metrics[\"Coverage\"] = coverage_classes / len(rule_top_feats)\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def hybrid_xai_analysis(model, X_2d, y, df, target_col, top_k=10):\n",
    "    print(\"\\n===== XAI Analysis (SHAP + Rules) =====\")\n",
    "    print(\"\\nTop SHAP Features:\")\n",
    "    wrapped_model = GRUModelWrapper(model)\n",
    "    explainer = shap.Explainer(wrapped_model, X_2d)\n",
    "    shap_vals = explainer(X_2d)\n",
    "    mean_abs_shap = np.abs(shap_vals.values).mean(axis=0)\n",
    "    shap_top_idx = np.argsort(mean_abs_shap)[-top_k:][::-1]\n",
    "    shap_top_feats = [str(df.drop(columns=[target_col]).columns[i]) for i in shap_top_idx]\n",
    "    print(shap_top_feats)\n",
    "\n",
    "    print(\"\\nTop Rule-Based Features:\")\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y_str = y.astype(str)\n",
    "    df_disc = X.apply(lambda col: pd.qcut(col, q=4, duplicates='drop').astype(str))\n",
    "    df_disc['target'] = y_str\n",
    "    transactions = [[f\"{col}={val}\" for col, val in row.items()] for _, row in df_disc.iterrows()]\n",
    "    te = TransactionEncoder()\n",
    "    df_encoded = pd.DataFrame(te.fit(transactions).transform(transactions), columns=te.columns_)\n",
    "    freq_items = apriori(df_encoded, min_support=0.1, use_colnames=True)\n",
    "    rules = association_rules(freq_items, metric=\"lift\", min_threshold=1.2)\n",
    "    rule_top_feats = {}\n",
    "    for cls in y.unique():\n",
    "        class_rules = rules[rules['consequents'].apply(lambda x: f'target={cls}' in x)]\n",
    "        feat_counts = {}\n",
    "        for _, row in class_rules.iterrows():\n",
    "            for antecedent in row['antecedents']:\n",
    "                if not antecedent.startswith('target='):\n",
    "                    feat = antecedent.split('=')[0]\n",
    "                    feat_counts[feat] = feat_counts.get(feat, 0) + 1\n",
    "        sorted_feats = sorted(feat_counts.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "        rule_top_feats[cls] = sorted_feats\n",
    "        print(f\"\\nClass {cls} Features: {[f for f, _ in sorted_feats]}\")\n",
    "\n",
    "    print(\"\\nOverlap SHAP ↔ Rule-Based Features:\")\n",
    "    for cls, feats in rule_top_feats.items():\n",
    "        rule_feats = {f for f, _ in feats}\n",
    "        overlap = set(shap_top_feats).intersection(rule_feats)\n",
    "        print(f\"Class {cls} → Overlap: {overlap} (count: {len(overlap)})\")\n",
    "\n",
    "    print(\"\\nTrust Metrics Summary:\")\n",
    "    trust_metrics = calculate_trust_metrics(shap_top_feats, rule_top_feats)\n",
    "    for metric, value in trust_metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    train_path = r\"E:\\Abroad period research\\Phenology datasets\\Lasso_Selected_Features_H2\\train_lasso_selected.csv\"\n",
    "    test_path = r\"E:\\Abroad period research\\Phenology datasets\\Lasso_Selected_Features_H2\\test_lasso_selected.csv\"\n",
    "    df_train, target_col = load_and_preprocess_data(train_path)\n",
    "    X_train_2d, X_train_3d, y_train, _ = feature_scaling(df_train, target_col)\n",
    "    df_test, _ = load_and_preprocess_data(test_path)\n",
    "    X_test_2d, X_test_3d, y_test, _ = feature_scaling(df_test, target_col)\n",
    "    print(\"\\nOptimizing Hyperparameters...\")\n",
    "    best_params = optimize_hyperparameters(X_train_3d, y_train)\n",
    "    print(\"\\nTraining Model...\")\n",
    "    model = train_and_evaluate_model(X_train_3d, X_test_3d, y_train, y_test, best_params)\n",
    "    print(\"\\nRunning Hybrid XAI...\")\n",
    "    hybrid_xai_analysis(model, X_train_2d, y_train, df_train, target_col)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3ed058",
   "metadata": {},
   "source": [
    "Keeping olny overlapping features \n",
    "extracting overlapping SHAP + rule-based features and save training and testing files with only these features, while retaining the TIME and target columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13eba6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define input file paths\n",
    "train_path = r\"E:\\Abroad period research\\Phenology datasets\\Lasso_Selected_Features_H2\\train_lasso_selected.csv\"\n",
    "test_path = r\"E:\\Abroad period research\\Phenology datasets\\Lasso_Selected_Features_H2\\test_lasso_selected.csv\"\n",
    "\n",
    "# Define selected hybrid XAI features + TIME + target\n",
    "selected_features = [\n",
    "    'month',\n",
    "    'fenologia_h2_lag2',\n",
    "    'fenologia_h2_lag3',\n",
    "    'fenologia_h2_roll_std_3',\n",
    "    'TIME',\n",
    "    'fenologia_h2'\n",
    "]\n",
    "\n",
    "# Load training and testing data\n",
    "df_train = pd.read_csv(train_path)\n",
    "df_test = pd.read_csv(test_path)\n",
    "\n",
    "# Strip any whitespace or quotes from column names\n",
    "df_train.columns = df_train.columns.str.strip().str.replace('\"', '')\n",
    "df_test.columns = df_test.columns.str.strip().str.replace('\"', '')\n",
    "\n",
    "# Keep only selected features (if any are missing, will raise error)\n",
    "df_train_selected = df_train[selected_features]\n",
    "df_test_selected = df_test[selected_features]\n",
    "\n",
    "# Save new files\n",
    "output_train_path = train_path.replace(\".csv\", \"_hybrid_XAI_selected_features.csv\")\n",
    "output_test_path = test_path.replace(\".csv\", \"_hybrid_XAI_selected_features.csv\")\n",
    "\n",
    "df_train_selected.to_csv(output_train_path, index=False)\n",
    "df_test_selected.to_csv(output_test_path, index=False)\n",
    "\n",
    "print(\"Filtered files saved:\")\n",
    "print(f\"Train → {output_train_path}\")\n",
    "print(f\"Test  → {output_test_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f74f8e4",
   "metadata": {},
   "source": [
    "REtraining model on overlapping features \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b77e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import psutil\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout\n",
    "import tensorflow as tf\n",
    "from bayes_opt import BayesianOptimization\n",
    "from scipy.fftpack import fft\n",
    "import warnings\n",
    "\n",
    "# Set seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================\n",
    "# 1. Data Loading and Preprocessing\n",
    "# ============================================\n",
    "def load_and_preprocess_data(filepath):\n",
    "    df = pd.read_csv(filepath, delimiter=\",\", low_memory=False)\n",
    "    df.columns = df.columns.str.replace('\"', '').str.strip()\n",
    "\n",
    "    if 'TIME' not in df.columns:\n",
    "        raise KeyError(\"The column 'TIME' does not exist in the dataset.\")\n",
    "\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'], errors='coerce')\n",
    "    df.dropna(subset=['TIME'], inplace=True)\n",
    "    df.set_index('TIME', inplace=True)\n",
    "\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "    target_col = 'fenologia_h2'\n",
    "    if target_col not in df.columns:\n",
    "        raise KeyError(f\"The target column '{target_col}' does not exist.\")\n",
    "\n",
    "    df[target_col].interpolate(method='linear', inplace=True)\n",
    "    df[target_col] = df[target_col] - 1\n",
    "\n",
    "    \n",
    "\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "    return df, target_col\n",
    "\n",
    "# ============================================\n",
    "# 2. Feature Scaling\n",
    "# ============================================\n",
    "def feature_scaling(df, target_col):\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))  # 3D for RNN\n",
    "    return X_scaled, y, scaler\n",
    "\n",
    "# ============================================\n",
    "# 3. Build GRU Model\n",
    "# ============================================\n",
    "def build_gru_model(input_shape, num_classes=4, num_units=64, dropout_rate=0.2):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(num_units, input_shape=input_shape))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(num_units // 2, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# ============================================\n",
    "# 4. Bayesian Optimization Objective\n",
    "# ============================================\n",
    "def objective_function(num_units, dropout_rate, epochs, batch_size, X_train, y_train):\n",
    "    model = build_gru_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        num_classes=4,\n",
    "        num_units=int(num_units),\n",
    "        dropout_rate=dropout_rate\n",
    "    )\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=int(epochs),\n",
    "        batch_size=int(batch_size),\n",
    "        validation_split=0.2,\n",
    "        verbose=0\n",
    "    )\n",
    "    return np.max(history.history['val_accuracy'])\n",
    "\n",
    "# ============================================\n",
    "# 5. Optimize Hyperparameters\n",
    "# ============================================\n",
    "def optimize_hyperparameters(X_train, y_train):\n",
    "    pbounds = {\n",
    "        'num_units': (32, 128),\n",
    "        'dropout_rate': (0.1, 0.5),\n",
    "        'epochs': (50, 100),\n",
    "        'batch_size': (16, 64)\n",
    "    }\n",
    "\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=lambda num_units, dropout_rate, epochs, batch_size: objective_function(\n",
    "            num_units, dropout_rate, epochs, batch_size, X_train, y_train\n",
    "        ),\n",
    "        pbounds=pbounds,\n",
    "        random_state=42,\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    optimizer.maximize(init_points=5, n_iter=15)\n",
    "    return optimizer.max['params']\n",
    "\n",
    "# ============================================\n",
    "# 6. Train & Evaluate Model\n",
    "# ============================================\n",
    "def train_and_evaluate_model(X_train, X_test, y_train, y_test, best_params):\n",
    "    model = build_gru_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        num_classes=4,\n",
    "        num_units=int(best_params['num_units']),\n",
    "        dropout_rate=float(best_params['dropout_rate'])\n",
    "    )\n",
    "\n",
    "    start_train = time.time()\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=int(best_params['epochs']),\n",
    "        batch_size=int(best_params['batch_size']),\n",
    "        validation_split=0.2,\n",
    "        verbose=1\n",
    "    )\n",
    "    training_time = time.time() - start_train\n",
    "\n",
    "    start_infer = time.time()\n",
    "    y_pred = model.predict(X_test).argmax(axis=1)\n",
    "    inference_time = (time.time() - start_infer) / len(X_test)\n",
    "\n",
    "    # Calculate total testing time\n",
    "    testing_time = time.time() - start_infer\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(\"\\n===== Evaluation =====\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(\"Report:\\n\", classification_report(y_test, y_pred, digits=4))\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "    model.save(\"temp_model.h5\", include_optimizer=False)\n",
    "    model_size = os.path.getsize(\"temp_model.h5\") / (1024 ** 2)\n",
    "    os.remove(\"temp_model.h5\")\n",
    "\n",
    "    total_params = np.sum([np.prod(v.shape) for v in model.trainable_weights])\n",
    "    ram_usage = psutil.Process(os.getpid()).memory_info().rss / (1024 ** 2)\n",
    "\n",
    "    print(\"\\n===== Model Metrics =====\")\n",
    "    print(f\"Training Time: {training_time:.2f}s\")\n",
    "    print(f\"Inference/sample: {inference_time:.6f}s\")\n",
    "    print(f\"Testing Time: {testing_time:.2f}s\")\n",
    "    print(f\"Model Size: {model_size:.2f} MB\")\n",
    "    print(f\"Trainable Params: {total_params:,}\")\n",
    "    print(f\"RAM Usage: {ram_usage:.2f} MB\")\n",
    "\n",
    "    return model, history\n",
    "\n",
    "# ============================================\n",
    "# 7. Main\n",
    "# ============================================\n",
    "def main():\n",
    "    train_path = r\"E:\\Abroad period research\\Phenology datasets\\Lasso_Selected_Features_H2\\train_lasso_selected_hybrid_XAI_selected_features.csv\"\n",
    "    test_path = r\"E:\\Abroad period research\\Phenology datasets\\Lasso_Selected_Features_H2\\test_lasso_selected_hybrid_XAI_selected_features.csv\"\n",
    "\n",
    "    df_train, target_col = load_and_preprocess_data(train_path)\n",
    "    X_train, y_train, _ = feature_scaling(df_train, target_col)\n",
    "\n",
    "    df_test, _ = load_and_preprocess_data(test_path)\n",
    "    X_test, y_test, _ = feature_scaling(df_test, target_col)\n",
    "\n",
    "    print(\"\\n Optimizing Hyperparameters...\")\n",
    "    best_params = optimize_hyperparameters(X_train, y_train)\n",
    "    print(\"\\n Best Params:\", best_params)\n",
    "\n",
    "    print(\"\\n Training Final GRU Model...\")\n",
    "    model, history = train_and_evaluate_model(X_train, X_test, y_train, y_test, best_params)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05670f8b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "582ca78d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9298196",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import psutil\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout\n",
    "import tensorflow as tf\n",
    "from bayes_opt import BayesianOptimization\n",
    "from scipy.fftpack import fft\n",
    "import warnings\n",
    "\n",
    "# Set seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================\n",
    "# 1. Data Loading and Preprocessing\n",
    "# ============================================\n",
    "def load_and_preprocess_data(filepath):\n",
    "    df = pd.read_csv(filepath, delimiter=\",\", low_memory=False)\n",
    "    df.columns = df.columns.str.replace('\"', '').str.strip()\n",
    "\n",
    "    if 'TIME' not in df.columns:\n",
    "        raise KeyError(\"The column 'TIME' does not exist in the dataset.\")\n",
    "\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'], errors='coerce')\n",
    "    df.dropna(subset=['TIME'], inplace=True)\n",
    "    df.set_index('TIME', inplace=True)\n",
    "\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "    target_col = 'fenologia_h2'\n",
    "    if target_col not in df.columns:\n",
    "        raise KeyError(f\"The target column '{target_col}' does not exist.\")\n",
    "\n",
    "    df[target_col].interpolate(method='linear', inplace=True)\n",
    "    df[target_col] = df[target_col] - 1\n",
    "\n",
    "    \n",
    "\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "    return df, target_col\n",
    "\n",
    "# ============================================\n",
    "# 2. Feature Scaling\n",
    "# ============================================\n",
    "def feature_scaling(df, target_col):\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))  # 3D for RNN\n",
    "    return X_scaled, y, scaler\n",
    "\n",
    "# ============================================\n",
    "# 3. Build GRU Model\n",
    "# ============================================\n",
    "def build_gru_model(input_shape, num_classes=4, num_units=64, dropout_rate=0.2):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(num_units, input_shape=input_shape))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(num_units // 2, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# ============================================\n",
    "# 4. Bayesian Optimization Objective\n",
    "# ============================================\n",
    "def objective_function(num_units, dropout_rate, epochs, batch_size, X_train, y_train):\n",
    "    model = build_gru_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        num_classes=4,\n",
    "        num_units=int(num_units),\n",
    "        dropout_rate=dropout_rate\n",
    "    )\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=int(epochs),\n",
    "        batch_size=int(batch_size),\n",
    "        validation_split=0.2,\n",
    "        verbose=0\n",
    "    )\n",
    "    return np.max(history.history['val_accuracy'])\n",
    "\n",
    "# ============================================\n",
    "# 5. Optimize Hyperparameters\n",
    "# ============================================\n",
    "def optimize_hyperparameters(X_train, y_train):\n",
    "    pbounds = {\n",
    "        'num_units': (32, 128),\n",
    "        'dropout_rate': (0.1, 0.5),\n",
    "        'epochs': (50, 100),\n",
    "        'batch_size': (16, 64)\n",
    "    }\n",
    "\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=lambda num_units, dropout_rate, epochs, batch_size: objective_function(\n",
    "            num_units, dropout_rate, epochs, batch_size, X_train, y_train\n",
    "        ),\n",
    "        pbounds=pbounds,\n",
    "        random_state=42,\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    optimizer.maximize(init_points=5, n_iter=15)\n",
    "    return optimizer.max['params']\n",
    "\n",
    "# ============================================\n",
    "# 6. Train & Evaluate Model\n",
    "# ============================================\n",
    "def train_and_evaluate_model(X_train, X_test, y_train, y_test, best_params):\n",
    "    model = build_gru_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        num_classes=4,\n",
    "        num_units=int(best_params['num_units']),\n",
    "        dropout_rate=float(best_params['dropout_rate'])\n",
    "    )\n",
    "\n",
    "    start_train = time.time()\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=int(best_params['epochs']),\n",
    "        batch_size=int(best_params['batch_size']),\n",
    "        validation_split=0.2,\n",
    "        verbose=1\n",
    "    )\n",
    "    training_time = time.time() - start_train\n",
    "\n",
    "    start_infer = time.time()\n",
    "    y_pred = model.predict(X_test).argmax(axis=1)\n",
    "    inference_time = (time.time() - start_infer) / len(X_test)\n",
    "\n",
    "    # Calculate total testing time\n",
    "    testing_time = time.time() - start_infer\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(\"\\n===== Evaluation =====\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(\"Report:\\n\", classification_report(y_test, y_pred, digits=4))\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "    model.save(\"temp_model.h5\", include_optimizer=False)\n",
    "    model_size = os.path.getsize(\"temp_model.h5\") / (1024 ** 2)\n",
    "    os.remove(\"temp_model.h5\")\n",
    "\n",
    "    total_params = np.sum([np.prod(v.shape) for v in model.trainable_weights])\n",
    "    ram_usage = psutil.Process(os.getpid()).memory_info().rss / (1024 ** 2)\n",
    "\n",
    "    print(\"\\n===== Model Metrics =====\")\n",
    "    print(f\"Training Time: {training_time:.2f}s\")\n",
    "    print(f\"Inference/sample: {inference_time:.6f}s\")\n",
    "    print(f\"Testing Time: {testing_time:.2f}s\")\n",
    "    print(f\"Model Size: {model_size:.2f} MB\")\n",
    "    print(f\"Trainable Params: {total_params:,}\")\n",
    "    print(f\"RAM Usage: {ram_usage:.2f} MB\")\n",
    "\n",
    "    return model, history\n",
    "\n",
    "# ============================================\n",
    "# 7. Main\n",
    "# ============================================\n",
    "def main():\n",
    "    train_path = r\"E:\\Abroad period research\\Phenology datasets\\Lasso_Selected_Features_H2\\train_lasso_selected_hybrid_XAI_selected_features.csv\"\n",
    "    test_path = r\"E:\\Abroad period research\\Phenology datasets\\Lasso_Selected_Features_H2\\test_lasso_selected_hybrid_XAI_selected_features.csv\"\n",
    "\n",
    "    df_train, target_col = load_and_preprocess_data(train_path)\n",
    "    X_train, y_train, _ = feature_scaling(df_train, target_col)\n",
    "\n",
    "    df_test, _ = load_and_preprocess_data(test_path)\n",
    "    X_test, y_test, _ = feature_scaling(df_test, target_col)\n",
    "\n",
    "    print(\"\\n Optimizing Hyperparameters...\")\n",
    "    best_params = optimize_hyperparameters(X_train, y_train)\n",
    "    print(\"\\n Best Params:\", best_params)\n",
    "\n",
    "    print(\"\\n Training Final GRU Model...\")\n",
    "    model, history = train_and_evaluate_model(X_train, X_test, y_train, y_test, best_params)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
