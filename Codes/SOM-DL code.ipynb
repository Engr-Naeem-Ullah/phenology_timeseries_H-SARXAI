{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "febbde4e",
   "metadata": {},
   "source": [
    "Horizon 1  Self-Organizing Maps (SOM) with Deep Learning Enhancements code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a9faa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimizing Hyperparameters...\n",
      "|   iter    |  target   | batch_... | dropou... |  epochs   | learni... |   x_dim   |   y_dim   |\n",
      "-------------------------------------------------------------------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.9228   \u001b[39m | \u001b[39m33.98    \u001b[39m | \u001b[39m0.4803   \u001b[39m | \u001b[39m86.6     \u001b[39m | \u001b[39m0.006187 \u001b[39m | \u001b[39m6.56     \u001b[39m | \u001b[39m6.56     \u001b[39m |\n",
      "| \u001b[35m2        \u001b[39m | \u001b[35m0.9431   \u001b[39m | \u001b[35m18.79    \u001b[39m | \u001b[35m0.4465   \u001b[39m | \u001b[35m80.06    \u001b[39m | \u001b[35m0.007227 \u001b[39m | \u001b[35m5.206    \u001b[39m | \u001b[35m14.7     \u001b[39m |\n",
      "| \u001b[35m3        \u001b[39m | \u001b[35m0.9512   \u001b[39m | \u001b[35m55.96    \u001b[39m | \u001b[35m0.1849   \u001b[39m | \u001b[35m59.09    \u001b[39m | \u001b[35m0.002242 \u001b[39m | \u001b[35m8.042    \u001b[39m | \u001b[35m10.25    \u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m0.9268   \u001b[39m | \u001b[39m36.73    \u001b[39m | \u001b[39m0.2165   \u001b[39m | \u001b[39m80.59    \u001b[39m | \u001b[39m0.001825 \u001b[39m | \u001b[39m7.921    \u001b[39m | \u001b[39m8.664    \u001b[39m |\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m0.9024   \u001b[39m | \u001b[39m37.89    \u001b[39m | \u001b[39m0.4141   \u001b[39m | \u001b[39m59.98    \u001b[39m | \u001b[39m0.005385 \u001b[39m | \u001b[39m10.92    \u001b[39m | \u001b[39m5.465    \u001b[39m |\n",
      "| \u001b[39m6        \u001b[39m | \u001b[39m0.9309   \u001b[39m | \u001b[39m60.92    \u001b[39m | \u001b[39m0.3895   \u001b[39m | \u001b[39m57.24    \u001b[39m | \u001b[39m0.004651 \u001b[39m | \u001b[39m7.884    \u001b[39m | \u001b[39m14.87    \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m0.9187   \u001b[39m | \u001b[39m56.58    \u001b[39m | \u001b[39m0.1641   \u001b[39m | \u001b[39m59.64    \u001b[39m | \u001b[39m0.003724 \u001b[39m | \u001b[39m7.802    \u001b[39m | \u001b[39m10.32    \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m0.9228   \u001b[39m | \u001b[39m26.1     \u001b[39m | \u001b[39m0.497    \u001b[39m | \u001b[39m60.7     \u001b[39m | \u001b[39m0.00716  \u001b[39m | \u001b[39m9.956    \u001b[39m | \u001b[39m7.877    \u001b[39m |\n",
      "| \u001b[35m9        \u001b[39m | \u001b[35m0.9634   \u001b[39m | \u001b[35m53.9     \u001b[39m | \u001b[35m0.3142   \u001b[39m | \u001b[35m94.74    \u001b[39m | \u001b[35m0.004845 \u001b[39m | \u001b[35m9.225    \u001b[39m | \u001b[35m11.54    \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m0.9634   \u001b[39m | \u001b[39m37.02    \u001b[39m | \u001b[39m0.1646   \u001b[39m | \u001b[39m57.01    \u001b[39m | \u001b[39m0.0005144\u001b[39m | \u001b[39m14.98    \u001b[39m | \u001b[39m10.33    \u001b[39m |\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m0.939    \u001b[39m | \u001b[39m50.96    \u001b[39m | \u001b[39m0.2747   \u001b[39m | \u001b[39m71.09    \u001b[39m | \u001b[39m0.000789 \u001b[39m | \u001b[39m5.756    \u001b[39m | \u001b[39m11.66    \u001b[39m |\n",
      "| \u001b[39m12       \u001b[39m | \u001b[39m0.9228   \u001b[39m | \u001b[39m36.1     \u001b[39m | \u001b[39m0.3902   \u001b[39m | \u001b[39m76.05    \u001b[39m | \u001b[39m0.007546 \u001b[39m | \u001b[39m5.782    \u001b[39m | \u001b[39m5.327    \u001b[39m |\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m0.935    \u001b[39m | \u001b[39m51.4     \u001b[39m | \u001b[39m0.4166   \u001b[39m | \u001b[39m78.82    \u001b[39m | \u001b[39m0.00421  \u001b[39m | \u001b[39m10.72    \u001b[39m | \u001b[39m5.745    \u001b[39m |\n",
      "| \u001b[35m14       \u001b[39m | \u001b[35m0.9756   \u001b[39m | \u001b[35m50.66    \u001b[39m | \u001b[35m0.1115   \u001b[39m | \u001b[35m88.68    \u001b[39m | \u001b[35m0.006832 \u001b[39m | \u001b[35m11.9     \u001b[39m | \u001b[35m10.36    \u001b[39m |\n",
      "| \u001b[39m15       \u001b[39m | \u001b[39m0.935    \u001b[39m | \u001b[39m58.03    \u001b[39m | \u001b[39m0.2338   \u001b[39m | \u001b[39m66.8     \u001b[39m | \u001b[39m0.009438 \u001b[39m | \u001b[39m10.48    \u001b[39m | \u001b[39m7.241    \u001b[39m |\n",
      "=================================================================================================\n",
      "\n",
      "Best Parameters Found: {'batch_size': 50.658723485309636, 'dropout_rate': 0.11147511205118993, 'epochs': 88.6814248148147, 'learning_rate': 0.006832182893167596, 'x_dim': 11.899893408887667, 'y_dim': 10.363634423483449}\n",
      "\n",
      "Training Final Model...\n",
      "Epoch 1/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4286 - loss: 1.1895\n",
      "Epoch 2/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9044 - loss: 0.4000 \n",
      "Epoch 3/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9364 - loss: 0.1599 \n",
      "Epoch 4/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9435 - loss: 0.1244 \n",
      "Epoch 5/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9482 - loss: 0.1096 \n",
      "Epoch 6/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9526 - loss: 0.1021\n",
      "Epoch 7/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9562 - loss: 0.1004 \n",
      "Epoch 8/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9520 - loss: 0.0984 \n",
      "Epoch 9/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9485 - loss: 0.1005 \n",
      "Epoch 10/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9514 - loss: 0.1027 \n",
      "Epoch 11/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9548 - loss: 0.0952 \n",
      "Epoch 12/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9487 - loss: 0.0989 \n",
      "Epoch 13/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9491 - loss: 0.0999 \n",
      "Epoch 14/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9502 - loss: 0.1008\n",
      "Epoch 15/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9556 - loss: 0.0951 \n",
      "Epoch 16/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9486 - loss: 0.0947\n",
      "Epoch 17/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9476 - loss: 0.0991 \n",
      "Epoch 18/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9583 - loss: 0.0916 \n",
      "Epoch 19/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9473 - loss: 0.0964 \n",
      "Epoch 20/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9440 - loss: 0.0950\n",
      "Epoch 21/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9456 - loss: 0.0985\n",
      "Epoch 22/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9472 - loss: 0.0939\n",
      "Epoch 23/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9519 - loss: 0.0948\n",
      "Epoch 24/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9548 - loss: 0.0921 \n",
      "Epoch 25/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9516 - loss: 0.0947 \n",
      "Epoch 26/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9474 - loss: 0.0935\n",
      "Epoch 27/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9576 - loss: 0.0919\n",
      "Epoch 28/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9502 - loss: 0.0945\n",
      "Epoch 29/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9536 - loss: 0.0906 \n",
      "Epoch 30/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9446 - loss: 0.0926 \n",
      "Epoch 31/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9491 - loss: 0.0943 \n",
      "Epoch 32/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9471 - loss: 0.0931 \n",
      "Epoch 33/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9511 - loss: 0.0949 \n",
      "Epoch 34/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9577 - loss: 0.0908 \n",
      "Epoch 35/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9558 - loss: 0.0902 \n",
      "Epoch 36/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9533 - loss: 0.0927 \n",
      "Epoch 37/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9578 - loss: 0.0891 \n",
      "Epoch 38/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9468 - loss: 0.0919 \n",
      "Epoch 39/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9485 - loss: 0.0936 \n",
      "Epoch 40/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9530 - loss: 0.0929  \n",
      "Epoch 41/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9483 - loss: 0.0919 \n",
      "Epoch 42/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9493 - loss: 0.0914 \n",
      "Epoch 43/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9567 - loss: 0.0889 \n",
      "Epoch 44/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9556 - loss: 0.0901 \n",
      "Epoch 45/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9490 - loss: 0.0920\n",
      "Epoch 46/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9519 - loss: 0.0927 \n",
      "Epoch 47/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9544 - loss: 0.0905 \n",
      "Epoch 48/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9583 - loss: 0.0903 \n",
      "Epoch 49/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9508 - loss: 0.0976 \n",
      "Epoch 50/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9507 - loss: 0.0898 \n",
      "Epoch 51/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9529 - loss: 0.0897 \n",
      "Epoch 52/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9476 - loss: 0.0919\n",
      "Epoch 53/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9565 - loss: 0.0921 \n",
      "Epoch 54/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9532 - loss: 0.0915 \n",
      "Epoch 55/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9505 - loss: 0.0932 \n",
      "Epoch 56/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9458 - loss: 0.0921 \n",
      "Epoch 57/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9500 - loss: 0.0932 \n",
      "Epoch 58/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9556 - loss: 0.0906 \n",
      "Epoch 59/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9494 - loss: 0.0919 \n",
      "Epoch 60/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9515 - loss: 0.0925 \n",
      "Epoch 61/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9475 - loss: 0.0921 \n",
      "Epoch 62/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9532 - loss: 0.0916\n",
      "Epoch 63/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9585 - loss: 0.0900 \n",
      "Epoch 64/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9511 - loss: 0.0928 \n",
      "Epoch 65/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9572 - loss: 0.0891 \n",
      "Epoch 66/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9511 - loss: 0.0906 \n",
      "Epoch 67/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9528 - loss: 0.0903 \n",
      "Epoch 68/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9480 - loss: 0.0917 \n",
      "Epoch 69/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9514 - loss: 0.0905\n",
      "Epoch 70/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9467 - loss: 0.0941 \n",
      "Epoch 71/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9521 - loss: 0.0897 \n",
      "Epoch 72/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9562 - loss: 0.0905 \n",
      "Epoch 73/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9472 - loss: 0.0904 \n",
      "Epoch 74/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9523 - loss: 0.0921 \n",
      "Epoch 75/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9533 - loss: 0.0922 \n",
      "Epoch 76/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9546 - loss: 0.0922 \n",
      "Epoch 77/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9577 - loss: 0.0895 \n",
      "Epoch 78/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9494 - loss: 0.0898 \n",
      "Epoch 79/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9495 - loss: 0.0896 \n",
      "Epoch 80/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9524 - loss: 0.0907 \n",
      "Epoch 81/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9509 - loss: 0.0910 \n",
      "Epoch 82/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9500 - loss: 0.0915 \n",
      "Epoch 83/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9549 - loss: 0.0901 \n",
      "Epoch 84/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9524 - loss: 0.0914\n",
      "Epoch 85/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9509 - loss: 0.0905 \n",
      "Epoch 86/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9511 - loss: 0.0934 \n",
      "Epoch 87/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9519 - loss: 0.0911 \n",
      "Epoch 88/88\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9521 - loss: 0.0928 \n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Evaluation =====\n",
      "Accuracy: 0.9656\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9020    0.8214    0.8598        56\n",
      "           1     0.9545    0.9625    0.9585       240\n",
      "           2     0.9917    0.9815    0.9866       487\n",
      "           3     0.7895    1.0000    0.8824        30\n",
      "\n",
      "    accuracy                         0.9656       813\n",
      "   macro avg     0.9094    0.9414    0.9218       813\n",
      "weighted avg     0.9671    0.9656    0.9657       813\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 46  10   0   0]\n",
      " [  5 231   4   0]\n",
      " [  0   1 478   8]\n",
      " [  0   0   0  30]]\n",
      "\n",
      "===== Model Metrics =====\n",
      "Training Time: 9.09s\n",
      "Inference/sample: 0.000266s\n",
      "Testing Time: 0.22s\n",
      "Model Size: 0.06 MB\n",
      "Trainable Params: 9,316\n",
      "RAM Usage: 645.81 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import psutil\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from minisom import MiniSom\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from bayes_opt import BayesianOptimization\n",
    "from scipy.fftpack import fft\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "def load_and_preprocess_data(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df.columns = df.columns.str.replace('\"', '').str.strip()\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'], errors='coerce')\n",
    "    df.dropna(subset=['TIME'], inplace=True)\n",
    "    df.set_index('TIME', inplace=True)\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "    target_col = 'fenologia_h1'\n",
    "    df[target_col].interpolate(method='linear', inplace=True)\n",
    "    df[target_col] = df[target_col] - 1\n",
    "\n",
    "    for i in range(1, 4):\n",
    "        df[f'{target_col}_lag{i}'] = df[target_col].shift(i)\n",
    "    for window in [3, 6]:\n",
    "        df[f'{target_col}_roll_mean_{window}'] = df[target_col].rolling(window).mean()\n",
    "        df[f'{target_col}_roll_std_{window}'] = df[target_col].rolling(window).std()\n",
    "\n",
    "    df['month'] = df.index.month\n",
    "    df['weekofyear'] = df.index.isocalendar().week\n",
    "    df['year'] = df.index.isocalendar().year\n",
    "    df['sin_week'] = np.sin(2 * np.pi * df['weekofyear'] / 52)\n",
    "    df['cos_week'] = np.cos(2 * np.pi * df['weekofyear'] / 52)\n",
    "    df['EMA_3'] = df[target_col].ewm(span=3, adjust=False).mean()\n",
    "    df['EMA_6'] = df[target_col].ewm(span=6, adjust=False).mean()\n",
    "    df['correlation_target_month'] = df[target_col].rolling(6).corr(df['month'])\n",
    "    df['correlation_target_week'] = df[target_col].rolling(6).corr(df['weekofyear'])\n",
    "\n",
    "    fft_vals = fft(df[target_col].dropna().values)\n",
    "    fft_real = np.real(fft_vals)[:len(df[target_col])]\n",
    "    fft_imag = np.imag(fft_vals)[:len(df[target_col])]\n",
    "    df['fft_real'] = np.pad(fft_real, (0, len(df) - len(fft_real)), constant_values=np.nan)\n",
    "    df['fft_imag'] = np.pad(fft_imag, (0, len(df) - len(fft_imag)), constant_values=np.nan)\n",
    "\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "    return df, target_col\n",
    "\n",
    "def feature_scaling(df, target_col):\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    return X_scaled, y, scaler\n",
    "\n",
    "def transform_with_som(som, X, y_dim, x_dim):\n",
    "    winners = np.array([som.winner(x) for x in X])\n",
    "    indices = winners[:, 0] * y_dim + winners[:, 1]\n",
    "    return to_categorical(indices, num_classes=x_dim * y_dim)\n",
    "\n",
    "def train_and_evaluate_som_dnn(X_train, X_test, y_train, y_test, x_dim, y_dim, epochs, batch_size, dropout_rate, learning_rate):\n",
    "    som = MiniSom(x_dim, y_dim, X_train.shape[1], sigma=1.0, learning_rate=learning_rate)\n",
    "    som.random_weights_init(X_train)\n",
    "    som.train_random(X_train, 100)\n",
    "\n",
    "    X_train_enc = transform_with_som(som, X_train, y_dim, x_dim)\n",
    "    X_test_enc = transform_with_som(som, X_test, y_dim, x_dim)\n",
    "    y_train_cat = to_categorical(y_train)\n",
    "    y_test_cat = to_categorical(y_test)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_dim=X_train_enc.shape[1]))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(y_train_cat.shape[1], activation='softmax'))\n",
    "    model.compile(optimizer=Adam(learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    start_train = time.time()\n",
    "    model.fit(X_train_enc, y_train_cat, epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "    training_time = time.time() - start_train\n",
    "\n",
    "    start_infer = time.time()\n",
    "    y_pred = model.predict(X_test_enc).argmax(axis=1)\n",
    "    inference_time = (time.time() - start_infer) / len(X_test_enc)\n",
    "    testing_time = time.time() - start_infer\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(\"\\n===== Evaluation =====\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(\"Report:\\n\", classification_report(y_test, y_pred, digits=4))\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "    model.save(\"temp_model_som_dnn.h5\", include_optimizer=False)\n",
    "    model_size = os.path.getsize(\"temp_model_som_dnn.h5\") / (1024 ** 2)\n",
    "    os.remove(\"temp_model_som_dnn.h5\")\n",
    "\n",
    "    total_params = np.sum([np.prod(v.shape) for v in model.trainable_weights])\n",
    "    ram_usage = psutil.Process(os.getpid()).memory_info().rss / (1024 ** 2)\n",
    "\n",
    "    print(\"\\n===== Model Metrics =====\")\n",
    "    print(f\"Training Time: {training_time:.2f}s\")\n",
    "    print(f\"Inference/sample: {inference_time:.6f}s\")\n",
    "    print(f\"Testing Time: {testing_time:.2f}s\")\n",
    "    print(f\"Model Size: {model_size:.2f} MB\")\n",
    "    print(f\"Trainable Params: {total_params:,}\")\n",
    "    print(f\"RAM Usage: {ram_usage:.2f} MB\")\n",
    "\n",
    "def objective_function(epochs, batch_size, x_dim, y_dim, dropout_rate, learning_rate, X_train, y_train, X_val, y_val):\n",
    "    x_dim, y_dim = int(x_dim), int(y_dim)\n",
    "    batch_size, epochs = int(batch_size), int(epochs)\n",
    "    dropout_rate = float(dropout_rate)\n",
    "    learning_rate = float(learning_rate)\n",
    "\n",
    "    som = MiniSom(x_dim, y_dim, X_train.shape[1], sigma=1.0, learning_rate=learning_rate)\n",
    "    som.random_weights_init(X_train)\n",
    "    som.train_random(X_train, 100)\n",
    "\n",
    "    X_train_enc = transform_with_som(som, X_train, y_dim, x_dim)\n",
    "    X_val_enc = transform_with_som(som, X_val, y_dim, x_dim)\n",
    "    y_train_cat = to_categorical(y_train)\n",
    "    y_val_cat = to_categorical(y_val)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_dim=X_train_enc.shape[1]))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(y_train_cat.shape[1], activation='softmax'))\n",
    "    model.compile(optimizer=Adam(learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(X_train_enc, y_train_cat, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    _, acc = model.evaluate(X_val_enc, y_val_cat, verbose=0)\n",
    "    return acc\n",
    "\n",
    "def optimize_som_dnn_hyperparameters(X, y):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    pbounds = {\n",
    "        'epochs': (50, 100),\n",
    "        'batch_size': (16, 64),\n",
    "        'x_dim': (5, 15),\n",
    "        'y_dim': (5, 15),\n",
    "        'dropout_rate': (0.1, 0.5),\n",
    "        'learning_rate': (0.0005, 0.01)\n",
    "    }\n",
    "\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=lambda epochs, batch_size, x_dim, y_dim, dropout_rate, learning_rate:\n",
    "            objective_function(epochs, batch_size, x_dim, y_dim, dropout_rate, learning_rate,\n",
    "                               X_train, y_train, X_val, y_val),\n",
    "        pbounds=pbounds,\n",
    "        random_state=42,\n",
    "        verbose=2\n",
    "    )\n",
    "    optimizer.maximize(init_points=5, n_iter=10)\n",
    "    return optimizer.max['params']\n",
    "\n",
    "def main():\n",
    "    train_path =  r\"E:\\Abroad period research\\Phenology datasets\\PHENOLOGY_H1\\Cadiz_train.csv\"\n",
    "    test_path =  r\"E:\\Abroad period research\\Phenology datasets\\PHENOLOGY_H1\\Cadiz_test.csv\"\n",
    "\n",
    "    df_train, target_col = load_and_preprocess_data(train_path)\n",
    "    X_train, y_train, _ = feature_scaling(df_train, target_col)\n",
    "\n",
    "    df_test, _ = load_and_preprocess_data(test_path)\n",
    "    X_test, y_test, _ = feature_scaling(df_test, target_col)\n",
    "\n",
    "    print(\"\\nOptimizing Hyperparameters...\")\n",
    "    best_params = optimize_som_dnn_hyperparameters(X_train, y_train)\n",
    "    print(\"\\nBest Parameters Found:\", best_params)\n",
    "\n",
    "    print(\"\\nTraining Final Model...\")\n",
    "    train_and_evaluate_som_dnn(\n",
    "        X_train, X_test, y_train, y_test,\n",
    "        x_dim=int(best_params['x_dim']),\n",
    "        y_dim=int(best_params['y_dim']),\n",
    "        epochs=int(best_params['epochs']),\n",
    "        batch_size=int(best_params['batch_size']),\n",
    "        dropout_rate=float(best_params['dropout_rate']),\n",
    "        learning_rate=float(best_params['learning_rate'])\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b241d47",
   "metadata": {},
   "source": [
    "Horizon 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dca8b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimizing Hyperparameters...\n",
      "|   iter    |  target   | batch_... | dropou... |  epochs   | learni... |   x_dim   |   y_dim   |\n",
      "-------------------------------------------------------------------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.9512   \u001b[39m | \u001b[39m33.98    \u001b[39m | \u001b[39m0.4803   \u001b[39m | \u001b[39m86.6     \u001b[39m | \u001b[39m0.006187 \u001b[39m | \u001b[39m6.56     \u001b[39m | \u001b[39m6.56     \u001b[39m |\n",
      "| \u001b[35m2        \u001b[39m | \u001b[35m0.9675   \u001b[39m | \u001b[35m18.79    \u001b[39m | \u001b[35m0.4465   \u001b[39m | \u001b[35m80.06    \u001b[39m | \u001b[35m0.007227 \u001b[39m | \u001b[35m5.206    \u001b[39m | \u001b[35m14.7     \u001b[39m |\n",
      "| \u001b[35m3        \u001b[39m | \u001b[35m0.9756   \u001b[39m | \u001b[35m55.96    \u001b[39m | \u001b[35m0.1849   \u001b[39m | \u001b[35m59.09    \u001b[39m | \u001b[35m0.002242 \u001b[39m | \u001b[35m8.042    \u001b[39m | \u001b[35m10.25    \u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m0.9593   \u001b[39m | \u001b[39m36.73    \u001b[39m | \u001b[39m0.2165   \u001b[39m | \u001b[39m80.59    \u001b[39m | \u001b[39m0.001825 \u001b[39m | \u001b[39m7.921    \u001b[39m | \u001b[39m8.664    \u001b[39m |\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m0.9756   \u001b[39m | \u001b[39m37.89    \u001b[39m | \u001b[39m0.4141   \u001b[39m | \u001b[39m59.98    \u001b[39m | \u001b[39m0.005385 \u001b[39m | \u001b[39m10.92    \u001b[39m | \u001b[39m5.465    \u001b[39m |\n",
      "| \u001b[39m6        \u001b[39m | \u001b[39m0.9756   \u001b[39m | \u001b[39m47.44    \u001b[39m | \u001b[39m0.2133   \u001b[39m | \u001b[39m50.01    \u001b[39m | \u001b[39m0.001706 \u001b[39m | \u001b[39m14.34    \u001b[39m | \u001b[39m13.67    \u001b[39m |\n",
      "| \u001b[35m7        \u001b[39m | \u001b[35m0.9837   \u001b[39m | \u001b[35m20.77    \u001b[39m | \u001b[35m0.2695   \u001b[39m | \u001b[35m50.02    \u001b[39m | \u001b[35m0.005759 \u001b[39m | \u001b[35m5.495    \u001b[39m | \u001b[35m14.31    \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m0.9797   \u001b[39m | \u001b[39m16.54    \u001b[39m | \u001b[39m0.4714   \u001b[39m | \u001b[39m50.34    \u001b[39m | \u001b[39m0.003031 \u001b[39m | \u001b[39m14.02    \u001b[39m | \u001b[39m6.376    \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m0.9756   \u001b[39m | \u001b[39m16.22    \u001b[39m | \u001b[39m0.21     \u001b[39m | \u001b[39m60.5     \u001b[39m | \u001b[39m0.004862 \u001b[39m | \u001b[39m5.037    \u001b[39m | \u001b[39m14.24    \u001b[39m |\n",
      "| \u001b[35m10       \u001b[39m | \u001b[35m0.9959   \u001b[39m | \u001b[35m30.27    \u001b[39m | \u001b[35m0.3373   \u001b[39m | \u001b[35m50.73    \u001b[39m | \u001b[35m0.005529 \u001b[39m | \u001b[35m14.46    \u001b[39m | \u001b[35m14.42    \u001b[39m |\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m0.9715   \u001b[39m | \u001b[39m27.32    \u001b[39m | \u001b[39m0.2362   \u001b[39m | \u001b[39m58.7     \u001b[39m | \u001b[39m0.001565 \u001b[39m | \u001b[39m14.5     \u001b[39m | \u001b[39m14.79    \u001b[39m |\n",
      "| \u001b[39m12       \u001b[39m | \u001b[39m0.9675   \u001b[39m | \u001b[39m32.96    \u001b[39m | \u001b[39m0.1826   \u001b[39m | \u001b[39m50.23    \u001b[39m | \u001b[39m0.001535 \u001b[39m | \u001b[39m6.784    \u001b[39m | \u001b[39m12.58    \u001b[39m |\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m0.9959   \u001b[39m | \u001b[39m26.53    \u001b[39m | \u001b[39m0.3023   \u001b[39m | \u001b[39m50.66    \u001b[39m | \u001b[39m0.007449 \u001b[39m | \u001b[39m14.71    \u001b[39m | \u001b[39m14.0     \u001b[39m |\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m0.9715   \u001b[39m | \u001b[39m28.91    \u001b[39m | \u001b[39m0.4177   \u001b[39m | \u001b[39m50.63    \u001b[39m | \u001b[39m0.005897 \u001b[39m | \u001b[39m13.66    \u001b[39m | \u001b[39m8.613    \u001b[39m |\n",
      "| \u001b[39m15       \u001b[39m | \u001b[39m0.9797   \u001b[39m | \u001b[39m21.47    \u001b[39m | \u001b[39m0.298    \u001b[39m | \u001b[39m51.85    \u001b[39m | \u001b[39m0.007915 \u001b[39m | \u001b[39m13.62    \u001b[39m | \u001b[39m13.9     \u001b[39m |\n",
      "=================================================================================================\n",
      "\n",
      "Best Parameters Found: {'batch_size': 30.271304722406995, 'dropout_rate': 0.3372752442136107, 'epochs': 50.73485194368483, 'learning_rate': 0.005528934339778727, 'x_dim': 14.457680895800676, 'y_dim': 14.41565301649433}\n",
      "\n",
      "Training Final Model...\n",
      "Epoch 1/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.4232 - loss: 1.2032\n",
      "Epoch 2/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8956 - loss: 0.4510 \n",
      "Epoch 3/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9709 - loss: 0.0984\n",
      "Epoch 4/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9856 - loss: 0.0537\n",
      "Epoch 5/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9892 - loss: 0.0469\n",
      "Epoch 6/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9896 - loss: 0.0378\n",
      "Epoch 7/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9902 - loss: 0.0273\n",
      "Epoch 8/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9899 - loss: 0.0297\n",
      "Epoch 9/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9862 - loss: 0.0380\n",
      "Epoch 10/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9912 - loss: 0.0280\n",
      "Epoch 11/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9937 - loss: 0.0251\n",
      "Epoch 12/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9918 - loss: 0.0299\n",
      "Epoch 13/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9935 - loss: 0.0200\n",
      "Epoch 14/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9923 - loss: 0.0295\n",
      "Epoch 15/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9929 - loss: 0.0254\n",
      "Epoch 16/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9940 - loss: 0.0201\n",
      "Epoch 17/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9917 - loss: 0.0268 \n",
      "Epoch 18/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9907 - loss: 0.0265\n",
      "Epoch 19/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 886us/step - accuracy: 0.9940 - loss: 0.0220\n",
      "Epoch 20/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9911 - loss: 0.0273\n",
      "Epoch 21/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9912 - loss: 0.0200\n",
      "Epoch 22/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9929 - loss: 0.0295\n",
      "Epoch 23/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9934 - loss: 0.0243\n",
      "Epoch 24/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9918 - loss: 0.0285\n",
      "Epoch 25/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9903 - loss: 0.0234\n",
      "Epoch 26/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9930 - loss: 0.0241\n",
      "Epoch 27/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9929 - loss: 0.0256 \n",
      "Epoch 28/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9924 - loss: 0.0258 \n",
      "Epoch 29/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9940 - loss: 0.0201\n",
      "Epoch 30/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9930 - loss: 0.0246\n",
      "Epoch 31/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9936 - loss: 0.0220\n",
      "Epoch 32/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9935 - loss: 0.0222\n",
      "Epoch 33/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9926 - loss: 0.0224\n",
      "Epoch 34/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9916 - loss: 0.0246\n",
      "Epoch 35/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9869 - loss: 0.0263\n",
      "Epoch 36/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9883 - loss: 0.0265\n",
      "Epoch 37/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9935 - loss: 0.0221\n",
      "Epoch 38/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9935 - loss: 0.0215\n",
      "Epoch 39/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9940 - loss: 0.0240\n",
      "Epoch 40/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9928 - loss: 0.0218\n",
      "Epoch 41/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9928 - loss: 0.0215\n",
      "Epoch 42/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9935 - loss: 0.0223 \n",
      "Epoch 43/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9915 - loss: 0.0225\n",
      "Epoch 44/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9942 - loss: 0.0213\n",
      "Epoch 45/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9942 - loss: 0.0212\n",
      "Epoch 46/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9935 - loss: 0.0208\n",
      "Epoch 47/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9935 - loss: 0.0225\n",
      "Epoch 48/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9935 - loss: 0.0226\n",
      "Epoch 49/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9930 - loss: 0.0238\n",
      "Epoch 50/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9935 - loss: 0.0232\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Evaluation =====\n",
      "Accuracy: 0.7196\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9302    0.9524    0.9412        42\n",
      "           1     0.8058    0.8628    0.8333       226\n",
      "           2     0.9691    0.6169    0.7539       509\n",
      "           3     0.1765    1.0000    0.3000        36\n",
      "\n",
      "    accuracy                         0.7196       813\n",
      "   macro avg     0.7204    0.8580    0.7071       813\n",
      "weighted avg     0.8866    0.7196    0.7656       813\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 40   2   0   0]\n",
      " [  3 195  10  18]\n",
      " [  0  45 314 150]\n",
      " [  0   0   0  36]]\n",
      "\n",
      "===== Model Metrics =====\n",
      "Training Time: 6.47s\n",
      "Inference/sample: 0.000365s\n",
      "Testing Time: 0.30s\n",
      "Model Size: 0.08 MB\n",
      "Trainable Params: 14,820\n",
      "RAM Usage: 1198.39 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import psutil\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from minisom import MiniSom\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from bayes_opt import BayesianOptimization\n",
    "from scipy.fftpack import fft\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "def load_and_preprocess_data(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df.columns = df.columns.str.replace('\"', '').str.strip()\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'], errors='coerce')\n",
    "    df.dropna(subset=['TIME'], inplace=True)\n",
    "    df.set_index('TIME', inplace=True)\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "    target_col = 'fenologia_h2'\n",
    "    df[target_col].interpolate(method='linear', inplace=True)\n",
    "    df[target_col] = df[target_col] - 1\n",
    "\n",
    "\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "    return df, target_col\n",
    "\n",
    "def feature_scaling(df, target_col):\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    return X_scaled, y, scaler\n",
    "\n",
    "def transform_with_som(som, X, y_dim, x_dim):\n",
    "    winners = np.array([som.winner(x) for x in X])\n",
    "    indices = winners[:, 0] * y_dim + winners[:, 1]\n",
    "    return to_categorical(indices, num_classes=x_dim * y_dim)\n",
    "\n",
    "def train_and_evaluate_som_dnn(X_train, X_test, y_train, y_test, x_dim, y_dim, epochs, batch_size, dropout_rate, learning_rate):\n",
    "    som = MiniSom(x_dim, y_dim, X_train.shape[1], sigma=1.0, learning_rate=learning_rate)\n",
    "    som.random_weights_init(X_train)\n",
    "    som.train_random(X_train, 100)\n",
    "\n",
    "    X_train_enc = transform_with_som(som, X_train, y_dim, x_dim)\n",
    "    X_test_enc = transform_with_som(som, X_test, y_dim, x_dim)\n",
    "    y_train_cat = to_categorical(y_train)\n",
    "    y_test_cat = to_categorical(y_test)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_dim=X_train_enc.shape[1]))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(y_train_cat.shape[1], activation='softmax'))\n",
    "    model.compile(optimizer=Adam(learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    start_train = time.time()\n",
    "    model.fit(X_train_enc, y_train_cat, epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "    training_time = time.time() - start_train\n",
    "\n",
    "    start_infer = time.time()\n",
    "    y_pred = model.predict(X_test_enc).argmax(axis=1)\n",
    "    inference_time = (time.time() - start_infer) / len(X_test_enc)\n",
    "    testing_time = time.time() - start_infer\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(\"\\n===== Evaluation =====\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(\"Report:\\n\", classification_report(y_test, y_pred, digits=4))\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "    model.save(\"temp_model_som_dnn.h5\", include_optimizer=False)\n",
    "    model_size = os.path.getsize(\"temp_model_som_dnn.h5\") / (1024 ** 2)\n",
    "    os.remove(\"temp_model_som_dnn.h5\")\n",
    "\n",
    "    total_params = np.sum([np.prod(v.shape) for v in model.trainable_weights])\n",
    "    ram_usage = psutil.Process(os.getpid()).memory_info().rss / (1024 ** 2)\n",
    "\n",
    "    print(\"\\n===== Model Metrics =====\")\n",
    "    print(f\"Training Time: {training_time:.2f}s\")\n",
    "    print(f\"Inference/sample: {inference_time:.6f}s\")\n",
    "    print(f\"Testing Time: {testing_time:.2f}s\")\n",
    "    print(f\"Model Size: {model_size:.2f} MB\")\n",
    "    print(f\"Trainable Params: {total_params:,}\")\n",
    "    print(f\"RAM Usage: {ram_usage:.2f} MB\")\n",
    "\n",
    "def objective_function(epochs, batch_size, x_dim, y_dim, dropout_rate, learning_rate, X_train, y_train, X_val, y_val):\n",
    "    x_dim, y_dim = int(x_dim), int(y_dim)\n",
    "    batch_size, epochs = int(batch_size), int(epochs)\n",
    "    dropout_rate = float(dropout_rate)\n",
    "    learning_rate = float(learning_rate)\n",
    "\n",
    "    som = MiniSom(x_dim, y_dim, X_train.shape[1], sigma=1.0, learning_rate=learning_rate)\n",
    "    som.random_weights_init(X_train)\n",
    "    som.train_random(X_train, 100)\n",
    "\n",
    "    X_train_enc = transform_with_som(som, X_train, y_dim, x_dim)\n",
    "    X_val_enc = transform_with_som(som, X_val, y_dim, x_dim)\n",
    "    y_train_cat = to_categorical(y_train)\n",
    "    y_val_cat = to_categorical(y_val)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_dim=X_train_enc.shape[1]))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(y_train_cat.shape[1], activation='softmax'))\n",
    "    model.compile(optimizer=Adam(learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(X_train_enc, y_train_cat, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    _, acc = model.evaluate(X_val_enc, y_val_cat, verbose=0)\n",
    "    return acc\n",
    "\n",
    "def optimize_som_dnn_hyperparameters(X, y):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    pbounds = {\n",
    "        'epochs': (50, 100),\n",
    "        'batch_size': (16, 64),\n",
    "        'x_dim': (5, 15),\n",
    "        'y_dim': (5, 15),\n",
    "        'dropout_rate': (0.1, 0.5),\n",
    "        'learning_rate': (0.0005, 0.01)\n",
    "    }\n",
    "\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=lambda epochs, batch_size, x_dim, y_dim, dropout_rate, learning_rate:\n",
    "            objective_function(epochs, batch_size, x_dim, y_dim, dropout_rate, learning_rate,\n",
    "                               X_train, y_train, X_val, y_val),\n",
    "        pbounds=pbounds,\n",
    "        random_state=42,\n",
    "        verbose=2\n",
    "    )\n",
    "    optimizer.maximize(init_points=5, n_iter=10)\n",
    "    return optimizer.max['params']\n",
    "\n",
    "def main():\n",
    "    train_path = r\"E:\\Abroad period research\\Phenology datasets\\Lasso_Selected_Features_H2\\train_lasso_selected.csv\"\n",
    "    test_path = r\"E:\\Abroad period research\\Phenology datasets\\Lasso_Selected_Features_H2\\test_lasso_selected.csv\"\n",
    "\n",
    "    df_train, target_col = load_and_preprocess_data(train_path)\n",
    "    X_train, y_train, _ = feature_scaling(df_train, target_col)\n",
    "\n",
    "    df_test, _ = load_and_preprocess_data(test_path)\n",
    "    X_test, y_test, _ = feature_scaling(df_test, target_col)\n",
    "\n",
    "    print(\"\\nOptimizing Hyperparameters...\")\n",
    "    best_params = optimize_som_dnn_hyperparameters(X_train, y_train)\n",
    "    print(\"\\nBest Parameters Found:\", best_params)\n",
    "\n",
    "    print(\"\\nTraining Final Model...\")\n",
    "    train_and_evaluate_som_dnn(\n",
    "        X_train, X_test, y_train, y_test,\n",
    "        x_dim=int(best_params['x_dim']),\n",
    "        y_dim=int(best_params['y_dim']),\n",
    "        epochs=int(best_params['epochs']),\n",
    "        batch_size=int(best_params['batch_size']),\n",
    "        dropout_rate=float(best_params['dropout_rate']),\n",
    "        learning_rate=float(best_params['learning_rate'])\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bf5863",
   "metadata": {},
   "source": [
    "Horizon 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72344fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimizing Hyperparameters...\n",
      "|   iter    |  target   | batch_... | dropou... |  epochs   | learni... |   x_dim   |   y_dim   |\n",
      "-------------------------------------------------------------------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.9472   \u001b[39m | \u001b[39m33.98    \u001b[39m | \u001b[39m0.4803   \u001b[39m | \u001b[39m86.6     \u001b[39m | \u001b[39m0.006187 \u001b[39m | \u001b[39m6.56     \u001b[39m | \u001b[39m6.56     \u001b[39m |\n",
      "| \u001b[39m2        \u001b[39m | \u001b[39m0.939    \u001b[39m | \u001b[39m18.79    \u001b[39m | \u001b[39m0.4465   \u001b[39m | \u001b[39m80.06    \u001b[39m | \u001b[39m0.007227 \u001b[39m | \u001b[39m5.206    \u001b[39m | \u001b[39m14.7     \u001b[39m |\n",
      "| \u001b[35m3        \u001b[39m | \u001b[35m0.9675   \u001b[39m | \u001b[35m55.96    \u001b[39m | \u001b[35m0.1849   \u001b[39m | \u001b[35m59.09    \u001b[39m | \u001b[35m0.002242 \u001b[39m | \u001b[35m8.042    \u001b[39m | \u001b[35m10.25    \u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m0.9512   \u001b[39m | \u001b[39m36.73    \u001b[39m | \u001b[39m0.2165   \u001b[39m | \u001b[39m80.59    \u001b[39m | \u001b[39m0.001825 \u001b[39m | \u001b[39m7.921    \u001b[39m | \u001b[39m8.664    \u001b[39m |\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m0.9553   \u001b[39m | \u001b[39m37.89    \u001b[39m | \u001b[39m0.4141   \u001b[39m | \u001b[39m59.98    \u001b[39m | \u001b[39m0.005385 \u001b[39m | \u001b[39m10.92    \u001b[39m | \u001b[39m5.465    \u001b[39m |\n",
      "| \u001b[35m6        \u001b[39m | \u001b[35m0.9715   \u001b[39m | \u001b[35m63.77    \u001b[39m | \u001b[35m0.4078   \u001b[39m | \u001b[35m78.38    \u001b[39m | \u001b[35m0.007585 \u001b[39m | \u001b[35m14.16    \u001b[39m | \u001b[35m14.63    \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m0.9634   \u001b[39m | \u001b[39m56.03    \u001b[39m | \u001b[39m0.2306   \u001b[39m | \u001b[39m58.46    \u001b[39m | \u001b[39m0.006116 \u001b[39m | \u001b[39m7.844    \u001b[39m | \u001b[39m11.52    \u001b[39m |\n",
      "| \u001b[35m8        \u001b[39m | \u001b[35m0.9878   \u001b[39m | \u001b[35m57.84    \u001b[39m | \u001b[35m0.272    \u001b[39m | \u001b[35m66.33    \u001b[39m | \u001b[35m0.0006901\u001b[39m | \u001b[35m10.63    \u001b[39m | \u001b[35m6.805    \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m0.8699   \u001b[39m | \u001b[39m61.62    \u001b[39m | \u001b[39m0.3397   \u001b[39m | \u001b[39m73.62    \u001b[39m | \u001b[39m0.00501  \u001b[39m | \u001b[39m5.785    \u001b[39m | \u001b[39m5.555    \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m0.9715   \u001b[39m | \u001b[39m55.02    \u001b[39m | \u001b[39m0.471    \u001b[39m | \u001b[39m63.0     \u001b[39m | \u001b[39m0.001332 \u001b[39m | \u001b[39m12.38    \u001b[39m | \u001b[39m6.115    \u001b[39m |\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m0.9756   \u001b[39m | \u001b[39m59.33    \u001b[39m | \u001b[39m0.1707   \u001b[39m | \u001b[39m64.85    \u001b[39m | \u001b[39m0.003359 \u001b[39m | \u001b[39m13.49    \u001b[39m | \u001b[39m10.77    \u001b[39m |\n",
      "| \u001b[39m12       \u001b[39m | \u001b[39m0.9146   \u001b[39m | \u001b[39m61.66    \u001b[39m | \u001b[39m0.4671   \u001b[39m | \u001b[39m61.54    \u001b[39m | \u001b[39m0.009465 \u001b[39m | \u001b[39m9.904    \u001b[39m | \u001b[39m5.841    \u001b[39m |\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m0.9675   \u001b[39m | \u001b[39m55.19    \u001b[39m | \u001b[39m0.3768   \u001b[39m | \u001b[39m65.5     \u001b[39m | \u001b[39m0.005976 \u001b[39m | \u001b[39m9.02     \u001b[39m | \u001b[39m10.45    \u001b[39m |\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m0.9715   \u001b[39m | \u001b[39m55.47    \u001b[39m | \u001b[39m0.3194   \u001b[39m | \u001b[39m67.93    \u001b[39m | \u001b[39m0.009683 \u001b[39m | \u001b[39m12.99    \u001b[39m | \u001b[39m7.245    \u001b[39m |\n",
      "| \u001b[39m15       \u001b[39m | \u001b[39m0.9797   \u001b[39m | \u001b[39m59.64    \u001b[39m | \u001b[39m0.2828   \u001b[39m | \u001b[39m67.9     \u001b[39m | \u001b[39m0.006894 \u001b[39m | \u001b[39m14.74    \u001b[39m | \u001b[39m6.554    \u001b[39m |\n",
      "=================================================================================================\n",
      "\n",
      "Best Parameters Found: {'batch_size': 57.8409293420101, 'dropout_rate': 0.272035740274469, 'epochs': 66.32977737447999, 'learning_rate': 0.0006900738288840708, 'x_dim': 10.634561353330376, 'y_dim': 6.804603304943683}\n",
      "\n",
      "Training Final Model...\n",
      "Epoch 1/66\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.2705 - loss: 1.3733   \n",
      "Epoch 2/66\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5860 - loss: 1.2434 \n",
      "Epoch 3/66\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6473 - loss: 1.1218 \n",
      "Epoch 4/66\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6558 - loss: 1.0012 \n",
      "Epoch 5/66\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6938 - loss: 0.8751 \n",
      "Epoch 6/66\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7470 - loss: 0.7564 \n",
      "Epoch 7/66\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 798us/step - accuracy: 0.8264 - loss: 0.6288\n",
      "Epoch 8/66\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8674 - loss: 0.5088 \n",
      "Epoch 9/66\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9020 - loss: 0.4186 \n",
      "Epoch 10/66\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9237 - loss: 0.3383 \n",
      "Epoch 11/66\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9486 - loss: 0.2777 \n",
      "Epoch 12/66\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9455 - loss: 0.2519 \n",
      "Epoch 13/66\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9538 - loss: 0.2198 \n",
      "Epoch 14/66\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9517 - loss: 0.2031 \n",
      "Epoch 15/66\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9525 - loss: 0.1844 \n",
      "Epoch 16/66\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9543 - loss: 0.1783 \n",
      "Epoch 17/66\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 895us/step - accuracy: 0.9528 - loss: 0.1877\n",
      "Epoch 18/66\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 977us/step - accuracy: 0.9589 - loss: 0.1663\n",
      "Epoch 19/66\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9544 - loss: 0.1585 \n",
      "Epoch 20/66\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9573 - loss: 0.1591 \n",
      "Epoch 21/66\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9568 - loss: 0.1579 \n",
      "Epoch 22/66\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9578 - loss: 0.1489\n",
      "Epoch 23/66\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9564 - loss: 0.1589 \n",
      "Epoch 24/66\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9512 - loss: 0.1537 \n",
      "Epoch 25/66\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9551 - loss: 0.1513 \n",
      "Epoch 26/66\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9567 - loss: 0.1484 \n",
      "Epoch 27/66\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9569 - loss: 0.1476 \n",
      "Epoch 28/66\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9565 - loss: 0.1416 \n",
      "Epoch 29/66\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9548 - loss: 0.1387 \n",
      "Epoch 30/66\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9547 - loss: 0.1403 \n",
      "Epoch 31/66\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9558 - loss: 0.1384 \n",
      "Epoch 32/66\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 818us/step - accuracy: 0.9581 - loss: 0.1385\n",
      "Epoch 33/66\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9589 - loss: 0.1396 \n",
      "Epoch 34/66\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9571 - loss: 0.1345 \n",
      "Epoch 35/66\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9529 - loss: 0.1432 \n",
      "Epoch 36/66\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9575 - loss: 0.1328 \n",
      "Epoch 37/66\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9573 - loss: 0.1360 \n",
      "Epoch 38/66\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9562 - loss: 0.1300 \n",
      "Epoch 39/66\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9561 - loss: 0.1450 \n",
      "Epoch 40/66\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9569 - loss: 0.1313 \n",
      "Epoch 41/66\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9573 - loss: 0.1309 \n",
      "Epoch 42/66\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9558 - loss: 0.1266 \n",
      "Epoch 43/66\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9533 - loss: 0.1307 \n",
      "Epoch 44/66\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9586 - loss: 0.1314 \n",
      "Epoch 45/66\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 882us/step - accuracy: 0.9574 - loss: 0.1240\n",
      "Epoch 46/66\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9575 - loss: 0.1298 \n",
      "Epoch 47/66\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9569 - loss: 0.1258  \n",
      "Epoch 48/66\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 734us/step - accuracy: 0.9558 - loss: 0.1264\n",
      "Epoch 49/66\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9568 - loss: 0.1228 \n",
      "Epoch 50/66\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9555 - loss: 0.1335 \n",
      "Epoch 51/66\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9551 - loss: 0.1233 \n",
      "Epoch 52/66\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 782us/step - accuracy: 0.9542 - loss: 0.1343\n",
      "Epoch 53/66\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 966us/step - accuracy: 0.9575 - loss: 0.1185\n",
      "Epoch 54/66\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9574 - loss: 0.1257 \n",
      "Epoch 55/66\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9564 - loss: 0.1241 \n",
      "Epoch 56/66\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 750us/step - accuracy: 0.9566 - loss: 0.1274\n",
      "Epoch 57/66\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9579 - loss: 0.1277 \n",
      "Epoch 58/66\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9585 - loss: 0.1286 \n",
      "Epoch 59/66\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9570 - loss: 0.1296 \n",
      "Epoch 60/66\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9575 - loss: 0.1247 \n",
      "Epoch 61/66\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9598 - loss: 0.1193 \n",
      "Epoch 62/66\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9559 - loss: 0.1341 \n",
      "Epoch 63/66\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 801us/step - accuracy: 0.9576 - loss: 0.1335\n",
      "Epoch 64/66\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9548 - loss: 0.1233 \n",
      "Epoch 65/66\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9575 - loss: 0.1271 \n",
      "Epoch 66/66\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 797us/step - accuracy: 0.9526 - loss: 0.1299\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\n",
      "===== Evaluation =====\n",
      "Accuracy: 0.9287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000        27\n",
      "           1     0.8361    0.9343    0.8825       213\n",
      "           2     0.9713    0.9676    0.9694       524\n",
      "           3     0.9245    1.0000    0.9608        49\n",
      "\n",
      "    accuracy                         0.9287       813\n",
      "   macro avg     0.6830    0.7255    0.7032       813\n",
      "weighted avg     0.9008    0.9287    0.9139       813\n",
      "\n",
      "Confusion Matrix:\n",
      " [[  0  26   1   0]\n",
      " [  0 199  14   0]\n",
      " [  0  13 507   4]\n",
      " [  0   0   0  49]]\n",
      "\n",
      "===== Model Metrics =====\n",
      "Training Time: 5.33s\n",
      "Inference/sample: 0.000205s\n",
      "Testing Time: 0.17s\n",
      "Model Size: 0.04 MB\n",
      "Trainable Params: 6,116\n",
      "RAM Usage: 931.06 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import psutil\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from minisom import MiniSom\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from bayes_opt import BayesianOptimization\n",
    "from scipy.fftpack import fft\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "def load_and_preprocess_data(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df.columns = df.columns.str.replace('\"', '').str.strip()\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'], errors='coerce')\n",
    "    df.dropna(subset=['TIME'], inplace=True)\n",
    "    df.set_index('TIME', inplace=True)\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "    target_col = 'fenologia_h3'\n",
    "    df[target_col].interpolate(method='linear', inplace=True)\n",
    "    df[target_col] = df[target_col] - 1\n",
    "\n",
    "    for i in range(1, 4):\n",
    "        df[f'{target_col}_lag{i}'] = df[target_col].shift(i)\n",
    "    for window in [3, 6]:\n",
    "        df[f'{target_col}_roll_mean_{window}'] = df[target_col].rolling(window).mean()\n",
    "        df[f'{target_col}_roll_std_{window}'] = df[target_col].rolling(window).std()\n",
    "\n",
    "\n",
    "\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "    return df, target_col\n",
    "\n",
    "def feature_scaling(df, target_col):\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    return X_scaled, y, scaler\n",
    "\n",
    "def transform_with_som(som, X, y_dim, x_dim):\n",
    "    winners = np.array([som.winner(x) for x in X])\n",
    "    indices = winners[:, 0] * y_dim + winners[:, 1]\n",
    "    return to_categorical(indices, num_classes=x_dim * y_dim)\n",
    "\n",
    "def train_and_evaluate_som_dnn(X_train, X_test, y_train, y_test, x_dim, y_dim, epochs, batch_size, dropout_rate, learning_rate):\n",
    "    som = MiniSom(x_dim, y_dim, X_train.shape[1], sigma=1.0, learning_rate=learning_rate)\n",
    "    som.random_weights_init(X_train)\n",
    "    som.train_random(X_train, 100)\n",
    "\n",
    "    X_train_enc = transform_with_som(som, X_train, y_dim, x_dim)\n",
    "    X_test_enc = transform_with_som(som, X_test, y_dim, x_dim)\n",
    "    y_train_cat = to_categorical(y_train)\n",
    "    y_test_cat = to_categorical(y_test)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_dim=X_train_enc.shape[1]))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(y_train_cat.shape[1], activation='softmax'))\n",
    "    model.compile(optimizer=Adam(learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    start_train = time.time()\n",
    "    model.fit(X_train_enc, y_train_cat, epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "    training_time = time.time() - start_train\n",
    "\n",
    "    start_infer = time.time()\n",
    "    y_pred = model.predict(X_test_enc).argmax(axis=1)\n",
    "    inference_time = (time.time() - start_infer) / len(X_test_enc)\n",
    "    testing_time = time.time() - start_infer\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(\"\\n===== Evaluation =====\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(\"Report:\\n\", classification_report(y_test, y_pred, digits=4))\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "    model.save(\"temp_model_som_dnn.h5\", include_optimizer=False)\n",
    "    model_size = os.path.getsize(\"temp_model_som_dnn.h5\") / (1024 ** 2)\n",
    "    os.remove(\"temp_model_som_dnn.h5\")\n",
    "\n",
    "    total_params = np.sum([np.prod(v.shape) for v in model.trainable_weights])\n",
    "    ram_usage = psutil.Process(os.getpid()).memory_info().rss / (1024 ** 2)\n",
    "\n",
    "    print(\"\\n===== Model Metrics =====\")\n",
    "    print(f\"Training Time: {training_time:.2f}s\")\n",
    "    print(f\"Inference/sample: {inference_time:.6f}s\")\n",
    "    print(f\"Testing Time: {testing_time:.2f}s\")\n",
    "    print(f\"Model Size: {model_size:.2f} MB\")\n",
    "    print(f\"Trainable Params: {total_params:,}\")\n",
    "    print(f\"RAM Usage: {ram_usage:.2f} MB\")\n",
    "\n",
    "def objective_function(epochs, batch_size, x_dim, y_dim, dropout_rate, learning_rate, X_train, y_train, X_val, y_val):\n",
    "    x_dim, y_dim = int(x_dim), int(y_dim)\n",
    "    batch_size, epochs = int(batch_size), int(epochs)\n",
    "    dropout_rate = float(dropout_rate)\n",
    "    learning_rate = float(learning_rate)\n",
    "\n",
    "    som = MiniSom(x_dim, y_dim, X_train.shape[1], sigma=1.0, learning_rate=learning_rate)\n",
    "    som.random_weights_init(X_train)\n",
    "    som.train_random(X_train, 100)\n",
    "\n",
    "    X_train_enc = transform_with_som(som, X_train, y_dim, x_dim)\n",
    "    X_val_enc = transform_with_som(som, X_val, y_dim, x_dim)\n",
    "    y_train_cat = to_categorical(y_train)\n",
    "    y_val_cat = to_categorical(y_val)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_dim=X_train_enc.shape[1]))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(y_train_cat.shape[1], activation='softmax'))\n",
    "    model.compile(optimizer=Adam(learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(X_train_enc, y_train_cat, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    _, acc = model.evaluate(X_val_enc, y_val_cat, verbose=0)\n",
    "    return acc\n",
    "\n",
    "def optimize_som_dnn_hyperparameters(X, y):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    pbounds = {\n",
    "        'epochs': (50, 100),\n",
    "        'batch_size': (16, 64),\n",
    "        'x_dim': (5, 15),\n",
    "        'y_dim': (5, 15),\n",
    "        'dropout_rate': (0.1, 0.5),\n",
    "        'learning_rate': (0.0005, 0.01)\n",
    "    }\n",
    "\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=lambda epochs, batch_size, x_dim, y_dim, dropout_rate, learning_rate:\n",
    "            objective_function(epochs, batch_size, x_dim, y_dim, dropout_rate, learning_rate,\n",
    "                               X_train, y_train, X_val, y_val),\n",
    "        pbounds=pbounds,\n",
    "        random_state=42,\n",
    "        verbose=2\n",
    "    )\n",
    "    optimizer.maximize(init_points=5, n_iter=10)\n",
    "    return optimizer.max['params']\n",
    "\n",
    "def main():\n",
    "    train_path = r\"E:\\Abroad period research\\Phenology datasets\\PHENOLOGY_H3_train.csv\"\n",
    "    test_path = r\"E:\\Abroad period research\\Phenology datasets\\PHENOLOGY_H3_test.csv\"\n",
    "\n",
    "    df_train, target_col = load_and_preprocess_data(train_path)\n",
    "    X_train, y_train, _ = feature_scaling(df_train, target_col)\n",
    "\n",
    "    df_test, _ = load_and_preprocess_data(test_path)\n",
    "    X_test, y_test, _ = feature_scaling(df_test, target_col)\n",
    "\n",
    "    print(\"\\nOptimizing Hyperparameters...\")\n",
    "    best_params = optimize_som_dnn_hyperparameters(X_train, y_train)\n",
    "    print(\"\\nBest Parameters Found:\", best_params)\n",
    "\n",
    "    print(\"\\nTraining Final Model...\")\n",
    "    train_and_evaluate_som_dnn(\n",
    "        X_train, X_test, y_train, y_test,\n",
    "        x_dim=int(best_params['x_dim']),\n",
    "        y_dim=int(best_params['y_dim']),\n",
    "        epochs=int(best_params['epochs']),\n",
    "        batch_size=int(best_params['batch_size']),\n",
    "        dropout_rate=float(best_params['dropout_rate']),\n",
    "        learning_rate=float(best_params['learning_rate'])\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed74060e",
   "metadata": {},
   "source": [
    "Horizon 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d17c8895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimizing Hyperparameters...\n",
      "|   iter    |  target   | batch_... | dropou... |  epochs   | learni... |   x_dim   |   y_dim   |\n",
      "-------------------------------------------------------------------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.935    \u001b[39m | \u001b[39m33.98    \u001b[39m | \u001b[39m0.4803   \u001b[39m | \u001b[39m86.6     \u001b[39m | \u001b[39m0.006187 \u001b[39m | \u001b[39m6.56     \u001b[39m | \u001b[39m6.56     \u001b[39m |\n",
      "| \u001b[35m2        \u001b[39m | \u001b[35m0.9797   \u001b[39m | \u001b[35m18.79    \u001b[39m | \u001b[35m0.4465   \u001b[39m | \u001b[35m80.06    \u001b[39m | \u001b[35m0.007227 \u001b[39m | \u001b[35m5.206    \u001b[39m | \u001b[35m14.7     \u001b[39m |\n",
      "| \u001b[39m3        \u001b[39m | \u001b[39m0.9756   \u001b[39m | \u001b[39m55.96    \u001b[39m | \u001b[39m0.1849   \u001b[39m | \u001b[39m59.09    \u001b[39m | \u001b[39m0.002242 \u001b[39m | \u001b[39m8.042    \u001b[39m | \u001b[39m10.25    \u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m0.9756   \u001b[39m | \u001b[39m36.73    \u001b[39m | \u001b[39m0.2165   \u001b[39m | \u001b[39m80.59    \u001b[39m | \u001b[39m0.001825 \u001b[39m | \u001b[39m7.921    \u001b[39m | \u001b[39m8.664    \u001b[39m |\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m0.9634   \u001b[39m | \u001b[39m37.89    \u001b[39m | \u001b[39m0.4141   \u001b[39m | \u001b[39m59.98    \u001b[39m | \u001b[39m0.005385 \u001b[39m | \u001b[39m10.92    \u001b[39m | \u001b[39m5.465    \u001b[39m |\n",
      "| \u001b[35m6        \u001b[39m | \u001b[35m0.9878   \u001b[39m | \u001b[35m52.17    \u001b[39m | \u001b[35m0.443    \u001b[39m | \u001b[35m79.51    \u001b[39m | \u001b[35m0.00398  \u001b[39m | \u001b[35m9.41     \u001b[39m | \u001b[35m12.86    \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m0.9715   \u001b[39m | \u001b[39m21.96    \u001b[39m | \u001b[39m0.3851   \u001b[39m | \u001b[39m87.03    \u001b[39m | \u001b[39m0.001986 \u001b[39m | \u001b[39m11.34    \u001b[39m | \u001b[39m13.85    \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m0.9878   \u001b[39m | \u001b[39m26.1     \u001b[39m | \u001b[39m0.497    \u001b[39m | \u001b[39m60.7     \u001b[39m | \u001b[39m0.00716  \u001b[39m | \u001b[39m9.956    \u001b[39m | \u001b[39m7.877    \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m0.9797   \u001b[39m | \u001b[39m53.9     \u001b[39m | \u001b[39m0.3142   \u001b[39m | \u001b[39m94.74    \u001b[39m | \u001b[39m0.004845 \u001b[39m | \u001b[39m9.225    \u001b[39m | \u001b[39m11.54    \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m0.9756   \u001b[39m | \u001b[39m26.89    \u001b[39m | \u001b[39m0.1021   \u001b[39m | \u001b[39m61.04    \u001b[39m | \u001b[39m0.003203 \u001b[39m | \u001b[39m9.675    \u001b[39m | \u001b[39m7.851    \u001b[39m |\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m0.9837   \u001b[39m | \u001b[39m25.54    \u001b[39m | \u001b[39m0.1847   \u001b[39m | \u001b[39m59.81    \u001b[39m | \u001b[39m0.004177 \u001b[39m | \u001b[39m10.59    \u001b[39m | \u001b[39m7.168    \u001b[39m |\n",
      "| \u001b[39m12       \u001b[39m | \u001b[39m0.9634   \u001b[39m | \u001b[39m24.6     \u001b[39m | \u001b[39m0.1854   \u001b[39m | \u001b[39m59.16    \u001b[39m | \u001b[39m0.001149 \u001b[39m | \u001b[39m9.225    \u001b[39m | \u001b[39m9.02     \u001b[39m |\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m0.9756   \u001b[39m | \u001b[39m50.72    \u001b[39m | \u001b[39m0.1834   \u001b[39m | \u001b[39m79.21    \u001b[39m | \u001b[39m0.005352 \u001b[39m | \u001b[39m9.388    \u001b[39m | \u001b[39m12.67    \u001b[39m |\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m0.9797   \u001b[39m | \u001b[39m51.97    \u001b[39m | \u001b[39m0.3267   \u001b[39m | \u001b[39m79.74    \u001b[39m | \u001b[39m0.005433 \u001b[39m | \u001b[39m10.74    \u001b[39m | \u001b[39m12.72    \u001b[39m |\n",
      "| \u001b[39m15       \u001b[39m | \u001b[39m0.9715   \u001b[39m | \u001b[39m52.76    \u001b[39m | \u001b[39m0.3163   \u001b[39m | \u001b[39m78.53    \u001b[39m | \u001b[39m0.008635 \u001b[39m | \u001b[39m8.334    \u001b[39m | \u001b[39m11.3     \u001b[39m |\n",
      "=================================================================================================\n",
      "\n",
      "Best Parameters Found: {'batch_size': 52.16992626336783, 'dropout_rate': 0.4430183503917814, 'epochs': 79.50664219685733, 'learning_rate': 0.003980157903208522, 'x_dim': 9.409671867126791, 'y_dim': 12.862288650347818}\n",
      "\n",
      "Training Final Model...\n",
      "Epoch 1/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.3356 - loss: 1.3290\n",
      "Epoch 2/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6864 - loss: 0.8415 \n",
      "Epoch 3/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9479 - loss: 0.3156 \n",
      "Epoch 4/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9760 - loss: 0.1484 \n",
      "Epoch 5/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9694 - loss: 0.1092 \n",
      "Epoch 6/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9754 - loss: 0.0946 \n",
      "Epoch 7/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9778 - loss: 0.0857 \n",
      "Epoch 8/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9768 - loss: 0.0839 \n",
      "Epoch 9/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9771 - loss: 0.0799 \n",
      "Epoch 10/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9800 - loss: 0.0702 \n",
      "Epoch 11/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9811 - loss: 0.0695 \n",
      "Epoch 12/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9809 - loss: 0.0655 \n",
      "Epoch 13/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9783 - loss: 0.0674 \n",
      "Epoch 14/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9803 - loss: 0.0584 \n",
      "Epoch 15/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9715 - loss: 0.0787 \n",
      "Epoch 16/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9805 - loss: 0.0690 \n",
      "Epoch 17/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9780 - loss: 0.0601 \n",
      "Epoch 18/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9798 - loss: 0.0629\n",
      "Epoch 19/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9808 - loss: 0.0570 \n",
      "Epoch 20/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9797 - loss: 0.0629 \n",
      "Epoch 21/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9747 - loss: 0.0695 \n",
      "Epoch 22/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9780 - loss: 0.0558 \n",
      "Epoch 23/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9769 - loss: 0.0673\n",
      "Epoch 24/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9787 - loss: 0.0595 \n",
      "Epoch 25/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9778 - loss: 0.0641 \n",
      "Epoch 26/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9811 - loss: 0.0616 \n",
      "Epoch 27/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9786 - loss: 0.0665 \n",
      "Epoch 28/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9800 - loss: 0.0558 \n",
      "Epoch 29/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9788 - loss: 0.0663 \n",
      "Epoch 30/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9754 - loss: 0.0690\n",
      "Epoch 31/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9776 - loss: 0.0629 \n",
      "Epoch 32/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9781 - loss: 0.0600 \n",
      "Epoch 33/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9810 - loss: 0.0581 \n",
      "Epoch 34/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9789 - loss: 0.0657 \n",
      "Epoch 35/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9788 - loss: 0.0593 \n",
      "Epoch 36/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9788 - loss: 0.0640 \n",
      "Epoch 37/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9789 - loss: 0.0633 \n",
      "Epoch 38/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9788 - loss: 0.0606 \n",
      "Epoch 39/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9789 - loss: 0.0616 \n",
      "Epoch 40/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9789 - loss: 0.0577 \n",
      "Epoch 41/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9784 - loss: 0.0550 \n",
      "Epoch 42/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9784 - loss: 0.0569 \n",
      "Epoch 43/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9781 - loss: 0.0602 \n",
      "Epoch 44/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9789 - loss: 0.0616\n",
      "Epoch 45/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9788 - loss: 0.0604 \n",
      "Epoch 46/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9799 - loss: 0.0536 \n",
      "Epoch 47/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9788 - loss: 0.0614 \n",
      "Epoch 48/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9811 - loss: 0.0566 \n",
      "Epoch 49/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9792 - loss: 0.0548 \n",
      "Epoch 50/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9789 - loss: 0.0593 \n",
      "Epoch 51/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9810 - loss: 0.0553 \n",
      "Epoch 52/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9789 - loss: 0.0542 \n",
      "Epoch 53/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9810 - loss: 0.0547 \n",
      "Epoch 54/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9798 - loss: 0.0507 \n",
      "Epoch 55/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 790us/step - accuracy: 0.9815 - loss: 0.0541\n",
      "Epoch 56/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9810 - loss: 0.0547 \n",
      "Epoch 57/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9811 - loss: 0.0557 \n",
      "Epoch 58/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9788 - loss: 0.0601 \n",
      "Epoch 59/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9810 - loss: 0.0599 \n",
      "Epoch 60/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9779 - loss: 0.0566 \n",
      "Epoch 61/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9811 - loss: 0.0507 \n",
      "Epoch 62/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9788 - loss: 0.0565 \n",
      "Epoch 63/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 713us/step - accuracy: 0.9810 - loss: 0.0584\n",
      "Epoch 64/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9788 - loss: 0.0616 \n",
      "Epoch 65/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9789 - loss: 0.0538 \n",
      "Epoch 66/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9811 - loss: 0.0579 \n",
      "Epoch 67/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9789 - loss: 0.0591 \n",
      "Epoch 68/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9811 - loss: 0.0550 \n",
      "Epoch 69/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9788 - loss: 0.0647 \n",
      "Epoch 70/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9776 - loss: 0.0613 \n",
      "Epoch 71/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9811 - loss: 0.0556 \n",
      "Epoch 72/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9788 - loss: 0.0538 \n",
      "Epoch 73/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9789 - loss: 0.0545 \n",
      "Epoch 74/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9788 - loss: 0.0585 \n",
      "Epoch 75/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9789 - loss: 0.0558 \n",
      "Epoch 76/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9789 - loss: 0.0546 \n",
      "Epoch 77/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9788 - loss: 0.0562 \n",
      "Epoch 78/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9788 - loss: 0.0524 \n",
      "Epoch 79/79\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9810 - loss: 0.0565 \n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\n",
      "===== Evaluation =====\n",
      "Accuracy: 0.9422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000        13\n",
      "           1     0.9341    0.8543    0.8924       199\n",
      "           2     0.9487    0.9908    0.9693       541\n",
      "           3     0.9091    1.0000    0.9524        60\n",
      "\n",
      "    accuracy                         0.9422       813\n",
      "   macro avg     0.6980    0.7113    0.7035       813\n",
      "weighted avg     0.9270    0.9422    0.9337       813\n",
      "\n",
      "Confusion Matrix:\n",
      " [[  0  12   1   0]\n",
      " [  0 170  28   1]\n",
      " [  0   0 536   5]\n",
      " [  0   0   0  60]]\n",
      "\n",
      "===== Model Metrics =====\n",
      "Training Time: 6.58s\n",
      "Inference/sample: 0.000205s\n",
      "Testing Time: 0.17s\n",
      "Model Size: 0.06 MB\n",
      "Trainable Params: 9,188\n",
      "RAM Usage: 1078.96 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import psutil\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from minisom import MiniSom\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from bayes_opt import BayesianOptimization\n",
    "from scipy.fftpack import fft\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "def load_and_preprocess_data(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df.columns = df.columns.str.replace('\"', '').str.strip()\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'], errors='coerce')\n",
    "    df.dropna(subset=['TIME'], inplace=True)\n",
    "    df.set_index('TIME', inplace=True)\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "    target_col = 'fenologia_h4'\n",
    "    df[target_col].interpolate(method='linear', inplace=True)\n",
    "    df[target_col] = df[target_col] - 1\n",
    "\n",
    "    for i in range(1, 4):\n",
    "        df[f'{target_col}_lag{i}'] = df[target_col].shift(i)\n",
    "    for window in [3, 6]:\n",
    "        df[f'{target_col}_roll_mean_{window}'] = df[target_col].rolling(window).mean()\n",
    "        df[f'{target_col}_roll_std_{window}'] = df[target_col].rolling(window).std()\n",
    "\n",
    "\n",
    "\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "    return df, target_col\n",
    "\n",
    "def feature_scaling(df, target_col):\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    return X_scaled, y, scaler\n",
    "\n",
    "def transform_with_som(som, X, y_dim, x_dim):\n",
    "    winners = np.array([som.winner(x) for x in X])\n",
    "    indices = winners[:, 0] * y_dim + winners[:, 1]\n",
    "    return to_categorical(indices, num_classes=x_dim * y_dim)\n",
    "\n",
    "def train_and_evaluate_som_dnn(X_train, X_test, y_train, y_test, x_dim, y_dim, epochs, batch_size, dropout_rate, learning_rate):\n",
    "    som = MiniSom(x_dim, y_dim, X_train.shape[1], sigma=1.0, learning_rate=learning_rate)\n",
    "    som.random_weights_init(X_train)\n",
    "    som.train_random(X_train, 100)\n",
    "\n",
    "    X_train_enc = transform_with_som(som, X_train, y_dim, x_dim)\n",
    "    X_test_enc = transform_with_som(som, X_test, y_dim, x_dim)\n",
    "    y_train_cat = to_categorical(y_train)\n",
    "    y_test_cat = to_categorical(y_test)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_dim=X_train_enc.shape[1]))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(y_train_cat.shape[1], activation='softmax'))\n",
    "    model.compile(optimizer=Adam(learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    start_train = time.time()\n",
    "    model.fit(X_train_enc, y_train_cat, epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "    training_time = time.time() - start_train\n",
    "\n",
    "    start_infer = time.time()\n",
    "    y_pred = model.predict(X_test_enc).argmax(axis=1)\n",
    "    inference_time = (time.time() - start_infer) / len(X_test_enc)\n",
    "    testing_time = time.time() - start_infer\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(\"\\n===== Evaluation =====\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(\"Report:\\n\", classification_report(y_test, y_pred, digits=4))\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "    model.save(\"temp_model_som_dnn.h5\", include_optimizer=False)\n",
    "    model_size = os.path.getsize(\"temp_model_som_dnn.h5\") / (1024 ** 2)\n",
    "    os.remove(\"temp_model_som_dnn.h5\")\n",
    "\n",
    "    total_params = np.sum([np.prod(v.shape) for v in model.trainable_weights])\n",
    "    ram_usage = psutil.Process(os.getpid()).memory_info().rss / (1024 ** 2)\n",
    "\n",
    "    print(\"\\n===== Model Metrics =====\")\n",
    "    print(f\"Training Time: {training_time:.2f}s\")\n",
    "    print(f\"Inference/sample: {inference_time:.6f}s\")\n",
    "    print(f\"Testing Time: {testing_time:.2f}s\")\n",
    "    print(f\"Model Size: {model_size:.2f} MB\")\n",
    "    print(f\"Trainable Params: {total_params:,}\")\n",
    "    print(f\"RAM Usage: {ram_usage:.2f} MB\")\n",
    "\n",
    "def objective_function(epochs, batch_size, x_dim, y_dim, dropout_rate, learning_rate, X_train, y_train, X_val, y_val):\n",
    "    x_dim, y_dim = int(x_dim), int(y_dim)\n",
    "    batch_size, epochs = int(batch_size), int(epochs)\n",
    "    dropout_rate = float(dropout_rate)\n",
    "    learning_rate = float(learning_rate)\n",
    "\n",
    "    som = MiniSom(x_dim, y_dim, X_train.shape[1], sigma=1.0, learning_rate=learning_rate)\n",
    "    som.random_weights_init(X_train)\n",
    "    som.train_random(X_train, 100)\n",
    "\n",
    "    X_train_enc = transform_with_som(som, X_train, y_dim, x_dim)\n",
    "    X_val_enc = transform_with_som(som, X_val, y_dim, x_dim)\n",
    "    y_train_cat = to_categorical(y_train)\n",
    "    y_val_cat = to_categorical(y_val)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_dim=X_train_enc.shape[1]))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(y_train_cat.shape[1], activation='softmax'))\n",
    "    model.compile(optimizer=Adam(learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(X_train_enc, y_train_cat, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    _, acc = model.evaluate(X_val_enc, y_val_cat, verbose=0)\n",
    "    return acc\n",
    "\n",
    "def optimize_som_dnn_hyperparameters(X, y):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    pbounds = {\n",
    "        'epochs': (50, 100),\n",
    "        'batch_size': (16, 64),\n",
    "        'x_dim': (5, 15),\n",
    "        'y_dim': (5, 15),\n",
    "        'dropout_rate': (0.1, 0.5),\n",
    "        'learning_rate': (0.0005, 0.01)\n",
    "    }\n",
    "\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=lambda epochs, batch_size, x_dim, y_dim, dropout_rate, learning_rate:\n",
    "            objective_function(epochs, batch_size, x_dim, y_dim, dropout_rate, learning_rate,\n",
    "                               X_train, y_train, X_val, y_val),\n",
    "        pbounds=pbounds,\n",
    "        random_state=42,\n",
    "        verbose=2\n",
    "    )\n",
    "    optimizer.maximize(init_points=5, n_iter=10)\n",
    "    return optimizer.max['params']\n",
    "\n",
    "def main():\n",
    "    train_path = r\"E:\\Abroad period research\\Phenology datasets\\PHENOLOGY_H4_train.csv\"\n",
    "    test_path = r\"E:\\Abroad period research\\Phenology datasets\\PHENOLOGY_H4_test.csv\"\n",
    "\n",
    "    df_train, target_col = load_and_preprocess_data(train_path)\n",
    "    X_train, y_train, _ = feature_scaling(df_train, target_col)\n",
    "\n",
    "    df_test, _ = load_and_preprocess_data(test_path)\n",
    "    X_test, y_test, _ = feature_scaling(df_test, target_col)\n",
    "\n",
    "    print(\"\\nOptimizing Hyperparameters...\")\n",
    "    best_params = optimize_som_dnn_hyperparameters(X_train, y_train)\n",
    "    print(\"\\nBest Parameters Found:\", best_params)\n",
    "\n",
    "    print(\"\\nTraining Final Model...\")\n",
    "    train_and_evaluate_som_dnn(\n",
    "        X_train, X_test, y_train, y_test,\n",
    "        x_dim=int(best_params['x_dim']),\n",
    "        y_dim=int(best_params['y_dim']),\n",
    "        epochs=int(best_params['epochs']),\n",
    "        batch_size=int(best_params['batch_size']),\n",
    "        dropout_rate=float(best_params['dropout_rate']),\n",
    "        learning_rate=float(best_params['learning_rate'])\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
