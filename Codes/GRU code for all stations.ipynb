{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690b6dbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b9a7570",
   "metadata": {},
   "source": [
    "Cadiz Station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e93619f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Optimizing Hyperparameters...\n",
      "|   iter    |  target   | batch_... | dropou... |  epochs   | num_units |\n",
      "-------------------------------------------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.9538   \u001b[39m | \u001b[39m33.98    \u001b[39m | \u001b[39m0.4803   \u001b[39m | \u001b[39m86.6     \u001b[39m | \u001b[39m89.47    \u001b[39m |\n",
      "| \u001b[35m2        \u001b[39m | \u001b[35m0.9846   \u001b[39m | \u001b[35m23.49    \u001b[39m | \u001b[35m0.1624   \u001b[39m | \u001b[35m52.9     \u001b[39m | \u001b[35m115.2    \u001b[39m |\n",
      "| \u001b[39m3        \u001b[39m | \u001b[39m0.9385   \u001b[39m | \u001b[39m44.85    \u001b[39m | \u001b[39m0.3832   \u001b[39m | \u001b[39m51.03    \u001b[39m | \u001b[39m125.1    \u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m0.9231   \u001b[39m | \u001b[39m55.96    \u001b[39m | \u001b[39m0.1849   \u001b[39m | \u001b[39m59.09    \u001b[39m | \u001b[39m49.61    \u001b[39m |\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m0.9385   \u001b[39m | \u001b[39m30.6     \u001b[39m | \u001b[39m0.3099   \u001b[39m | \u001b[39m71.6     \u001b[39m | \u001b[39m59.96    \u001b[39m |\n",
      "| \u001b[39m6        \u001b[39m | \u001b[39m0.9692   \u001b[39m | \u001b[39m22.97    \u001b[39m | \u001b[39m0.3185   \u001b[39m | \u001b[39m53.09    \u001b[39m | \u001b[39m116.6    \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m0.9692   \u001b[39m | \u001b[39m23.74    \u001b[39m | \u001b[39m0.304    \u001b[39m | \u001b[39m52.37    \u001b[39m | \u001b[39m112.6    \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m0.9538   \u001b[39m | \u001b[39m25.57    \u001b[39m | \u001b[39m0.4356   \u001b[39m | \u001b[39m53.0     \u001b[39m | \u001b[39m114.3    \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m0.9692   \u001b[39m | \u001b[39m21.95    \u001b[39m | \u001b[39m0.3101   \u001b[39m | \u001b[39m54.97    \u001b[39m | \u001b[39m114.9    \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m0.9692   \u001b[39m | \u001b[39m21.73    \u001b[39m | \u001b[39m0.3643   \u001b[39m | \u001b[39m50.46    \u001b[39m | \u001b[39m112.5    \u001b[39m |\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m0.9692   \u001b[39m | \u001b[39m23.88    \u001b[39m | \u001b[39m0.2827   \u001b[39m | \u001b[39m50.69    \u001b[39m | \u001b[39m115.4    \u001b[39m |\n",
      "| \u001b[39m12       \u001b[39m | \u001b[39m0.9538   \u001b[39m | \u001b[39m20.39    \u001b[39m | \u001b[39m0.4855   \u001b[39m | \u001b[39m51.18    \u001b[39m | \u001b[39m114.7    \u001b[39m |\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m0.9538   \u001b[39m | \u001b[39m20.35    \u001b[39m | \u001b[39m0.4359   \u001b[39m | \u001b[39m53.85    \u001b[39m | \u001b[39m110.6    \u001b[39m |\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m0.9692   \u001b[39m | \u001b[39m22.25    \u001b[39m | \u001b[39m0.229    \u001b[39m | \u001b[39m52.66    \u001b[39m | \u001b[39m114.2    \u001b[39m |\n",
      "| \u001b[39m15       \u001b[39m | \u001b[39m0.9692   \u001b[39m | \u001b[39m23.66    \u001b[39m | \u001b[39m0.4338   \u001b[39m | \u001b[39m56.27    \u001b[39m | \u001b[39m116.2    \u001b[39m |\n",
      "| \u001b[35m16       \u001b[39m | \u001b[35m1.0      \u001b[39m | \u001b[35m22.52    \u001b[39m | \u001b[35m0.2917   \u001b[39m | \u001b[35m57.81    \u001b[39m | \u001b[35m113.7    \u001b[39m |\n",
      "| \u001b[39m17       \u001b[39m | \u001b[39m0.9846   \u001b[39m | \u001b[39m22.88    \u001b[39m | \u001b[39m0.1432   \u001b[39m | \u001b[39m59.47    \u001b[39m | \u001b[39m113.3    \u001b[39m |\n",
      "| \u001b[39m18       \u001b[39m | \u001b[39m0.9846   \u001b[39m | \u001b[39m25.27    \u001b[39m | \u001b[39m0.1242   \u001b[39m | \u001b[39m58.03    \u001b[39m | \u001b[39m113.6    \u001b[39m |\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m0.9692   \u001b[39m | \u001b[39m21.19    \u001b[39m | \u001b[39m0.4994   \u001b[39m | \u001b[39m58.36    \u001b[39m | \u001b[39m116.1    \u001b[39m |\n",
      "| \u001b[39m20       \u001b[39m | \u001b[39m0.9538   \u001b[39m | \u001b[39m20.2     \u001b[39m | \u001b[39m0.4591   \u001b[39m | \u001b[39m58.63    \u001b[39m | \u001b[39m111.7    \u001b[39m |\n",
      "=========================================================================\n",
      "\n",
      " Best Params: {'batch_size': 22.521309284338912, 'dropout_rate': 0.2917166281752329, 'epochs': 57.80857372205241, 'num_units': 113.69339174508767}\n",
      "\n",
      " Training Final GRU Model...\n",
      "Epoch 1/57\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step - accuracy: 0.3325 - loss: 1.3709 - val_accuracy: 0.5692 - val_loss: 1.2115\n",
      "Epoch 2/57\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5320 - loss: 1.1971 - val_accuracy: 0.3385 - val_loss: 1.0907\n",
      "Epoch 3/57\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4713 - loss: 1.0905 - val_accuracy: 0.5385 - val_loss: 0.9925\n",
      "Epoch 4/57\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5612 - loss: 1.0302 - val_accuracy: 0.6308 - val_loss: 0.8885\n",
      "Epoch 5/57\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6181 - loss: 0.9183 - val_accuracy: 0.7077 - val_loss: 0.7633\n",
      "Epoch 6/57\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7046 - loss: 0.7936 - val_accuracy: 0.7538 - val_loss: 0.6432\n",
      "Epoch 7/57\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7516 - loss: 0.7098 - val_accuracy: 0.8000 - val_loss: 0.5481\n",
      "Epoch 8/57\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7766 - loss: 0.6239 - val_accuracy: 0.8308 - val_loss: 0.4771\n",
      "Epoch 9/57\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7720 - loss: 0.5548 - val_accuracy: 0.8769 - val_loss: 0.4378\n",
      "Epoch 10/57\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8006 - loss: 0.5022 - val_accuracy: 0.8462 - val_loss: 0.4120\n",
      "Epoch 11/57\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8022 - loss: 0.4917 - val_accuracy: 0.8923 - val_loss: 0.3905\n",
      "Epoch 12/57\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8408 - loss: 0.4397 - val_accuracy: 0.8462 - val_loss: 0.3850\n",
      "Epoch 13/57\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8293 - loss: 0.4549 - val_accuracy: 0.8923 - val_loss: 0.3602\n",
      "Epoch 14/57\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8223 - loss: 0.4531 - val_accuracy: 0.8769 - val_loss: 0.3585\n",
      "Epoch 15/57\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8284 - loss: 0.4249 - val_accuracy: 0.8923 - val_loss: 0.3383\n",
      "Epoch 16/57\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8353 - loss: 0.3962 - val_accuracy: 0.9231 - val_loss: 0.3203\n",
      "Epoch 17/57\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8293 - loss: 0.3928 - val_accuracy: 0.8923 - val_loss: 0.3223\n",
      "Epoch 18/57\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8572 - loss: 0.3874 - val_accuracy: 0.8923 - val_loss: 0.3086\n",
      "Epoch 19/57\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8335 - loss: 0.3793 - val_accuracy: 0.9077 - val_loss: 0.2998\n",
      "Epoch 20/57\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8427 - loss: 0.3678 - val_accuracy: 0.9231 - val_loss: 0.2824\n",
      "Epoch 21/57\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8505 - loss: 0.3713 - val_accuracy: 0.9077 - val_loss: 0.2833\n",
      "Epoch 22/57\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8795 - loss: 0.3294 - val_accuracy: 0.9231 - val_loss: 0.2639\n",
      "Epoch 23/57\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8553 - loss: 0.3342 - val_accuracy: 0.9077 - val_loss: 0.2655\n",
      "Epoch 24/57\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8584 - loss: 0.3272 - val_accuracy: 0.9231 - val_loss: 0.2424\n",
      "Epoch 25/57\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8940 - loss: 0.2983 - val_accuracy: 0.9231 - val_loss: 0.2343\n",
      "Epoch 26/57\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8723 - loss: 0.3146 - val_accuracy: 0.9231 - val_loss: 0.2315\n",
      "Epoch 27/57\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8841 - loss: 0.2951 - val_accuracy: 0.9231 - val_loss: 0.2187\n",
      "Epoch 28/57\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8624 - loss: 0.3068 - val_accuracy: 0.9231 - val_loss: 0.2180\n",
      "Epoch 29/57\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8886 - loss: 0.2708 - val_accuracy: 0.9231 - val_loss: 0.2053\n",
      "Epoch 30/57\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8688 - loss: 0.2716 - val_accuracy: 0.9231 - val_loss: 0.2002\n",
      "Epoch 31/57\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8783 - loss: 0.2875 - val_accuracy: 0.9385 - val_loss: 0.2018\n",
      "Epoch 32/57\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8858 - loss: 0.3107 - val_accuracy: 0.9231 - val_loss: 0.1815\n",
      "Epoch 33/57\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8769 - loss: 0.2633 - val_accuracy: 0.9231 - val_loss: 0.1685\n",
      "Epoch 34/57\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8821 - loss: 0.2634 - val_accuracy: 0.9385 - val_loss: 0.1726\n",
      "Epoch 35/57\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8965 - loss: 0.2596 - val_accuracy: 0.9231 - val_loss: 0.1575\n",
      "Epoch 36/57\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9077 - loss: 0.2671 - val_accuracy: 0.9231 - val_loss: 0.1552\n",
      "Epoch 37/57\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8809 - loss: 0.2615 - val_accuracy: 0.9385 - val_loss: 0.1544\n",
      "Epoch 38/57\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8986 - loss: 0.2287 - val_accuracy: 0.9385 - val_loss: 0.1547\n",
      "Epoch 39/57\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8988 - loss: 0.2194 - val_accuracy: 0.9385 - val_loss: 0.1379\n",
      "Epoch 40/57\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8990 - loss: 0.2219 - val_accuracy: 0.9385 - val_loss: 0.1369\n",
      "Epoch 41/57\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9040 - loss: 0.2364 - val_accuracy: 0.9538 - val_loss: 0.1408\n",
      "Epoch 42/57\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9306 - loss: 0.2251 - val_accuracy: 0.9385 - val_loss: 0.1301\n",
      "Epoch 43/57\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9130 - loss: 0.2102 - val_accuracy: 0.9385 - val_loss: 0.1336\n",
      "Epoch 44/57\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9220 - loss: 0.2212 - val_accuracy: 0.9538 - val_loss: 0.1185\n",
      "Epoch 45/57\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9097 - loss: 0.2266 - val_accuracy: 0.9385 - val_loss: 0.1198\n",
      "Epoch 46/57\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9348 - loss: 0.1915 - val_accuracy: 0.9692 - val_loss: 0.1223\n",
      "Epoch 47/57\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9180 - loss: 0.1970 - val_accuracy: 0.9385 - val_loss: 0.1084\n",
      "Epoch 48/57\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9084 - loss: 0.1933 - val_accuracy: 0.9538 - val_loss: 0.1068\n",
      "Epoch 49/57\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9161 - loss: 0.1912 - val_accuracy: 0.9846 - val_loss: 0.0975\n",
      "Epoch 50/57\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9207 - loss: 0.1962 - val_accuracy: 0.9692 - val_loss: 0.0963\n",
      "Epoch 51/57\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9348 - loss: 0.1828 - val_accuracy: 0.9692 - val_loss: 0.1008\n",
      "Epoch 52/57\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9397 - loss: 0.1869 - val_accuracy: 0.9692 - val_loss: 0.0877\n",
      "Epoch 53/57\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9212 - loss: 0.1899 - val_accuracy: 0.9846 - val_loss: 0.0870\n",
      "Epoch 54/57\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9221 - loss: 0.1686 - val_accuracy: 0.9846 - val_loss: 0.0842\n",
      "Epoch 55/57\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9423 - loss: 0.1672 - val_accuracy: 0.9692 - val_loss: 0.0835\n",
      "Epoch 56/57\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9352 - loss: 0.1548 - val_accuracy: 0.9692 - val_loss: 0.0946\n",
      "Epoch 57/57\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9169 - loss: 0.1898 - val_accuracy: 0.9846 - val_loss: 0.0808\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Evaluation =====\n",
      "Accuracy: 0.9310\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7692    0.6250    0.6897        16\n",
      "           1     0.8800    0.9565    0.9167        69\n",
      "           2     0.9779    0.9708    0.9744       137\n",
      "           3     0.8750    0.7000    0.7778        10\n",
      "\n",
      "    accuracy                         0.9310       232\n",
      "   macro avg     0.8755    0.8131    0.8396       232\n",
      "weighted avg     0.9300    0.9310    0.9291       232\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 10   6   0   0]\n",
      " [  3  66   0   0]\n",
      " [  0   3 133   1]\n",
      " [  0   0   3   7]]\n",
      "\n",
      "===== Model Metrics =====\n",
      "Training Time: 11.79s\n",
      "Inference/sample: 0.003043s\n",
      "Testing Time: 0.71s\n",
      "Model Size: 0.22 MB\n",
      "Trainable Params: 51,360\n",
      "RAM Usage: 1072.01 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import psutil\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout\n",
    "import tensorflow as tf\n",
    "from bayes_opt import BayesianOptimization\n",
    "from scipy.fftpack import fft\n",
    "import warnings\n",
    "\n",
    "# Set seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================\n",
    "# 1. Data Loading and Preprocessing\n",
    "# ============================================\n",
    "def load_and_preprocess_data(filepath):\n",
    "    df = pd.read_csv(filepath, delimiter=\",\", low_memory=False)\n",
    "    df.columns = df.columns.str.replace('\"', '').str.strip()\n",
    "\n",
    "    if 'TIME' not in df.columns:\n",
    "        raise KeyError(\"The column 'TIME' does not exist in the dataset.\")\n",
    "\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'], errors='coerce')\n",
    "    df.dropna(subset=['TIME'], inplace=True)\n",
    "    df.set_index('TIME', inplace=True)\n",
    "\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "    target_col = 'fenologia_h1'\n",
    "    if target_col not in df.columns:\n",
    "        raise KeyError(f\"The target column '{target_col}' does not exist.\")\n",
    "\n",
    "    df[target_col].interpolate(method='linear', inplace=True)\n",
    "    df[target_col] = df[target_col] - 1\n",
    "\n",
    "    for i in range(1, 4):\n",
    "        df[f'{target_col}_lag{i}'] = df[target_col].shift(i)\n",
    "\n",
    "    for window in [3, 6]:\n",
    "        df[f'{target_col}_roll_mean_{window}'] = df[target_col].rolling(window).mean()\n",
    "        df[f'{target_col}_roll_std_{window}'] = df[target_col].rolling(window).std()\n",
    "\n",
    "    # df['month'] = df.index.month\n",
    "    # df['weekofyear'] = df.index.isocalendar().week\n",
    "    # df['year'] = df.index.isocalendar().year\n",
    "    # df['sin_week'] = np.sin(2 * np.pi * df['weekofyear'] / 52)\n",
    "    # df['cos_week'] = np.cos(2 * np.pi * df['weekofyear'] / 52)\n",
    "    # df['EMA_3'] = df[target_col].ewm(span=3, adjust=False).mean()\n",
    "    # df['EMA_6'] = df[target_col].ewm(span=6, adjust=False).mean()\n",
    "    # df['correlation_target_month'] = df[target_col].rolling(window=6).corr(df['month'])\n",
    "    # df['correlation_target_week'] = df[target_col].rolling(window=6).corr(df['weekofyear'])\n",
    "\n",
    "    # fft_values = fft(df[target_col].dropna().values)\n",
    "    # fft_real = np.real(fft_values)[:len(df[target_col])]\n",
    "    # fft_imag = np.imag(fft_values)[:len(df[target_col])]\n",
    "    # df['fft_real'] = np.concatenate([fft_real, np.nan * np.ones(len(df) - len(fft_real))])\n",
    "    # df['fft_imag'] = np.concatenate([fft_imag, np.nan * np.ones(len(df) - len(fft_imag))])\n",
    "\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "    return df, target_col\n",
    "\n",
    "# ============================================\n",
    "# 2. Feature Scaling\n",
    "# ============================================\n",
    "def feature_scaling(df, target_col):\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))  # 3D for RNN\n",
    "    return X_scaled, y, scaler\n",
    "\n",
    "# ============================================\n",
    "# 3. Build GRU Model\n",
    "# ============================================\n",
    "def build_gru_model(input_shape, num_classes=4, num_units=64, dropout_rate=0.2):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(num_units, input_shape=input_shape))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(num_units // 2, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# ============================================\n",
    "# 4. Bayesian Optimization Objective\n",
    "# ============================================\n",
    "def objective_function(num_units, dropout_rate, epochs, batch_size, X_train, y_train):\n",
    "    model = build_gru_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        num_classes=4,\n",
    "        num_units=int(num_units),\n",
    "        dropout_rate=dropout_rate\n",
    "    )\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=int(epochs),\n",
    "        batch_size=int(batch_size),\n",
    "        validation_split=0.2,\n",
    "        verbose=0\n",
    "    )\n",
    "    return np.max(history.history['val_accuracy'])\n",
    "\n",
    "# ============================================\n",
    "# 5. Optimize Hyperparameters\n",
    "# ============================================\n",
    "def optimize_hyperparameters(X_train, y_train):\n",
    "    pbounds = {\n",
    "        'num_units': (32, 128),\n",
    "        'dropout_rate': (0.1, 0.5),\n",
    "        'epochs': (50, 100),\n",
    "        'batch_size': (16, 64)\n",
    "    }\n",
    "\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=lambda num_units, dropout_rate, epochs, batch_size: objective_function(\n",
    "            num_units, dropout_rate, epochs, batch_size, X_train, y_train\n",
    "        ),\n",
    "        pbounds=pbounds,\n",
    "        random_state=42,\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    optimizer.maximize(init_points=5, n_iter=15)\n",
    "    return optimizer.max['params']\n",
    "\n",
    "# ============================================\n",
    "# 6. Train & Evaluate Model\n",
    "# ============================================\n",
    "def train_and_evaluate_model(X_train, X_test, y_train, y_test, best_params):\n",
    "    model = build_gru_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        num_classes=4,\n",
    "        num_units=int(best_params['num_units']),\n",
    "        dropout_rate=float(best_params['dropout_rate'])\n",
    "    )\n",
    "\n",
    "    start_train = time.time()\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=int(best_params['epochs']),\n",
    "        batch_size=int(best_params['batch_size']),\n",
    "        validation_split=0.2,\n",
    "        verbose=1\n",
    "    )\n",
    "    training_time = time.time() - start_train\n",
    "\n",
    "    start_infer = time.time()\n",
    "    y_pred = model.predict(X_test).argmax(axis=1)\n",
    "    inference_time = (time.time() - start_infer) / len(X_test)\n",
    "\n",
    "    # Calculate total testing time\n",
    "    testing_time = time.time() - start_infer\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(\"\\n===== Evaluation =====\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(\"Report:\\n\", classification_report(y_test, y_pred, digits=4))\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "    model.save(\"temp_model.h5\", include_optimizer=False)\n",
    "    model_size = os.path.getsize(\"temp_model.h5\") / (1024 ** 2)\n",
    "    os.remove(\"temp_model.h5\")\n",
    "\n",
    "    total_params = np.sum([np.prod(v.shape) for v in model.trainable_weights])\n",
    "    ram_usage = psutil.Process(os.getpid()).memory_info().rss / (1024 ** 2)\n",
    "\n",
    "    print(\"\\n===== Model Metrics =====\")\n",
    "    print(f\"Training Time: {training_time:.2f}s\")\n",
    "    print(f\"Inference/sample: {inference_time:.6f}s\")\n",
    "    print(f\"Testing Time: {testing_time:.2f}s\")\n",
    "    print(f\"Model Size: {model_size:.2f} MB\")\n",
    "    print(f\"Trainable Params: {total_params:,}\")\n",
    "    print(f\"RAM Usage: {ram_usage:.2f} MB\")\n",
    "\n",
    "    return model, history\n",
    "\n",
    "# ============================================\n",
    "# 7. Main\n",
    "# ============================================\n",
    "def main():\n",
    "    train_path =  r\"E:\\Abroad period research\\Phenology datasets\\PHENOLOGY_H1\\Cadiz_train.csv\"\n",
    "    test_path =  r\"E:\\Abroad period research\\Phenology datasets\\PHENOLOGY_H1\\Cadiz_test.csv\"\n",
    "\n",
    "    df_train, target_col = load_and_preprocess_data(train_path)\n",
    "    X_train, y_train, _ = feature_scaling(df_train, target_col)\n",
    "\n",
    "    df_test, _ = load_and_preprocess_data(test_path)\n",
    "    X_test, y_test, _ = feature_scaling(df_test, target_col)\n",
    "\n",
    "    print(\"\\n Optimizing Hyperparameters...\")\n",
    "    best_params = optimize_hyperparameters(X_train, y_train)\n",
    "    print(\"\\n Best Params:\", best_params)\n",
    "\n",
    "    print(\"\\n Training Final GRU Model...\")\n",
    "    model, history = train_and_evaluate_model(X_train, X_test, y_train, y_test, best_params)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9225d68",
   "metadata": {},
   "source": [
    "Cordoba station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0470a5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Optimizing Hyperparameters...\n",
      "|   iter    |  target   | batch_... | dropou... |  epochs   | num_units |\n",
      "-------------------------------------------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.9355   \u001b[39m | \u001b[39m33.98    \u001b[39m | \u001b[39m0.4803   \u001b[39m | \u001b[39m86.6     \u001b[39m | \u001b[39m89.47    \u001b[39m |\n",
      "| \u001b[39m2        \u001b[39m | \u001b[39m0.9355   \u001b[39m | \u001b[39m23.49    \u001b[39m | \u001b[39m0.1624   \u001b[39m | \u001b[39m52.9     \u001b[39m | \u001b[39m115.2    \u001b[39m |\n",
      "| \u001b[39m3        \u001b[39m | \u001b[39m0.9032   \u001b[39m | \u001b[39m44.85    \u001b[39m | \u001b[39m0.3832   \u001b[39m | \u001b[39m51.03    \u001b[39m | \u001b[39m125.1    \u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m0.871    \u001b[39m | \u001b[39m55.96    \u001b[39m | \u001b[39m0.1849   \u001b[39m | \u001b[39m59.09    \u001b[39m | \u001b[39m49.61    \u001b[39m |\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m0.9032   \u001b[39m | \u001b[39m30.6     \u001b[39m | \u001b[39m0.3099   \u001b[39m | \u001b[39m71.6     \u001b[39m | \u001b[39m59.96    \u001b[39m |\n",
      "| \u001b[35m6        \u001b[39m | \u001b[35m0.9516   \u001b[39m | \u001b[35m22.97    \u001b[39m | \u001b[35m0.3185   \u001b[39m | \u001b[35m53.09    \u001b[39m | \u001b[35m116.6    \u001b[39m |\n",
      "| \u001b[35m7        \u001b[39m | \u001b[35m0.9677   \u001b[39m | \u001b[35m20.86    \u001b[39m | \u001b[35m0.1756   \u001b[39m | \u001b[35m51.67    \u001b[39m | \u001b[35m118.6    \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m0.9355   \u001b[39m | \u001b[39m19.8     \u001b[39m | \u001b[39m0.3965   \u001b[39m | \u001b[39m56.4     \u001b[39m | \u001b[39m122.9    \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m0.9516   \u001b[39m | \u001b[39m25.82    \u001b[39m | \u001b[39m0.4462   \u001b[39m | \u001b[39m51.56    \u001b[39m | \u001b[39m123.0    \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m0.9677   \u001b[39m | \u001b[39m17.1     \u001b[39m | \u001b[39m0.3969   \u001b[39m | \u001b[39m52.62    \u001b[39m | \u001b[39m115.3    \u001b[39m |\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m0.9677   \u001b[39m | \u001b[39m16.79    \u001b[39m | \u001b[39m0.2093   \u001b[39m | \u001b[39m60.63    \u001b[39m | \u001b[39m113.5    \u001b[39m |\n",
      "| \u001b[39m12       \u001b[39m | \u001b[39m0.9677   \u001b[39m | \u001b[39m16.36    \u001b[39m | \u001b[39m0.3231   \u001b[39m | \u001b[39m69.09    \u001b[39m | \u001b[39m112.5    \u001b[39m |\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m0.9516   \u001b[39m | \u001b[39m16.16    \u001b[39m | \u001b[39m0.1816   \u001b[39m | \u001b[39m64.22    \u001b[39m | \u001b[39m105.5    \u001b[39m |\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m0.9677   \u001b[39m | \u001b[39m24.77    \u001b[39m | \u001b[39m0.1452   \u001b[39m | \u001b[39m67.75    \u001b[39m | \u001b[39m112.3    \u001b[39m |\n",
      "| \u001b[39m15       \u001b[39m | \u001b[39m0.9677   \u001b[39m | \u001b[39m22.47    \u001b[39m | \u001b[39m0.2536   \u001b[39m | \u001b[39m74.53    \u001b[39m | \u001b[39m109.6    \u001b[39m |\n",
      "| \u001b[39m16       \u001b[39m | \u001b[39m0.9677   \u001b[39m | \u001b[39m22.17    \u001b[39m | \u001b[39m0.4679   \u001b[39m | \u001b[39m74.38    \u001b[39m | \u001b[39m118.5    \u001b[39m |\n",
      "| \u001b[39m17       \u001b[39m | \u001b[39m0.9516   \u001b[39m | \u001b[39m30.25    \u001b[39m | \u001b[39m0.4851   \u001b[39m | \u001b[39m74.46    \u001b[39m | \u001b[39m114.6    \u001b[39m |\n",
      "| \u001b[39m18       \u001b[39m | \u001b[39m0.9677   \u001b[39m | \u001b[39m16.95    \u001b[39m | \u001b[39m0.1214   \u001b[39m | \u001b[39m80.73    \u001b[39m | \u001b[39m115.0    \u001b[39m |\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m0.9677   \u001b[39m | \u001b[39m16.94    \u001b[39m | \u001b[39m0.2784   \u001b[39m | \u001b[39m81.44    \u001b[39m | \u001b[39m124.6    \u001b[39m |\n",
      "| \u001b[39m20       \u001b[39m | \u001b[39m0.9677   \u001b[39m | \u001b[39m23.23    \u001b[39m | \u001b[39m0.1899   \u001b[39m | \u001b[39m86.39    \u001b[39m | \u001b[39m120.0    \u001b[39m |\n",
      "=========================================================================\n",
      "\n",
      " Best Params: {'batch_size': 20.86220386253398, 'dropout_rate': 0.17561706077039912, 'epochs': 51.66574861345459, 'num_units': 118.61136563967776}\n",
      "\n",
      " Training Final GRU Model...\n",
      "Epoch 1/51\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.5396 - loss: 1.2980 - val_accuracy: 0.5806 - val_loss: 1.0809\n",
      "Epoch 2/51\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5279 - loss: 1.0924 - val_accuracy: 0.5806 - val_loss: 0.9487\n",
      "Epoch 3/51\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5316 - loss: 0.9645 - val_accuracy: 0.6129 - val_loss: 0.8514\n",
      "Epoch 4/51\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5894 - loss: 0.8795 - val_accuracy: 0.6613 - val_loss: 0.7608\n",
      "Epoch 5/51\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6623 - loss: 0.7620 - val_accuracy: 0.6613 - val_loss: 0.6746\n",
      "Epoch 6/51\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7763 - loss: 0.6715 - val_accuracy: 0.7258 - val_loss: 0.6022\n",
      "Epoch 7/51\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7987 - loss: 0.5856 - val_accuracy: 0.7581 - val_loss: 0.5391\n",
      "Epoch 8/51\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8198 - loss: 0.5199 - val_accuracy: 0.8065 - val_loss: 0.4903\n",
      "Epoch 9/51\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8466 - loss: 0.4541 - val_accuracy: 0.8226 - val_loss: 0.4627\n",
      "Epoch 10/51\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8342 - loss: 0.4108 - val_accuracy: 0.8387 - val_loss: 0.4356\n",
      "Epoch 11/51\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8492 - loss: 0.3842 - val_accuracy: 0.8548 - val_loss: 0.4179\n",
      "Epoch 12/51\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8454 - loss: 0.3606 - val_accuracy: 0.8548 - val_loss: 0.3904\n",
      "Epoch 13/51\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8708 - loss: 0.3423 - val_accuracy: 0.8548 - val_loss: 0.3855\n",
      "Epoch 14/51\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8577 - loss: 0.3236 - val_accuracy: 0.8548 - val_loss: 0.3649\n",
      "Epoch 15/51\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8768 - loss: 0.3102 - val_accuracy: 0.8548 - val_loss: 0.3574\n",
      "Epoch 16/51\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8909 - loss: 0.3029 - val_accuracy: 0.8548 - val_loss: 0.3449\n",
      "Epoch 17/51\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8964 - loss: 0.2760 - val_accuracy: 0.8548 - val_loss: 0.3358\n",
      "Epoch 18/51\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8873 - loss: 0.2807 - val_accuracy: 0.8548 - val_loss: 0.3396\n",
      "Epoch 19/51\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8948 - loss: 0.2495 - val_accuracy: 0.8710 - val_loss: 0.3133\n",
      "Epoch 20/51\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8997 - loss: 0.2644 - val_accuracy: 0.8548 - val_loss: 0.3226\n",
      "Epoch 21/51\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8922 - loss: 0.2574 - val_accuracy: 0.8710 - val_loss: 0.3054\n",
      "Epoch 22/51\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8855 - loss: 0.2559 - val_accuracy: 0.8710 - val_loss: 0.2916\n",
      "Epoch 23/51\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9275 - loss: 0.2209 - val_accuracy: 0.8548 - val_loss: 0.2977\n",
      "Epoch 24/51\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9141 - loss: 0.2269 - val_accuracy: 0.8871 - val_loss: 0.2712\n",
      "Epoch 25/51\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9440 - loss: 0.2187 - val_accuracy: 0.8710 - val_loss: 0.2791\n",
      "Epoch 26/51\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9270 - loss: 0.2168 - val_accuracy: 0.8871 - val_loss: 0.2590\n",
      "Epoch 27/51\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9224 - loss: 0.2113 - val_accuracy: 0.8710 - val_loss: 0.2640\n",
      "Epoch 28/51\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9158 - loss: 0.2259 - val_accuracy: 0.8871 - val_loss: 0.2580\n",
      "Epoch 29/51\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9135 - loss: 0.2196 - val_accuracy: 0.8871 - val_loss: 0.2508\n",
      "Epoch 30/51\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9256 - loss: 0.1948 - val_accuracy: 0.8710 - val_loss: 0.2504\n",
      "Epoch 31/51\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9374 - loss: 0.1927 - val_accuracy: 0.8710 - val_loss: 0.2428\n",
      "Epoch 32/51\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9563 - loss: 0.1953 - val_accuracy: 0.8871 - val_loss: 0.2407\n",
      "Epoch 33/51\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9410 - loss: 0.1872 - val_accuracy: 0.8871 - val_loss: 0.2287\n",
      "Epoch 34/51\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9363 - loss: 0.1759 - val_accuracy: 0.8871 - val_loss: 0.2231\n",
      "Epoch 35/51\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9585 - loss: 0.1706 - val_accuracy: 0.8871 - val_loss: 0.2098\n",
      "Epoch 36/51\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9523 - loss: 0.1792 - val_accuracy: 0.8871 - val_loss: 0.2163\n",
      "Epoch 37/51\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9509 - loss: 0.1690 - val_accuracy: 0.8871 - val_loss: 0.2073\n",
      "Epoch 38/51\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9574 - loss: 0.1651 - val_accuracy: 0.8871 - val_loss: 0.2014\n",
      "Epoch 39/51\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9505 - loss: 0.1544 - val_accuracy: 0.8871 - val_loss: 0.1976\n",
      "Epoch 40/51\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9496 - loss: 0.1549 - val_accuracy: 0.9032 - val_loss: 0.2039\n",
      "Epoch 41/51\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9485 - loss: 0.1651 - val_accuracy: 0.9032 - val_loss: 0.1923\n",
      "Epoch 42/51\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9641 - loss: 0.1399 - val_accuracy: 0.9032 - val_loss: 0.1958\n",
      "Epoch 43/51\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9602 - loss: 0.1471 - val_accuracy: 0.9194 - val_loss: 0.1769\n",
      "Epoch 44/51\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9533 - loss: 0.1488 - val_accuracy: 0.9032 - val_loss: 0.1898\n",
      "Epoch 45/51\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9521 - loss: 0.1420 - val_accuracy: 0.9194 - val_loss: 0.1780\n",
      "Epoch 46/51\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9611 - loss: 0.1352 - val_accuracy: 0.9194 - val_loss: 0.1741\n",
      "Epoch 47/51\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9488 - loss: 0.1373 - val_accuracy: 0.9032 - val_loss: 0.1753\n",
      "Epoch 48/51\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9689 - loss: 0.1355 - val_accuracy: 0.9194 - val_loss: 0.1772\n",
      "Epoch 49/51\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9650 - loss: 0.1335 - val_accuracy: 0.9355 - val_loss: 0.1626\n",
      "Epoch 50/51\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9551 - loss: 0.1249 - val_accuracy: 0.9355 - val_loss: 0.1643\n",
      "Epoch 51/51\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9471 - loss: 0.1248 - val_accuracy: 0.9355 - val_loss: 0.1710\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Evaluation =====\n",
      "Accuracy: 0.8563\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5714    0.2353    0.3333        17\n",
      "           1     0.7091    0.8667    0.7800        45\n",
      "           2     0.9794    0.9406    0.9596       101\n",
      "           3     0.7333    1.0000    0.8462        11\n",
      "\n",
      "    accuracy                         0.8563       174\n",
      "   macro avg     0.7483    0.7606    0.7298       174\n",
      "weighted avg     0.8541    0.8563    0.8448       174\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 4 12  0  1]\n",
      " [ 3 39  2  1]\n",
      " [ 0  4 95  2]\n",
      " [ 0  0  0 11]]\n",
      "\n",
      "===== Model Metrics =====\n",
      "Training Time: 8.09s\n",
      "Inference/sample: 0.002836s\n",
      "Testing Time: 0.49s\n",
      "Model Size: 0.23 MB\n",
      "Trainable Params: 55,759\n",
      "RAM Usage: 1495.27 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import psutil\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout\n",
    "import tensorflow as tf\n",
    "from bayes_opt import BayesianOptimization\n",
    "from scipy.fftpack import fft\n",
    "import warnings\n",
    "\n",
    "# Set seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================\n",
    "# 1. Data Loading and Preprocessing\n",
    "# ============================================\n",
    "def load_and_preprocess_data(filepath):\n",
    "    df = pd.read_csv(filepath, delimiter=\",\", low_memory=False)\n",
    "    df.columns = df.columns.str.replace('\"', '').str.strip()\n",
    "\n",
    "    if 'TIME' not in df.columns:\n",
    "        raise KeyError(\"The column 'TIME' does not exist in the dataset.\")\n",
    "\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'], errors='coerce')\n",
    "    df.dropna(subset=['TIME'], inplace=True)\n",
    "    df.set_index('TIME', inplace=True)\n",
    "\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "    target_col = 'fenologia_h1'\n",
    "    if target_col not in df.columns:\n",
    "        raise KeyError(f\"The target column '{target_col}' does not exist.\")\n",
    "\n",
    "    df[target_col].interpolate(method='linear', inplace=True)\n",
    "    df[target_col] = df[target_col] - 1\n",
    "\n",
    "    for i in range(1, 4):\n",
    "        df[f'{target_col}_lag{i}'] = df[target_col].shift(i)\n",
    "\n",
    "    for window in [3, 6]:\n",
    "        df[f'{target_col}_roll_mean_{window}'] = df[target_col].rolling(window).mean()\n",
    "        df[f'{target_col}_roll_std_{window}'] = df[target_col].rolling(window).std()\n",
    "\n",
    "    # df['month'] = df.index.month\n",
    "    # df['weekofyear'] = df.index.isocalendar().week\n",
    "    # df['year'] = df.index.isocalendar().year\n",
    "    # df['sin_week'] = np.sin(2 * np.pi * df['weekofyear'] / 52)\n",
    "    # df['cos_week'] = np.cos(2 * np.pi * df['weekofyear'] / 52)\n",
    "    # df['EMA_3'] = df[target_col].ewm(span=3, adjust=False).mean()\n",
    "    # df['EMA_6'] = df[target_col].ewm(span=6, adjust=False).mean()\n",
    "    # df['correlation_target_month'] = df[target_col].rolling(window=6).corr(df['month'])\n",
    "    # df['correlation_target_week'] = df[target_col].rolling(window=6).corr(df['weekofyear'])\n",
    "\n",
    "    # fft_values = fft(df[target_col].dropna().values)\n",
    "    # fft_real = np.real(fft_values)[:len(df[target_col])]\n",
    "    # fft_imag = np.imag(fft_values)[:len(df[target_col])]\n",
    "    # df['fft_real'] = np.concatenate([fft_real, np.nan * np.ones(len(df) - len(fft_real))])\n",
    "    # df['fft_imag'] = np.concatenate([fft_imag, np.nan * np.ones(len(df) - len(fft_imag))])\n",
    "\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "    return df, target_col\n",
    "\n",
    "# ============================================\n",
    "# 2. Feature Scaling\n",
    "# ============================================\n",
    "def feature_scaling(df, target_col):\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))  # 3D for RNN\n",
    "    return X_scaled, y, scaler\n",
    "\n",
    "# ============================================\n",
    "# 3. Build GRU Model\n",
    "# ============================================\n",
    "def build_gru_model(input_shape, num_classes=4, num_units=64, dropout_rate=0.2):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(num_units, input_shape=input_shape))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(num_units // 2, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# ============================================\n",
    "# 4. Bayesian Optimization Objective\n",
    "# ============================================\n",
    "def objective_function(num_units, dropout_rate, epochs, batch_size, X_train, y_train):\n",
    "    model = build_gru_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        num_classes=4,\n",
    "        num_units=int(num_units),\n",
    "        dropout_rate=dropout_rate\n",
    "    )\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=int(epochs),\n",
    "        batch_size=int(batch_size),\n",
    "        validation_split=0.2,\n",
    "        verbose=0\n",
    "    )\n",
    "    return np.max(history.history['val_accuracy'])\n",
    "\n",
    "# ============================================\n",
    "# 5. Optimize Hyperparameters\n",
    "# ============================================\n",
    "def optimize_hyperparameters(X_train, y_train):\n",
    "    pbounds = {\n",
    "        'num_units': (32, 128),\n",
    "        'dropout_rate': (0.1, 0.5),\n",
    "        'epochs': (50, 100),\n",
    "        'batch_size': (16, 64)\n",
    "    }\n",
    "\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=lambda num_units, dropout_rate, epochs, batch_size: objective_function(\n",
    "            num_units, dropout_rate, epochs, batch_size, X_train, y_train\n",
    "        ),\n",
    "        pbounds=pbounds,\n",
    "        random_state=42,\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    optimizer.maximize(init_points=5, n_iter=15)\n",
    "    return optimizer.max['params']\n",
    "\n",
    "# ============================================\n",
    "# 6. Train & Evaluate Model\n",
    "# ============================================\n",
    "def train_and_evaluate_model(X_train, X_test, y_train, y_test, best_params):\n",
    "    model = build_gru_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        num_classes=4,\n",
    "        num_units=int(best_params['num_units']),\n",
    "        dropout_rate=float(best_params['dropout_rate'])\n",
    "    )\n",
    "\n",
    "    start_train = time.time()\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=int(best_params['epochs']),\n",
    "        batch_size=int(best_params['batch_size']),\n",
    "        validation_split=0.2,\n",
    "        verbose=1\n",
    "    )\n",
    "    training_time = time.time() - start_train\n",
    "\n",
    "    start_infer = time.time()\n",
    "    y_pred = model.predict(X_test).argmax(axis=1)\n",
    "    inference_time = (time.time() - start_infer) / len(X_test)\n",
    "\n",
    "    # Calculate total testing time\n",
    "    testing_time = time.time() - start_infer\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(\"\\n===== Evaluation =====\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(\"Report:\\n\", classification_report(y_test, y_pred, digits=4))\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "    model.save(\"temp_model.h5\", include_optimizer=False)\n",
    "    model_size = os.path.getsize(\"temp_model.h5\") / (1024 ** 2)\n",
    "    os.remove(\"temp_model.h5\")\n",
    "\n",
    "    total_params = np.sum([np.prod(v.shape) for v in model.trainable_weights])\n",
    "    ram_usage = psutil.Process(os.getpid()).memory_info().rss / (1024 ** 2)\n",
    "\n",
    "    print(\"\\n===== Model Metrics =====\")\n",
    "    print(f\"Training Time: {training_time:.2f}s\")\n",
    "    print(f\"Inference/sample: {inference_time:.6f}s\")\n",
    "    print(f\"Testing Time: {testing_time:.2f}s\")\n",
    "    print(f\"Model Size: {model_size:.2f} MB\")\n",
    "    print(f\"Trainable Params: {total_params:,}\")\n",
    "    print(f\"RAM Usage: {ram_usage:.2f} MB\")\n",
    "\n",
    "    return model, history\n",
    "\n",
    "# ============================================\n",
    "# 7. Main\n",
    "# ============================================\n",
    "def main():\n",
    "    train_path = r\"E:\\Abroad period research\\Phenology datasets\\PHENOLOGY_H1\\Cordoba_train.csv\"\n",
    "    test_path = r\"E:\\Abroad period research\\Phenology datasets\\PHENOLOGY_H1\\Cordoba_test.csv\"\n",
    "\n",
    "    df_train, target_col = load_and_preprocess_data(train_path)\n",
    "    X_train, y_train, _ = feature_scaling(df_train, target_col)\n",
    "\n",
    "    df_test, _ = load_and_preprocess_data(test_path)\n",
    "    X_test, y_test, _ = feature_scaling(df_test, target_col)\n",
    "\n",
    "    print(\"\\n Optimizing Hyperparameters...\")\n",
    "    best_params = optimize_hyperparameters(X_train, y_train)\n",
    "    print(\"\\n Best Params:\", best_params)\n",
    "\n",
    "    print(\"\\n Training Final GRU Model...\")\n",
    "    model, history = train_and_evaluate_model(X_train, X_test, y_train, y_test, best_params)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df5b4c7",
   "metadata": {},
   "source": [
    "Jaen station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "779c131c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Optimizing Hyperparameters...\n",
      "|   iter    |  target   | batch_... | dropou... |  epochs   | num_units |\n",
      "-------------------------------------------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.98     \u001b[39m | \u001b[39m33.98    \u001b[39m | \u001b[39m0.4803   \u001b[39m | \u001b[39m86.6     \u001b[39m | \u001b[39m89.47    \u001b[39m |\n",
      "| \u001b[35m2        \u001b[39m | \u001b[35m1.0      \u001b[39m | \u001b[35m23.49    \u001b[39m | \u001b[35m0.1624   \u001b[39m | \u001b[35m52.9     \u001b[39m | \u001b[35m115.2    \u001b[39m |\n",
      "| \u001b[39m3        \u001b[39m | \u001b[39m0.96     \u001b[39m | \u001b[39m44.85    \u001b[39m | \u001b[39m0.3832   \u001b[39m | \u001b[39m51.03    \u001b[39m | \u001b[39m125.1    \u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m0.94     \u001b[39m | \u001b[39m55.96    \u001b[39m | \u001b[39m0.1849   \u001b[39m | \u001b[39m59.09    \u001b[39m | \u001b[39m49.61    \u001b[39m |\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m0.96     \u001b[39m | \u001b[39m30.6     \u001b[39m | \u001b[39m0.3099   \u001b[39m | \u001b[39m71.6     \u001b[39m | \u001b[39m59.96    \u001b[39m |\n",
      "| \u001b[39m6        \u001b[39m | \u001b[39m0.98     \u001b[39m | \u001b[39m22.97    \u001b[39m | \u001b[39m0.3185   \u001b[39m | \u001b[39m53.09    \u001b[39m | \u001b[39m116.6    \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m22.45    \u001b[39m | \u001b[39m0.1024   \u001b[39m | \u001b[39m51.48    \u001b[39m | \u001b[39m115.3    \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m23.3     \u001b[39m | \u001b[39m0.1324   \u001b[39m | \u001b[39m52.36    \u001b[39m | \u001b[39m112.9    \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m0.98     \u001b[39m | \u001b[39m27.35    \u001b[39m | \u001b[39m0.4989   \u001b[39m | \u001b[39m53.15    \u001b[39m | \u001b[39m113.9    \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m0.94     \u001b[39m | \u001b[39m20.14    \u001b[39m | \u001b[39m0.4617   \u001b[39m | \u001b[39m51.11    \u001b[39m | \u001b[39m112.8    \u001b[39m |\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m0.96     \u001b[39m | \u001b[39m23.88    \u001b[39m | \u001b[39m0.2827   \u001b[39m | \u001b[39m50.69    \u001b[39m | \u001b[39m115.4    \u001b[39m |\n",
      "| \u001b[39m12       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m24.7     \u001b[39m | \u001b[39m0.2247   \u001b[39m | \u001b[39m53.66    \u001b[39m | \u001b[39m114.7    \u001b[39m |\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m23.49    \u001b[39m | \u001b[39m0.3391   \u001b[39m | \u001b[39m55.25    \u001b[39m | \u001b[39m113.5    \u001b[39m |\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m22.25    \u001b[39m | \u001b[39m0.229    \u001b[39m | \u001b[39m52.66    \u001b[39m | \u001b[39m114.2    \u001b[39m |\n",
      "| \u001b[39m15       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m25.06    \u001b[39m | \u001b[39m0.1906   \u001b[39m | \u001b[39m54.15    \u001b[39m | \u001b[39m112.8    \u001b[39m |\n",
      "| \u001b[39m16       \u001b[39m | \u001b[39m0.98     \u001b[39m | \u001b[39m23.09    \u001b[39m | \u001b[39m0.3106   \u001b[39m | \u001b[39m53.84    \u001b[39m | \u001b[39m111.5    \u001b[39m |\n",
      "| \u001b[39m17       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m24.91    \u001b[39m | \u001b[39m0.1274   \u001b[39m | \u001b[39m57.33    \u001b[39m | \u001b[39m113.2    \u001b[39m |\n",
      "| \u001b[39m18       \u001b[39m | \u001b[39m0.98     \u001b[39m | \u001b[39m22.46    \u001b[39m | \u001b[39m0.4907   \u001b[39m | \u001b[39m57.54    \u001b[39m | \u001b[39m113.3    \u001b[39m |\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m26.99    \u001b[39m | \u001b[39m0.1665   \u001b[39m | \u001b[39m57.71    \u001b[39m | \u001b[39m111.9    \u001b[39m |\n",
      "| \u001b[39m20       \u001b[39m | \u001b[39m0.98     \u001b[39m | \u001b[39m25.8     \u001b[39m | \u001b[39m0.3092   \u001b[39m | \u001b[39m55.89    \u001b[39m | \u001b[39m113.8    \u001b[39m |\n",
      "=========================================================================\n",
      "\n",
      " Best Params: {'batch_size': 23.488894741236955, 'dropout_rate': 0.16239780813448107, 'epochs': 52.90418060840997, 'num_units': 115.15290999439378}\n",
      "\n",
      " Training Final GRU Model...\n",
      "Epoch 1/52\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 112ms/step - accuracy: 0.3706 - loss: 1.3752 - val_accuracy: 0.6000 - val_loss: 1.1227\n",
      "Epoch 2/52\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5738 - loss: 1.0825 - val_accuracy: 0.6000 - val_loss: 0.9424\n",
      "Epoch 3/52\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.5675 - loss: 0.9249 - val_accuracy: 0.6400 - val_loss: 0.8573\n",
      "Epoch 4/52\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.6509 - loss: 0.8045 - val_accuracy: 0.6600 - val_loss: 0.7662\n",
      "Epoch 5/52\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7503 - loss: 0.6971 - val_accuracy: 0.6600 - val_loss: 0.6525\n",
      "Epoch 6/52\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7833 - loss: 0.5743 - val_accuracy: 0.7400 - val_loss: 0.5501\n",
      "Epoch 7/52\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8102 - loss: 0.4866 - val_accuracy: 0.8000 - val_loss: 0.4596\n",
      "Epoch 8/52\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8638 - loss: 0.4108 - val_accuracy: 0.8200 - val_loss: 0.3884\n",
      "Epoch 9/52\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8757 - loss: 0.3500 - val_accuracy: 0.8200 - val_loss: 0.3319\n",
      "Epoch 10/52\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8716 - loss: 0.3053 - val_accuracy: 0.8600 - val_loss: 0.2899\n",
      "Epoch 11/52\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9034 - loss: 0.2638 - val_accuracy: 0.8600 - val_loss: 0.2634\n",
      "Epoch 12/52\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9100 - loss: 0.2432 - val_accuracy: 0.9200 - val_loss: 0.2353\n",
      "Epoch 13/52\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9297 - loss: 0.2258 - val_accuracy: 0.9400 - val_loss: 0.2096\n",
      "Epoch 14/52\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9424 - loss: 0.2063 - val_accuracy: 0.9400 - val_loss: 0.1989\n",
      "Epoch 15/52\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9206 - loss: 0.1852 - val_accuracy: 0.9400 - val_loss: 0.1866\n",
      "Epoch 16/52\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9324 - loss: 0.1668 - val_accuracy: 0.9400 - val_loss: 0.1761\n",
      "Epoch 17/52\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9468 - loss: 0.1650 - val_accuracy: 0.9600 - val_loss: 0.1634\n",
      "Epoch 18/52\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9495 - loss: 0.1513 - val_accuracy: 0.9600 - val_loss: 0.1574\n",
      "Epoch 19/52\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9485 - loss: 0.1425 - val_accuracy: 0.9600 - val_loss: 0.1533\n",
      "Epoch 20/52\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9308 - loss: 0.1312 - val_accuracy: 0.9600 - val_loss: 0.1482\n",
      "Epoch 21/52\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9549 - loss: 0.1157 - val_accuracy: 0.9600 - val_loss: 0.1380\n",
      "Epoch 22/52\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9448 - loss: 0.1208 - val_accuracy: 0.9600 - val_loss: 0.1337\n",
      "Epoch 23/52\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9618 - loss: 0.1094 - val_accuracy: 0.9600 - val_loss: 0.1407\n",
      "Epoch 24/52\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9553 - loss: 0.1160 - val_accuracy: 0.9600 - val_loss: 0.1367\n",
      "Epoch 25/52\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9746 - loss: 0.1059 - val_accuracy: 0.9600 - val_loss: 0.1265\n",
      "Epoch 26/52\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9642 - loss: 0.0964 - val_accuracy: 0.9600 - val_loss: 0.1258\n",
      "Epoch 27/52\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9549 - loss: 0.0945 - val_accuracy: 0.9600 - val_loss: 0.1234\n",
      "Epoch 28/52\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9808 - loss: 0.0901 - val_accuracy: 0.9600 - val_loss: 0.1179\n",
      "Epoch 29/52\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9719 - loss: 0.0902 - val_accuracy: 0.9600 - val_loss: 0.1123\n",
      "Epoch 30/52\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9595 - loss: 0.0819 - val_accuracy: 0.9600 - val_loss: 0.1044\n",
      "Epoch 31/52\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9533 - loss: 0.0829 - val_accuracy: 0.9600 - val_loss: 0.0982\n",
      "Epoch 32/52\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9644 - loss: 0.0763 - val_accuracy: 0.9600 - val_loss: 0.1005\n",
      "Epoch 33/52\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9932 - loss: 0.0675 - val_accuracy: 0.9600 - val_loss: 0.1000\n",
      "Epoch 34/52\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9585 - loss: 0.0808 - val_accuracy: 0.9600 - val_loss: 0.0974\n",
      "Epoch 35/52\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9847 - loss: 0.0617 - val_accuracy: 0.9600 - val_loss: 0.0871\n",
      "Epoch 36/52\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9847 - loss: 0.0648 - val_accuracy: 0.9600 - val_loss: 0.0850\n",
      "Epoch 37/52\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9932 - loss: 0.0552 - val_accuracy: 0.9600 - val_loss: 0.0870\n",
      "Epoch 38/52\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9932 - loss: 0.0636 - val_accuracy: 0.9600 - val_loss: 0.0837\n",
      "Epoch 39/52\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9743 - loss: 0.0596 - val_accuracy: 0.9800 - val_loss: 0.0779\n",
      "Epoch 40/52\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9932 - loss: 0.0614 - val_accuracy: 0.9800 - val_loss: 0.0750\n",
      "Epoch 41/52\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9729 - loss: 0.0668 - val_accuracy: 0.9600 - val_loss: 0.0747\n",
      "Epoch 42/52\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9942 - loss: 0.0549 - val_accuracy: 0.9800 - val_loss: 0.0664\n",
      "Epoch 43/52\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9942 - loss: 0.0512 - val_accuracy: 0.9800 - val_loss: 0.0586\n",
      "Epoch 44/52\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9724 - loss: 0.0527 - val_accuracy: 0.9800 - val_loss: 0.0577\n",
      "Epoch 45/52\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9942 - loss: 0.0441 - val_accuracy: 0.9800 - val_loss: 0.0557\n",
      "Epoch 46/52\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9843 - loss: 0.0462 - val_accuracy: 0.9800 - val_loss: 0.0508\n",
      "Epoch 47/52\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9971 - loss: 0.0439 - val_accuracy: 0.9800 - val_loss: 0.0517\n",
      "Epoch 48/52\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9758 - loss: 0.0516 - val_accuracy: 0.9800 - val_loss: 0.0603\n",
      "Epoch 49/52\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9857 - loss: 0.0486 - val_accuracy: 0.9800 - val_loss: 0.0559\n",
      "Epoch 50/52\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9932 - loss: 0.0355 - val_accuracy: 1.0000 - val_loss: 0.0452\n",
      "Epoch 51/52\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9942 - loss: 0.0464 - val_accuracy: 1.0000 - val_loss: 0.0470\n",
      "Epoch 52/52\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9857 - loss: 0.0415 - val_accuracy: 1.0000 - val_loss: 0.0434\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Evaluation =====\n",
      "Accuracy: 0.9402\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.3846    0.5556        13\n",
      "           1     0.8868    0.9400    0.9126        50\n",
      "           2     0.9680    1.0000    0.9837       121\n",
      "           3     0.0000    0.0000    0.0000         0\n",
      "\n",
      "    accuracy                         0.9402       184\n",
      "   macro avg     0.7137    0.5812    0.6130       184\n",
      "weighted avg     0.9482    0.9402    0.9342       184\n",
      "\n",
      "Confusion Matrix:\n",
      " [[  5   6   1   1]\n",
      " [  0  47   3   0]\n",
      " [  0   0 121   0]\n",
      " [  0   0   0   0]]\n",
      "\n",
      "===== Model Metrics =====\n",
      "Training Time: 16.23s\n",
      "Inference/sample: 0.003573s\n",
      "Testing Time: 0.66s\n",
      "Model Size: 0.24 MB\n",
      "Trainable Params: 56,179\n",
      "RAM Usage: 2838.71 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import psutil\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout\n",
    "import tensorflow as tf\n",
    "from bayes_opt import BayesianOptimization\n",
    "from scipy.fftpack import fft\n",
    "import warnings\n",
    "\n",
    "# Set seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================\n",
    "# 1. Data Loading and Preprocessing\n",
    "# ============================================\n",
    "def load_and_preprocess_data(filepath):\n",
    "    df = pd.read_csv(filepath, delimiter=\",\", low_memory=False)\n",
    "    df.columns = df.columns.str.replace('\"', '').str.strip()\n",
    "\n",
    "    if 'TIME' not in df.columns:\n",
    "        raise KeyError(\"The column 'TIME' does not exist in the dataset.\")\n",
    "\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'], errors='coerce')\n",
    "    df.dropna(subset=['TIME'], inplace=True)\n",
    "    df.set_index('TIME', inplace=True)\n",
    "\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "    target_col = 'fenologia_h1'\n",
    "    if target_col not in df.columns:\n",
    "        raise KeyError(f\"The target column '{target_col}' does not exist.\")\n",
    "\n",
    "    df[target_col].interpolate(method='linear', inplace=True)\n",
    "    df[target_col] = df[target_col] - 1\n",
    "\n",
    "    for i in range(1, 4):\n",
    "        df[f'{target_col}_lag{i}'] = df[target_col].shift(i)\n",
    "\n",
    "    for window in [3, 6]:\n",
    "        df[f'{target_col}_roll_mean_{window}'] = df[target_col].rolling(window).mean()\n",
    "        df[f'{target_col}_roll_std_{window}'] = df[target_col].rolling(window).std()\n",
    "\n",
    "    df['month'] = df.index.month\n",
    "    df['weekofyear'] = df.index.isocalendar().week\n",
    "    df['year'] = df.index.isocalendar().year\n",
    "    df['sin_week'] = np.sin(2 * np.pi * df['weekofyear'] / 52)\n",
    "    df['cos_week'] = np.cos(2 * np.pi * df['weekofyear'] / 52)\n",
    "    df['EMA_3'] = df[target_col].ewm(span=3, adjust=False).mean()\n",
    "    df['EMA_6'] = df[target_col].ewm(span=6, adjust=False).mean()\n",
    "    df['correlation_target_month'] = df[target_col].rolling(window=6).corr(df['month'])\n",
    "    df['correlation_target_week'] = df[target_col].rolling(window=6).corr(df['weekofyear'])\n",
    "\n",
    "    # fft_values = fft(df[target_col].dropna().values)\n",
    "    # fft_real = np.real(fft_values)[:len(df[target_col])]\n",
    "    # fft_imag = np.imag(fft_values)[:len(df[target_col])]\n",
    "    # df['fft_real'] = np.concatenate([fft_real, np.nan * np.ones(len(df) - len(fft_real))])\n",
    "    # df['fft_imag'] = np.concatenate([fft_imag, np.nan * np.ones(len(df) - len(fft_imag))])\n",
    "\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "    return df, target_col\n",
    "\n",
    "# ============================================\n",
    "# 2. Feature Scaling\n",
    "# ============================================\n",
    "def feature_scaling(df, target_col):\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))  # 3D for RNN\n",
    "    return X_scaled, y, scaler\n",
    "\n",
    "# ============================================\n",
    "# 3. Build GRU Model\n",
    "# ============================================\n",
    "def build_gru_model(input_shape, num_classes=4, num_units=64, dropout_rate=0.2):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(num_units, input_shape=input_shape))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(num_units // 2, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# ============================================\n",
    "# 4. Bayesian Optimization Objective\n",
    "# ============================================\n",
    "def objective_function(num_units, dropout_rate, epochs, batch_size, X_train, y_train):\n",
    "    model = build_gru_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        num_classes=4,\n",
    "        num_units=int(num_units),\n",
    "        dropout_rate=dropout_rate\n",
    "    )\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=int(epochs),\n",
    "        batch_size=int(batch_size),\n",
    "        validation_split=0.2,\n",
    "        verbose=0\n",
    "    )\n",
    "    return np.max(history.history['val_accuracy'])\n",
    "\n",
    "# ============================================\n",
    "# 5. Optimize Hyperparameters\n",
    "# ============================================\n",
    "def optimize_hyperparameters(X_train, y_train):\n",
    "    pbounds = {\n",
    "        'num_units': (32, 128),\n",
    "        'dropout_rate': (0.1, 0.5),\n",
    "        'epochs': (50, 100),\n",
    "        'batch_size': (16, 64)\n",
    "    }\n",
    "\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=lambda num_units, dropout_rate, epochs, batch_size: objective_function(\n",
    "            num_units, dropout_rate, epochs, batch_size, X_train, y_train\n",
    "        ),\n",
    "        pbounds=pbounds,\n",
    "        random_state=42,\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    optimizer.maximize(init_points=5, n_iter=15)\n",
    "    return optimizer.max['params']\n",
    "\n",
    "# ============================================\n",
    "# 6. Train & Evaluate Model\n",
    "# ============================================\n",
    "def train_and_evaluate_model(X_train, X_test, y_train, y_test, best_params):\n",
    "    model = build_gru_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        num_classes=4,\n",
    "        num_units=int(best_params['num_units']),\n",
    "        dropout_rate=float(best_params['dropout_rate'])\n",
    "    )\n",
    "\n",
    "    start_train = time.time()\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=int(best_params['epochs']),\n",
    "        batch_size=int(best_params['batch_size']),\n",
    "        validation_split=0.2,\n",
    "        verbose=1\n",
    "    )\n",
    "    training_time = time.time() - start_train\n",
    "\n",
    "    start_infer = time.time()\n",
    "    y_pred = model.predict(X_test).argmax(axis=1)\n",
    "    inference_time = (time.time() - start_infer) / len(X_test)\n",
    "\n",
    "    # Calculate total testing time\n",
    "    testing_time = time.time() - start_infer\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(\"\\n===== Evaluation =====\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(\"Report:\\n\", classification_report(y_test, y_pred, digits=4))\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "    model.save(\"temp_model.h5\", include_optimizer=False)\n",
    "    model_size = os.path.getsize(\"temp_model.h5\") / (1024 ** 2)\n",
    "    os.remove(\"temp_model.h5\")\n",
    "\n",
    "    total_params = np.sum([np.prod(v.shape) for v in model.trainable_weights])\n",
    "    ram_usage = psutil.Process(os.getpid()).memory_info().rss / (1024 ** 2)\n",
    "\n",
    "    print(\"\\n===== Model Metrics =====\")\n",
    "    print(f\"Training Time: {training_time:.2f}s\")\n",
    "    print(f\"Inference/sample: {inference_time:.6f}s\")\n",
    "    print(f\"Testing Time: {testing_time:.2f}s\")\n",
    "    print(f\"Model Size: {model_size:.2f} MB\")\n",
    "    print(f\"Trainable Params: {total_params:,}\")\n",
    "    print(f\"RAM Usage: {ram_usage:.2f} MB\")\n",
    "\n",
    "    return model, history\n",
    "\n",
    "# ============================================\n",
    "# 7. Main\n",
    "# ============================================\n",
    "def main():\n",
    "    train_path = r\"E:\\Abroad period research\\Phenology datasets\\PHENOLOGY_H1\\jaen_train.csv\"\n",
    "    test_path = r\"E:\\Abroad period research\\Phenology datasets\\PHENOLOGY_H1\\jaen_test.csv\"\n",
    "\n",
    "    df_train, target_col = load_and_preprocess_data(train_path)\n",
    "    X_train, y_train, _ = feature_scaling(df_train, target_col)\n",
    "\n",
    "    df_test, _ = load_and_preprocess_data(test_path)\n",
    "    X_test, y_test, _ = feature_scaling(df_test, target_col)\n",
    "\n",
    "    print(\"\\n Optimizing Hyperparameters...\")\n",
    "    best_params = optimize_hyperparameters(X_train, y_train)\n",
    "    print(\"\\n Best Params:\", best_params)\n",
    "\n",
    "    print(\"\\n Training Final GRU Model...\")\n",
    "    model, history = train_and_evaluate_model(X_train, X_test, y_train, y_test, best_params)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab46e7a1",
   "metadata": {},
   "source": [
    "Sevilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9545bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Optimizing Hyperparameters...\n",
      "|   iter    |  target   | batch_... | dropou... |  epochs   | num_units |\n",
      "-------------------------------------------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m33.98    \u001b[39m | \u001b[39m0.4803   \u001b[39m | \u001b[39m86.6     \u001b[39m | \u001b[39m89.47    \u001b[39m |\n",
      "| \u001b[39m2        \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m23.49    \u001b[39m | \u001b[39m0.1624   \u001b[39m | \u001b[39m52.9     \u001b[39m | \u001b[39m115.2    \u001b[39m |\n",
      "| \u001b[39m3        \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m44.85    \u001b[39m | \u001b[39m0.3832   \u001b[39m | \u001b[39m51.03    \u001b[39m | \u001b[39m125.1    \u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m0.9857   \u001b[39m | \u001b[39m55.96    \u001b[39m | \u001b[39m0.1849   \u001b[39m | \u001b[39m59.09    \u001b[39m | \u001b[39m49.61    \u001b[39m |\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m30.6     \u001b[39m | \u001b[39m0.3099   \u001b[39m | \u001b[39m71.6     \u001b[39m | \u001b[39m59.96    \u001b[39m |\n",
      "| \u001b[39m6        \u001b[39m | \u001b[39m0.9857   \u001b[39m | \u001b[39m31.85    \u001b[39m | \u001b[39m0.4144   \u001b[39m | \u001b[39m54.7     \u001b[39m | \u001b[39m125.6    \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m18.36    \u001b[39m | \u001b[39m0.1074   \u001b[39m | \u001b[39m80.03    \u001b[39m | \u001b[39m101.5    \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m16.21    \u001b[39m | \u001b[39m0.1161   \u001b[39m | \u001b[39m73.32    \u001b[39m | \u001b[39m65.8     \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m0.9857   \u001b[39m | \u001b[39m62.6     \u001b[39m | \u001b[39m0.3838   \u001b[39m | \u001b[39m99.69    \u001b[39m | \u001b[39m67.9     \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m0.9857   \u001b[39m | \u001b[39m38.63    \u001b[39m | \u001b[39m0.1642   \u001b[39m | \u001b[39m90.1     \u001b[39m | \u001b[39m41.26    \u001b[39m |\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m61.04    \u001b[39m | \u001b[39m0.3825   \u001b[39m | \u001b[39m72.51    \u001b[39m | \u001b[39m127.9    \u001b[39m |\n",
      "| \u001b[39m12       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m30.67    \u001b[39m | \u001b[39m0.1671   \u001b[39m | \u001b[39m72.5     \u001b[39m | \u001b[39m59.03    \u001b[39m |\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m26.98    \u001b[39m | \u001b[39m0.4583   \u001b[39m | \u001b[39m79.16    \u001b[39m | \u001b[39m94.8     \u001b[39m |\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m57.54    \u001b[39m | \u001b[39m0.2872   \u001b[39m | \u001b[39m56.49    \u001b[39m | \u001b[39m125.7    \u001b[39m |\n",
      "| \u001b[39m15       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m17.2     \u001b[39m | \u001b[39m0.2267   \u001b[39m | \u001b[39m60.04    \u001b[39m | \u001b[39m105.8    \u001b[39m |\n",
      "| \u001b[39m16       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m24.39    \u001b[39m | \u001b[39m0.3783   \u001b[39m | \u001b[39m81.28    \u001b[39m | \u001b[39m77.93    \u001b[39m |\n",
      "| \u001b[39m17       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m29.45    \u001b[39m | \u001b[39m0.4584   \u001b[39m | \u001b[39m50.34    \u001b[39m | \u001b[39m100.9    \u001b[39m |\n",
      "| \u001b[39m18       \u001b[39m | \u001b[39m0.9857   \u001b[39m | \u001b[39m51.77    \u001b[39m | \u001b[39m0.3945   \u001b[39m | \u001b[39m50.44    \u001b[39m | \u001b[39m111.0    \u001b[39m |\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m20.52    \u001b[39m | \u001b[39m0.2804   \u001b[39m | \u001b[39m90.4     \u001b[39m | \u001b[39m90.35    \u001b[39m |\n",
      "| \u001b[39m20       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m17.05    \u001b[39m | \u001b[39m0.3284   \u001b[39m | \u001b[39m68.54    \u001b[39m | \u001b[39m89.23    \u001b[39m |\n",
      "=========================================================================\n",
      "\n",
      " Best Params: {'batch_size': 33.977925704673396, 'dropout_rate': 0.4802857225639665, 'epochs': 86.59969709057026, 'num_units': 89.47121448291551}\n",
      "\n",
      " Training Final GRU Model...\n",
      "Epoch 1/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - accuracy: 0.4767 - loss: 1.2814 - val_accuracy: 0.5143 - val_loss: 1.0881\n",
      "Epoch 2/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4919 - loss: 1.1146 - val_accuracy: 0.5143 - val_loss: 0.9817\n",
      "Epoch 3/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5766 - loss: 1.0030 - val_accuracy: 0.6571 - val_loss: 0.8730\n",
      "Epoch 4/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6512 - loss: 0.9290 - val_accuracy: 0.7857 - val_loss: 0.7656\n",
      "Epoch 5/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7183 - loss: 0.8363 - val_accuracy: 0.8429 - val_loss: 0.6593\n",
      "Epoch 6/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7673 - loss: 0.7381 - val_accuracy: 0.8857 - val_loss: 0.5591\n",
      "Epoch 7/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8060 - loss: 0.6683 - val_accuracy: 0.9143 - val_loss: 0.4738\n",
      "Epoch 8/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8387 - loss: 0.5880 - val_accuracy: 0.9143 - val_loss: 0.4028\n",
      "Epoch 9/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8357 - loss: 0.5324 - val_accuracy: 0.9000 - val_loss: 0.3488\n",
      "Epoch 10/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8635 - loss: 0.4503 - val_accuracy: 0.9143 - val_loss: 0.3065\n",
      "Epoch 11/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8582 - loss: 0.4416 - val_accuracy: 0.9286 - val_loss: 0.2726\n",
      "Epoch 12/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8763 - loss: 0.3723 - val_accuracy: 0.9429 - val_loss: 0.2452\n",
      "Epoch 13/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8998 - loss: 0.3355 - val_accuracy: 0.9571 - val_loss: 0.2244\n",
      "Epoch 14/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8966 - loss: 0.3344 - val_accuracy: 0.9571 - val_loss: 0.2064\n",
      "Epoch 15/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8714 - loss: 0.3219 - val_accuracy: 0.9571 - val_loss: 0.1903\n",
      "Epoch 16/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8996 - loss: 0.2991 - val_accuracy: 0.9714 - val_loss: 0.1797\n",
      "Epoch 17/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8950 - loss: 0.2995 - val_accuracy: 0.9571 - val_loss: 0.1695\n",
      "Epoch 18/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9183 - loss: 0.2642 - val_accuracy: 0.9571 - val_loss: 0.1610\n",
      "Epoch 19/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8831 - loss: 0.2849 - val_accuracy: 0.9571 - val_loss: 0.1477\n",
      "Epoch 20/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9189 - loss: 0.2687 - val_accuracy: 0.9571 - val_loss: 0.1400\n",
      "Epoch 21/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9251 - loss: 0.2530 - val_accuracy: 0.9571 - val_loss: 0.1357\n",
      "Epoch 22/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9107 - loss: 0.2394 - val_accuracy: 0.9571 - val_loss: 0.1308\n",
      "Epoch 23/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9040 - loss: 0.2321 - val_accuracy: 0.9714 - val_loss: 0.1231\n",
      "Epoch 24/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9130 - loss: 0.2443 - val_accuracy: 0.9714 - val_loss: 0.1181\n",
      "Epoch 25/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9177 - loss: 0.2195 - val_accuracy: 0.9714 - val_loss: 0.1131\n",
      "Epoch 26/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9364 - loss: 0.2237 - val_accuracy: 0.9714 - val_loss: 0.1114\n",
      "Epoch 27/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9317 - loss: 0.2094 - val_accuracy: 0.9714 - val_loss: 0.1091\n",
      "Epoch 28/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9187 - loss: 0.2226 - val_accuracy: 0.9714 - val_loss: 0.1014\n",
      "Epoch 29/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9200 - loss: 0.2225 - val_accuracy: 0.9714 - val_loss: 0.0942\n",
      "Epoch 30/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9188 - loss: 0.2140 - val_accuracy: 0.9714 - val_loss: 0.0892\n",
      "Epoch 31/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9189 - loss: 0.2094 - val_accuracy: 0.9714 - val_loss: 0.0869\n",
      "Epoch 32/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9335 - loss: 0.1815 - val_accuracy: 0.9714 - val_loss: 0.0871\n",
      "Epoch 33/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9433 - loss: 0.1940 - val_accuracy: 0.9714 - val_loss: 0.0847\n",
      "Epoch 34/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9474 - loss: 0.1676 - val_accuracy: 0.9714 - val_loss: 0.0825\n",
      "Epoch 35/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9310 - loss: 0.1672 - val_accuracy: 0.9714 - val_loss: 0.0784\n",
      "Epoch 36/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9596 - loss: 0.1579 - val_accuracy: 0.9714 - val_loss: 0.0750\n",
      "Epoch 37/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9252 - loss: 0.1878 - val_accuracy: 0.9714 - val_loss: 0.0712\n",
      "Epoch 38/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9494 - loss: 0.1532 - val_accuracy: 0.9714 - val_loss: 0.0682\n",
      "Epoch 39/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9410 - loss: 0.1683 - val_accuracy: 0.9714 - val_loss: 0.0693\n",
      "Epoch 40/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9610 - loss: 0.1503 - val_accuracy: 0.9714 - val_loss: 0.0684\n",
      "Epoch 41/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9560 - loss: 0.1545 - val_accuracy: 0.9714 - val_loss: 0.0635\n",
      "Epoch 42/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9556 - loss: 0.1461 - val_accuracy: 0.9857 - val_loss: 0.0595\n",
      "Epoch 43/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9633 - loss: 0.1531 - val_accuracy: 0.9857 - val_loss: 0.0602\n",
      "Epoch 44/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9290 - loss: 0.1519 - val_accuracy: 0.9714 - val_loss: 0.0612\n",
      "Epoch 45/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9672 - loss: 0.1389 - val_accuracy: 0.9714 - val_loss: 0.0573\n",
      "Epoch 46/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9660 - loss: 0.1256 - val_accuracy: 0.9857 - val_loss: 0.0534\n",
      "Epoch 47/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9794 - loss: 0.1225 - val_accuracy: 0.9857 - val_loss: 0.0495\n",
      "Epoch 48/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9657 - loss: 0.1200 - val_accuracy: 0.9857 - val_loss: 0.0526\n",
      "Epoch 49/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9480 - loss: 0.1357 - val_accuracy: 0.9857 - val_loss: 0.0526\n",
      "Epoch 50/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9520 - loss: 0.1421 - val_accuracy: 0.9857 - val_loss: 0.0508\n",
      "Epoch 51/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9624 - loss: 0.1109 - val_accuracy: 0.9857 - val_loss: 0.0488\n",
      "Epoch 52/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9507 - loss: 0.1263 - val_accuracy: 0.9857 - val_loss: 0.0444\n",
      "Epoch 53/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9673 - loss: 0.1240 - val_accuracy: 0.9857 - val_loss: 0.0440\n",
      "Epoch 54/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9623 - loss: 0.1201 - val_accuracy: 0.9857 - val_loss: 0.0443\n",
      "Epoch 55/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9647 - loss: 0.1204 - val_accuracy: 0.9857 - val_loss: 0.0414\n",
      "Epoch 56/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9615 - loss: 0.1123 - val_accuracy: 0.9857 - val_loss: 0.0402\n",
      "Epoch 57/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9706 - loss: 0.0996 - val_accuracy: 0.9857 - val_loss: 0.0352\n",
      "Epoch 58/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9650 - loss: 0.1112 - val_accuracy: 0.9857 - val_loss: 0.0359\n",
      "Epoch 59/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9484 - loss: 0.1154 - val_accuracy: 0.9857 - val_loss: 0.0382\n",
      "Epoch 60/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9630 - loss: 0.1012 - val_accuracy: 0.9857 - val_loss: 0.0381\n",
      "Epoch 61/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9673 - loss: 0.0969 - val_accuracy: 0.9857 - val_loss: 0.0370\n",
      "Epoch 62/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9644 - loss: 0.0863 - val_accuracy: 0.9857 - val_loss: 0.0331\n",
      "Epoch 63/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9759 - loss: 0.0817 - val_accuracy: 0.9857 - val_loss: 0.0342\n",
      "Epoch 64/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9597 - loss: 0.0980 - val_accuracy: 0.9857 - val_loss: 0.0335\n",
      "Epoch 65/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9726 - loss: 0.0860 - val_accuracy: 0.9857 - val_loss: 0.0353\n",
      "Epoch 66/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9536 - loss: 0.0971 - val_accuracy: 0.9857 - val_loss: 0.0306\n",
      "Epoch 67/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9778 - loss: 0.0923 - val_accuracy: 0.9857 - val_loss: 0.0305\n",
      "Epoch 68/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9688 - loss: 0.0907 - val_accuracy: 0.9857 - val_loss: 0.0287\n",
      "Epoch 69/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9719 - loss: 0.0776 - val_accuracy: 0.9857 - val_loss: 0.0305\n",
      "Epoch 70/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9783 - loss: 0.0744 - val_accuracy: 0.9857 - val_loss: 0.0300\n",
      "Epoch 71/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9724 - loss: 0.0745 - val_accuracy: 0.9857 - val_loss: 0.0254\n",
      "Epoch 72/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9800 - loss: 0.0852 - val_accuracy: 0.9857 - val_loss: 0.0273\n",
      "Epoch 73/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9778 - loss: 0.0850 - val_accuracy: 0.9857 - val_loss: 0.0267\n",
      "Epoch 74/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9735 - loss: 0.0684 - val_accuracy: 0.9857 - val_loss: 0.0255\n",
      "Epoch 75/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9734 - loss: 0.0724 - val_accuracy: 0.9857 - val_loss: 0.0260\n",
      "Epoch 76/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9646 - loss: 0.0792 - val_accuracy: 0.9857 - val_loss: 0.0225\n",
      "Epoch 77/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9760 - loss: 0.0643 - val_accuracy: 0.9857 - val_loss: 0.0250\n",
      "Epoch 78/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9816 - loss: 0.0786 - val_accuracy: 0.9857 - val_loss: 0.0236\n",
      "Epoch 79/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9854 - loss: 0.0615 - val_accuracy: 0.9857 - val_loss: 0.0219\n",
      "Epoch 80/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9821 - loss: 0.0620 - val_accuracy: 0.9857 - val_loss: 0.0229\n",
      "Epoch 81/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9860 - loss: 0.0548 - val_accuracy: 0.9857 - val_loss: 0.0201\n",
      "Epoch 82/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9858 - loss: 0.0668 - val_accuracy: 1.0000 - val_loss: 0.0174\n",
      "Epoch 83/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9729 - loss: 0.0739 - val_accuracy: 0.9857 - val_loss: 0.0188\n",
      "Epoch 84/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9831 - loss: 0.0647 - val_accuracy: 0.9857 - val_loss: 0.0201\n",
      "Epoch 85/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9850 - loss: 0.0611 - val_accuracy: 0.9857 - val_loss: 0.0211\n",
      "Epoch 86/86\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9692 - loss: 0.0782 - val_accuracy: 0.9857 - val_loss: 0.0197\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Evaluation =====\n",
      "Accuracy: 0.8834\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3077    0.4000    0.3478        10\n",
      "           1     0.9178    0.8816    0.8993        76\n",
      "           2     0.9916    0.9219    0.9555       128\n",
      "           3     0.4444    0.8889    0.5926         9\n",
      "\n",
      "    accuracy                         0.8834       223\n",
      "   macro avg     0.6654    0.7731    0.6988       223\n",
      "weighted avg     0.9137    0.8834    0.8944       223\n",
      "\n",
      "Confusion Matrix:\n",
      " [[  4   6   0   0]\n",
      " [  9  67   0   0]\n",
      " [  0   0 118  10]\n",
      " [  0   0   1   8]]\n",
      "\n",
      "===== Model Metrics =====\n",
      "Training Time: 10.34s\n",
      "Inference/sample: 0.002560s\n",
      "Testing Time: 0.57s\n",
      "Model Size: 0.16 MB\n",
      "Trainable Params: 35,379\n",
      "RAM Usage: 3180.14 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import psutil\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout\n",
    "import tensorflow as tf\n",
    "from bayes_opt import BayesianOptimization\n",
    "from scipy.fftpack import fft\n",
    "import warnings\n",
    "\n",
    "# Set seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================\n",
    "# 1. Data Loading and Preprocessing\n",
    "# ============================================\n",
    "def load_and_preprocess_data(filepath):\n",
    "    df = pd.read_csv(filepath, delimiter=\",\", low_memory=False)\n",
    "    df.columns = df.columns.str.replace('\"', '').str.strip()\n",
    "\n",
    "    if 'TIME' not in df.columns:\n",
    "        raise KeyError(\"The column 'TIME' does not exist in the dataset.\")\n",
    "\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'], errors='coerce')\n",
    "    df.dropna(subset=['TIME'], inplace=True)\n",
    "    df.set_index('TIME', inplace=True)\n",
    "\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "    target_col = 'fenologia_h1'\n",
    "    if target_col not in df.columns:\n",
    "        raise KeyError(f\"The target column '{target_col}' does not exist.\")\n",
    "\n",
    "    df[target_col].interpolate(method='linear', inplace=True)\n",
    "    df[target_col] = df[target_col] - 1\n",
    "\n",
    "    for i in range(1, 4):\n",
    "        df[f'{target_col}_lag{i}'] = df[target_col].shift(i)\n",
    "\n",
    "    for window in [3, 6]:\n",
    "        df[f'{target_col}_roll_mean_{window}'] = df[target_col].rolling(window).mean()\n",
    "        df[f'{target_col}_roll_std_{window}'] = df[target_col].rolling(window).std()\n",
    "\n",
    "    df['month'] = df.index.month\n",
    "    df['weekofyear'] = df.index.isocalendar().week\n",
    "    df['year'] = df.index.isocalendar().year\n",
    "    df['sin_week'] = np.sin(2 * np.pi * df['weekofyear'] / 52)\n",
    "    df['cos_week'] = np.cos(2 * np.pi * df['weekofyear'] / 52)\n",
    "    df['EMA_3'] = df[target_col].ewm(span=3, adjust=False).mean()\n",
    "    df['EMA_6'] = df[target_col].ewm(span=6, adjust=False).mean()\n",
    "    df['correlation_target_month'] = df[target_col].rolling(window=6).corr(df['month'])\n",
    "    df['correlation_target_week'] = df[target_col].rolling(window=6).corr(df['weekofyear'])\n",
    "\n",
    "    # fft_values = fft(df[target_col].dropna().values)\n",
    "    # fft_real = np.real(fft_values)[:len(df[target_col])]\n",
    "    # fft_imag = np.imag(fft_values)[:len(df[target_col])]\n",
    "    # df['fft_real'] = np.concatenate([fft_real, np.nan * np.ones(len(df) - len(fft_real))])\n",
    "    # df['fft_imag'] = np.concatenate([fft_imag, np.nan * np.ones(len(df) - len(fft_imag))])\n",
    "\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "    return df, target_col\n",
    "\n",
    "# ============================================\n",
    "# 2. Feature Scaling\n",
    "# ============================================\n",
    "def feature_scaling(df, target_col):\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))  # 3D for RNN\n",
    "    return X_scaled, y, scaler\n",
    "\n",
    "# ============================================\n",
    "# 3. Build GRU Model\n",
    "# ============================================\n",
    "def build_gru_model(input_shape, num_classes=4, num_units=64, dropout_rate=0.2):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(num_units, input_shape=input_shape))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(num_units // 2, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# ============================================\n",
    "# 4. Bayesian Optimization Objective\n",
    "# ============================================\n",
    "def objective_function(num_units, dropout_rate, epochs, batch_size, X_train, y_train):\n",
    "    model = build_gru_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        num_classes=4,\n",
    "        num_units=int(num_units),\n",
    "        dropout_rate=dropout_rate\n",
    "    )\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=int(epochs),\n",
    "        batch_size=int(batch_size),\n",
    "        validation_split=0.2,\n",
    "        verbose=0\n",
    "    )\n",
    "    return np.max(history.history['val_accuracy'])\n",
    "\n",
    "# ============================================\n",
    "# 5. Optimize Hyperparameters\n",
    "# ============================================\n",
    "def optimize_hyperparameters(X_train, y_train):\n",
    "    pbounds = {\n",
    "        'num_units': (32, 128),\n",
    "        'dropout_rate': (0.1, 0.5),\n",
    "        'epochs': (50, 100),\n",
    "        'batch_size': (16, 64)\n",
    "    }\n",
    "\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=lambda num_units, dropout_rate, epochs, batch_size: objective_function(\n",
    "            num_units, dropout_rate, epochs, batch_size, X_train, y_train\n",
    "        ),\n",
    "        pbounds=pbounds,\n",
    "        random_state=42,\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    optimizer.maximize(init_points=5, n_iter=15)\n",
    "    return optimizer.max['params']\n",
    "\n",
    "# ============================================\n",
    "# 6. Train & Evaluate Model\n",
    "# ============================================\n",
    "def train_and_evaluate_model(X_train, X_test, y_train, y_test, best_params):\n",
    "    model = build_gru_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        num_classes=4,\n",
    "        num_units=int(best_params['num_units']),\n",
    "        dropout_rate=float(best_params['dropout_rate'])\n",
    "    )\n",
    "\n",
    "    start_train = time.time()\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=int(best_params['epochs']),\n",
    "        batch_size=int(best_params['batch_size']),\n",
    "        validation_split=0.2,\n",
    "        verbose=1\n",
    "    )\n",
    "    training_time = time.time() - start_train\n",
    "\n",
    "    start_infer = time.time()\n",
    "    y_pred = model.predict(X_test).argmax(axis=1)\n",
    "    inference_time = (time.time() - start_infer) / len(X_test)\n",
    "\n",
    "    # Calculate total testing time\n",
    "    testing_time = time.time() - start_infer\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(\"\\n===== Evaluation =====\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(\"Report:\\n\", classification_report(y_test, y_pred, digits=4))\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "    model.save(\"temp_model.h5\", include_optimizer=False)\n",
    "    model_size = os.path.getsize(\"temp_model.h5\") / (1024 ** 2)\n",
    "    os.remove(\"temp_model.h5\")\n",
    "\n",
    "    total_params = np.sum([np.prod(v.shape) for v in model.trainable_weights])\n",
    "    ram_usage = psutil.Process(os.getpid()).memory_info().rss / (1024 ** 2)\n",
    "\n",
    "    print(\"\\n===== Model Metrics =====\")\n",
    "    print(f\"Training Time: {training_time:.2f}s\")\n",
    "    print(f\"Inference/sample: {inference_time:.6f}s\")\n",
    "    print(f\"Testing Time: {testing_time:.2f}s\")\n",
    "    print(f\"Model Size: {model_size:.2f} MB\")\n",
    "    print(f\"Trainable Params: {total_params:,}\")\n",
    "    print(f\"RAM Usage: {ram_usage:.2f} MB\")\n",
    "\n",
    "    return model, history\n",
    "\n",
    "# ============================================\n",
    "# 7. Main\n",
    "# ============================================\n",
    "def main():\n",
    "    train_path = r\"E:\\Abroad period research\\Phenology datasets\\PHENOLOGY_H1\\sevilla_train.csv\"\n",
    "    test_path = r\"E:\\Abroad period research\\Phenology datasets\\PHENOLOGY_H1\\sevilla_test.csv\"\n",
    "\n",
    "    df_train, target_col = load_and_preprocess_data(train_path)\n",
    "    X_train, y_train, _ = feature_scaling(df_train, target_col)\n",
    "\n",
    "    df_test, _ = load_and_preprocess_data(test_path)\n",
    "    X_test, y_test, _ = feature_scaling(df_test, target_col)\n",
    "\n",
    "    print(\"\\n Optimizing Hyperparameters...\")\n",
    "    best_params = optimize_hyperparameters(X_train, y_train)\n",
    "    print(\"\\n Best Params:\", best_params)\n",
    "\n",
    "    print(\"\\n Training Final GRU Model...\")\n",
    "    model, history = train_and_evaluate_model(X_train, X_test, y_train, y_test, best_params)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
