{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8ce3eb7",
   "metadata": {},
   "source": [
    "Transformer for cadiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa8291a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Optimizing Hyperparameters...\n",
      "|   iter    |  target   | batch_... | dropou... |  epochs   | num_units |\n",
      "-------------------------------------------------------------------------\n",
      "WARNING:tensorflow:From c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:204: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.9692   \u001b[39m | \u001b[39m33.98    \u001b[39m | \u001b[39m0.4803   \u001b[39m | \u001b[39m51.96    \u001b[39m | \u001b[39m89.47    \u001b[39m |\n",
      "| \u001b[35m2        \u001b[39m | \u001b[35m1.0      \u001b[39m | \u001b[35m23.49    \u001b[39m | \u001b[35m0.1624   \u001b[39m | \u001b[35m31.74    \u001b[39m | \u001b[35m115.2    \u001b[39m |\n",
      "| \u001b[39m3        \u001b[39m | \u001b[39m0.9692   \u001b[39m | \u001b[39m44.85    \u001b[39m | \u001b[39m0.3832   \u001b[39m | \u001b[39m30.62    \u001b[39m | \u001b[39m125.1    \u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m0.9846   \u001b[39m | \u001b[39m55.96    \u001b[39m | \u001b[39m0.1849   \u001b[39m | \u001b[39m35.45    \u001b[39m | \u001b[39m49.61    \u001b[39m |\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m30.6     \u001b[39m | \u001b[39m0.3099   \u001b[39m | \u001b[39m42.96    \u001b[39m | \u001b[39m59.96    \u001b[39m |\n",
      "| \u001b[39m6        \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m23.48    \u001b[39m | \u001b[39m0.2078   \u001b[39m | \u001b[39m33.0     \u001b[39m | \u001b[39m115.1    \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m0.9692   \u001b[39m | \u001b[39m19.37    \u001b[39m | \u001b[39m0.3686   \u001b[39m | \u001b[39m38.21    \u001b[39m | \u001b[39m51.12    \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m38.05    \u001b[39m | \u001b[39m0.2945   \u001b[39m | \u001b[39m48.98    \u001b[39m | \u001b[39m60.93    \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m39.01    \u001b[39m | \u001b[39m0.1325   \u001b[39m | \u001b[39m36.55    \u001b[39m | \u001b[39m65.57    \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m16.86    \u001b[39m | \u001b[39m0.1669   \u001b[39m | \u001b[39m34.74    \u001b[39m | \u001b[39m127.2    \u001b[39m |\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m16.57    \u001b[39m | \u001b[39m0.1685   \u001b[39m | \u001b[39m52.55    \u001b[39m | \u001b[39m123.2    \u001b[39m |\n",
      "| \u001b[39m12       \u001b[39m | \u001b[39m0.9692   \u001b[39m | \u001b[39m16.33    \u001b[39m | \u001b[39m0.4155   \u001b[39m | \u001b[39m31.9     \u001b[39m | \u001b[39m100.1    \u001b[39m |\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m29.6     \u001b[39m | \u001b[39m0.1583   \u001b[39m | \u001b[39m54.16    \u001b[39m | \u001b[39m127.2    \u001b[39m |\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m39.76    \u001b[39m | \u001b[39m0.2274   \u001b[39m | \u001b[39m58.81    \u001b[39m | \u001b[39m43.08    \u001b[39m |\n",
      "| \u001b[39m15       \u001b[39m | \u001b[39m0.9692   \u001b[39m | \u001b[39m54.67    \u001b[39m | \u001b[39m0.4953   \u001b[39m | \u001b[39m58.43    \u001b[39m | \u001b[39m32.22    \u001b[39m |\n",
      "| \u001b[39m16       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m29.8     \u001b[39m | \u001b[39m0.3283   \u001b[39m | \u001b[39m59.46    \u001b[39m | \u001b[39m51.26    \u001b[39m |\n",
      "| \u001b[39m17       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m28.44    \u001b[39m | \u001b[39m0.202    \u001b[39m | \u001b[39m59.88    \u001b[39m | \u001b[39m34.56    \u001b[39m |\n",
      "| \u001b[39m18       \u001b[39m | \u001b[39m0.9692   \u001b[39m | \u001b[39m62.14    \u001b[39m | \u001b[39m0.2787   \u001b[39m | \u001b[39m30.25    \u001b[39m | \u001b[39m78.84    \u001b[39m |\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m62.79    \u001b[39m | \u001b[39m0.4373   \u001b[39m | \u001b[39m59.92    \u001b[39m | \u001b[39m127.9    \u001b[39m |\n",
      "| \u001b[39m20       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m44.47    \u001b[39m | \u001b[39m0.1497   \u001b[39m | \u001b[39m59.64    \u001b[39m | \u001b[39m122.8    \u001b[39m |\n",
      "=========================================================================\n",
      "\n",
      " Best Hyperparameters: {'batch_size': 23.488894741236955, 'dropout_rate': 0.16239780813448107, 'epochs': 31.742508365045985, 'num_units': 115.15290999439378}\n",
      "\n",
      " Training Final Transformer Model...\n",
      "Epoch 1/31\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 34ms/step - accuracy: 0.5564 - loss: 1.1738 - val_accuracy: 0.5538 - val_loss: 0.6508\n",
      "Epoch 2/31\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7709 - loss: 0.5876 - val_accuracy: 0.8462 - val_loss: 0.3528\n",
      "Epoch 3/31\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7725 - loss: 0.4788 - val_accuracy: 0.9077 - val_loss: 0.2472\n",
      "Epoch 4/31\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8459 - loss: 0.3624 - val_accuracy: 0.9077 - val_loss: 0.2621\n",
      "Epoch 5/31\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8660 - loss: 0.3667 - val_accuracy: 0.9385 - val_loss: 0.1887\n",
      "Epoch 6/31\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8922 - loss: 0.2767 - val_accuracy: 0.9231 - val_loss: 0.1994\n",
      "Epoch 7/31\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9002 - loss: 0.2928 - val_accuracy: 0.9385 - val_loss: 0.1168\n",
      "Epoch 8/31\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9142 - loss: 0.2045 - val_accuracy: 0.9692 - val_loss: 0.0795\n",
      "Epoch 9/31\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9288 - loss: 0.1799 - val_accuracy: 0.9692 - val_loss: 0.0745\n",
      "Epoch 10/31\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9317 - loss: 0.1596 - val_accuracy: 0.9692 - val_loss: 0.0927\n",
      "Epoch 11/31\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9349 - loss: 0.1896 - val_accuracy: 0.9846 - val_loss: 0.0470\n",
      "Epoch 12/31\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9407 - loss: 0.1413 - val_accuracy: 0.9692 - val_loss: 0.0920\n",
      "Epoch 13/31\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9517 - loss: 0.1048 - val_accuracy: 0.9846 - val_loss: 0.0366\n",
      "Epoch 14/31\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9638 - loss: 0.1030 - val_accuracy: 0.9692 - val_loss: 0.0948\n",
      "Epoch 15/31\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9495 - loss: 0.1482 - val_accuracy: 1.0000 - val_loss: 0.0427\n",
      "Epoch 16/31\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9748 - loss: 0.1078 - val_accuracy: 0.9692 - val_loss: 0.0940\n",
      "Epoch 17/31\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9407 - loss: 0.1546 - val_accuracy: 0.9846 - val_loss: 0.0348\n",
      "Epoch 18/31\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9573 - loss: 0.1259 - val_accuracy: 0.9692 - val_loss: 0.1416\n",
      "Epoch 19/31\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9464 - loss: 0.1101 - val_accuracy: 0.9846 - val_loss: 0.0372\n",
      "Epoch 20/31\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9597 - loss: 0.1044 - val_accuracy: 0.9846 - val_loss: 0.0301\n",
      "Epoch 21/31\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9722 - loss: 0.0795 - val_accuracy: 0.9692 - val_loss: 0.0589\n",
      "Epoch 22/31\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9616 - loss: 0.0994 - val_accuracy: 1.0000 - val_loss: 0.0209\n",
      "Epoch 23/31\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9902 - loss: 0.0497 - val_accuracy: 1.0000 - val_loss: 0.0376\n",
      "Epoch 24/31\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9807 - loss: 0.0508 - val_accuracy: 1.0000 - val_loss: 0.0222\n",
      "Epoch 25/31\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9705 - loss: 0.0803 - val_accuracy: 1.0000 - val_loss: 0.0098\n",
      "Epoch 26/31\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9700 - loss: 0.0666 - val_accuracy: 1.0000 - val_loss: 0.0113\n",
      "Epoch 27/31\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9812 - loss: 0.0499 - val_accuracy: 1.0000 - val_loss: 0.0154\n",
      "Epoch 28/31\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9902 - loss: 0.0471 - val_accuracy: 1.0000 - val_loss: 0.0162\n",
      "Epoch 29/31\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9849 - loss: 0.0602 - val_accuracy: 1.0000 - val_loss: 0.0130\n",
      "Epoch 30/31\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9881 - loss: 0.0440 - val_accuracy: 0.9692 - val_loss: 0.0444\n",
      "Epoch 31/31\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9881 - loss: 0.0451 - val_accuracy: 1.0000 - val_loss: 0.0090\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Evaluation =====\n",
      "Accuracy: 0.9483\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.4375    0.6087        16\n",
      "           1     0.8846    1.0000    0.9388        69\n",
      "           2     1.0000    0.9781    0.9889       137\n",
      "           3     0.7692    1.0000    0.8696        10\n",
      "\n",
      "    accuracy                         0.9483       232\n",
      "   macro avg     0.9135    0.8539    0.8515       232\n",
      "weighted avg     0.9557    0.9483    0.9426       232\n",
      "\n",
      "[[  7   9   0   0]\n",
      " [  0  69   0   0]\n",
      " [  0   0 134   3]\n",
      " [  0   0   0  10]]\n",
      "\n",
      "===== Model Metrics =====\n",
      "Training Time: 9.07s\n",
      "Total Testing Time: 0.43s\n",
      "Inference Time per Sample: 0.001867s\n",
      "Model Size: 2.24 MB\n",
      "Trainable Params: 189,984\n",
      "RAM Usage: 1425.90 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import psutil\n",
    "import warnings\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from tensorflow.keras import layers, models\n",
    "import tensorflow as tf\n",
    "from bayes_opt import BayesianOptimization\n",
    "from scipy.fftpack import fft\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================\n",
    "# 1. Load and Preprocess Data\n",
    "# ============================================\n",
    "\n",
    "def load_and_preprocess_data(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df.columns = df.columns.str.replace('\"', '').str.strip()\n",
    "    if 'TIME' not in df.columns:\n",
    "        raise KeyError(\"Column 'TIME' missing.\")\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'], errors='coerce')\n",
    "    df.dropna(subset=['TIME'], inplace=True)\n",
    "    df.set_index('TIME', inplace=True)\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "    target_col = 'fenologia_h1'\n",
    "    if target_col not in df.columns:\n",
    "        target_col = 'target'\n",
    "    if target_col not in df.columns:\n",
    "        raise KeyError(\"Target column not found.\")\n",
    "\n",
    "    df[target_col].interpolate(method='linear', inplace=True)\n",
    "    df[target_col] = df[target_col].astype(int)\n",
    "    df[target_col] = df[target_col] - 1\n",
    "\n",
    "    for i in range(1, 4):\n",
    "        df[f'{target_col}_lag{i}'] = df[target_col].shift(i)\n",
    "    for window in [3, 6]:\n",
    "        df[f'{target_col}_roll_mean_{window}'] = df[target_col].rolling(window).mean()\n",
    "        df[f'{target_col}_roll_std_{window}'] = df[target_col].rolling(window).std()\n",
    "\n",
    "    # df['month'] = df.index.month\n",
    "    # df['weekofyear'] = df.index.isocalendar().week\n",
    "    # df['year'] = df.index.isocalendar().year\n",
    "    # df['sin_week'] = np.sin(2 * np.pi * df['weekofyear'] / 52)\n",
    "    # df['cos_week'] = np.cos(2 * np.pi * df['weekofyear'] / 52)\n",
    "    # df['EMA_3'] = df[target_col].ewm(span=3, adjust=False).mean()\n",
    "    # df['EMA_6'] = df[target_col].ewm(span=6, adjust=False).mean()\n",
    "    # df['correlation_target_month'] = df[target_col].rolling(window=6).corr(df['month'])\n",
    "    # df['correlation_target_week'] = df[target_col].rolling(window=6).corr(df['weekofyear'])\n",
    "\n",
    "    # fft_values = fft(df[target_col].dropna().values)\n",
    "    # fft_real = np.real(fft_values)[:len(df[target_col])]\n",
    "    # fft_imag = np.imag(fft_values)[:len(df[target_col])]\n",
    "    # df['fft_real'] = np.concatenate([fft_real, np.nan * np.ones(len(df) - len(fft_real))])\n",
    "    # df['fft_imag'] = np.concatenate([fft_imag, np.nan * np.ones(len(df) - len(fft_imag))])\n",
    "\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "    return df, target_col\n",
    "\n",
    "# ============================================\n",
    "# 2. Feature Scaling\n",
    "# ============================================\n",
    "def feature_scaling(df, target_col):\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col].astype(int)\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))  # 3D for Transformer\n",
    "    return X_scaled, y, scaler\n",
    "\n",
    "# ============================================\n",
    "# 3. Transformer Model\n",
    "# ============================================\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = models.Sequential([\n",
    "            layers.Dense(ff_dim, activation=\"relu\"),\n",
    "            layers.Dense(embed_dim),\n",
    "        ])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        attn_output = self.att(inputs, inputs, training=training)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1, training=training)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "def build_transformer_model(input_shape, num_classes=4, embed_dim=64, num_heads=2, ff_dim=128, dropout_rate=0.1):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = layers.Dense(embed_dim)(inputs)\n",
    "    x = TransformerBlock(embed_dim, num_heads, ff_dim, dropout_rate)(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    x = layers.Dense(ff_dim, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "# ============================================\n",
    "# 4. Objective Function\n",
    "# ============================================\n",
    "def objective_function(num_units, dropout_rate, epochs, batch_size, X_train, y_train):\n",
    "    model = build_transformer_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        num_classes=4,\n",
    "        embed_dim=int(num_units),\n",
    "        num_heads=2,\n",
    "        ff_dim=int(num_units * 2),\n",
    "        dropout_rate=dropout_rate\n",
    "    )\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=int(epochs),\n",
    "        batch_size=int(batch_size),\n",
    "        validation_split=0.2,\n",
    "        verbose=0\n",
    "    )\n",
    "    return np.max(history.history['val_accuracy'])\n",
    "\n",
    "# ============================================\n",
    "# 5. Optimize Hyperparameters\n",
    "# ============================================\n",
    "def optimize_hyperparameters(X_train, y_train):\n",
    "    pbounds = {\n",
    "        'num_units': (32, 128),\n",
    "        'dropout_rate': (0.1, 0.5),\n",
    "        'epochs': (30, 60),\n",
    "        'batch_size': (16, 64)\n",
    "    }\n",
    "\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=lambda num_units, dropout_rate, epochs, batch_size: objective_function(\n",
    "            num_units, dropout_rate, epochs, batch_size, X_train, y_train\n",
    "        ),\n",
    "        pbounds=pbounds,\n",
    "        random_state=42,\n",
    "        verbose=2\n",
    "    )\n",
    "    optimizer.maximize(init_points=5, n_iter=15)\n",
    "    return optimizer.max['params']\n",
    "\n",
    "# ============================================\n",
    "# 6. Train and Evaluate\n",
    "# ============================================\n",
    "def train_and_evaluate_model(X_train, X_test, y_train, y_test, best_params):\n",
    "    model = build_transformer_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        num_classes=4,\n",
    "        embed_dim=int(best_params['num_units']),\n",
    "        num_heads=2,\n",
    "        ff_dim=int(best_params['num_units'] * 2),\n",
    "        dropout_rate=float(best_params['dropout_rate'])\n",
    "    )\n",
    "\n",
    "    # Training time\n",
    "    start_train = time.time()\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=int(best_params['epochs']),\n",
    "        batch_size=int(best_params['batch_size']),\n",
    "        validation_split=0.2,\n",
    "        verbose=1\n",
    "    )\n",
    "    training_time = time.time() - start_train\n",
    "\n",
    "    # Testing time\n",
    "    start_test = time.time()\n",
    "    y_pred = model.predict(X_test).argmax(axis=1)\n",
    "    total_testing_time = time.time() - start_test\n",
    "    inference_time_per_sample = total_testing_time / len(X_test)\n",
    "\n",
    "    print(\"\\n===== Evaluation =====\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(classification_report(y_test, y_pred, digits=4))\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    model.save(\"temp_model.h5\")\n",
    "    model_size = os.path.getsize(\"temp_model.h5\") / (1024 ** 2)\n",
    "    os.remove(\"temp_model.h5\")\n",
    "\n",
    "    total_params = np.sum([np.prod(v.shape) for v in model.trainable_weights])\n",
    "    ram_usage = psutil.Process(os.getpid()).memory_info().rss / (1024 ** 2)\n",
    "\n",
    "    print(\"\\n===== Model Metrics =====\")\n",
    "    print(f\"Training Time: {training_time:.2f}s\")\n",
    "    print(f\"Total Testing Time: {total_testing_time:.2f}s\")\n",
    "    print(f\"Inference Time per Sample: {inference_time_per_sample:.6f}s\")\n",
    "    print(f\"Model Size: {model_size:.2f} MB\")\n",
    "    print(f\"Trainable Params: {total_params:,}\")\n",
    "    print(f\"RAM Usage: {ram_usage:.2f} MB\")\n",
    "\n",
    "    return model, history\n",
    "\n",
    "# ============================================\n",
    "# 7. Main Execution\n",
    "# ============================================\n",
    "def main():\n",
    "    train_path =  r\"E:\\Abroad period research\\Phenology datasets\\PHENOLOGY_H1\\Cadiz_train.csv\"\n",
    "    test_path =  r\"E:\\Abroad period research\\Phenology datasets\\PHENOLOGY_H1\\Cadiz_test.csv\"\n",
    "\n",
    "    df_train, target_col = load_and_preprocess_data(train_path)\n",
    "    X_train, y_train, _ = feature_scaling(df_train, target_col)\n",
    "\n",
    "    df_test, _ = load_and_preprocess_data(test_path)\n",
    "    X_test, y_test, _ = feature_scaling(df_test, target_col)\n",
    "\n",
    "    print(\"\\n Optimizing Hyperparameters...\")\n",
    "    best_params = optimize_hyperparameters(X_train, y_train)\n",
    "    print(\"\\n Best Hyperparameters:\", best_params)\n",
    "\n",
    "    print(\"\\n Training Final Transformer Model...\")\n",
    "    model, history = train_and_evaluate_model(X_train, X_test, y_train, y_test, best_params)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0435f35",
   "metadata": {},
   "source": [
    "Transformer for Cordoba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d57a4273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Optimizing Hyperparameters...\n",
      "|   iter    |  target   | batch_... | dropou... |  epochs   | num_units |\n",
      "-------------------------------------------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.9516   \u001b[39m | \u001b[39m33.98    \u001b[39m | \u001b[39m0.4803   \u001b[39m | \u001b[39m51.96    \u001b[39m | \u001b[39m89.47    \u001b[39m |\n",
      "| \u001b[35m2        \u001b[39m | \u001b[35m0.9839   \u001b[39m | \u001b[35m23.49    \u001b[39m | \u001b[35m0.1624   \u001b[39m | \u001b[35m31.74    \u001b[39m | \u001b[35m115.2    \u001b[39m |\n",
      "| \u001b[39m3        \u001b[39m | \u001b[39m0.9839   \u001b[39m | \u001b[39m44.85    \u001b[39m | \u001b[39m0.3832   \u001b[39m | \u001b[39m30.62    \u001b[39m | \u001b[39m125.1    \u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m0.9677   \u001b[39m | \u001b[39m55.96    \u001b[39m | \u001b[39m0.1849   \u001b[39m | \u001b[39m35.45    \u001b[39m | \u001b[39m49.61    \u001b[39m |\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m0.9355   \u001b[39m | \u001b[39m30.6     \u001b[39m | \u001b[39m0.3099   \u001b[39m | \u001b[39m42.96    \u001b[39m | \u001b[39m59.96    \u001b[39m |\n",
      "| \u001b[39m6        \u001b[39m | \u001b[39m0.9839   \u001b[39m | \u001b[39m29.65    \u001b[39m | \u001b[39m0.1325   \u001b[39m | \u001b[39m54.78    \u001b[39m | \u001b[39m127.8    \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m0.9677   \u001b[39m | \u001b[39m63.83    \u001b[39m | \u001b[39m0.1911   \u001b[39m | \u001b[39m59.46    \u001b[39m | \u001b[39m127.8    \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m0.9677   \u001b[39m | \u001b[39m23.86    \u001b[39m | \u001b[39m0.2556   \u001b[39m | \u001b[39m31.17    \u001b[39m | \u001b[39m114.0    \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m0.9516   \u001b[39m | \u001b[39m62.6     \u001b[39m | \u001b[39m0.3838   \u001b[39m | \u001b[39m59.82    \u001b[39m | \u001b[39m67.9     \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m0.9677   \u001b[39m | \u001b[39m22.58    \u001b[39m | \u001b[39m0.2599   \u001b[39m | \u001b[39m32.23    \u001b[39m | \u001b[39m115.6    \u001b[39m |\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m0.9677   \u001b[39m | \u001b[39m44.44    \u001b[39m | \u001b[39m0.148    \u001b[39m | \u001b[39m30.44    \u001b[39m | \u001b[39m124.6    \u001b[39m |\n",
      "| \u001b[39m12       \u001b[39m | \u001b[39m0.9677   \u001b[39m | \u001b[39m24.7     \u001b[39m | \u001b[39m0.2247   \u001b[39m | \u001b[39m32.2     \u001b[39m | \u001b[39m114.7    \u001b[39m |\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m0.9839   \u001b[39m | \u001b[39m29.6     \u001b[39m | \u001b[39m0.1583   \u001b[39m | \u001b[39m54.16    \u001b[39m | \u001b[39m127.2    \u001b[39m |\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m0.9839   \u001b[39m | \u001b[39m45.56    \u001b[39m | \u001b[39m0.2568   \u001b[39m | \u001b[39m30.5     \u001b[39m | \u001b[39m125.8    \u001b[39m |\n",
      "| \u001b[39m15       \u001b[39m | \u001b[39m0.9516   \u001b[39m | \u001b[39m30.77    \u001b[39m | \u001b[39m0.4819   \u001b[39m | \u001b[39m55.03    \u001b[39m | \u001b[39m126.5    \u001b[39m |\n",
      "| \u001b[39m16       \u001b[39m | \u001b[39m0.9677   \u001b[39m | \u001b[39m28.55    \u001b[39m | \u001b[39m0.3297   \u001b[39m | \u001b[39m55.09    \u001b[39m | \u001b[39m126.9    \u001b[39m |\n",
      "| \u001b[39m17       \u001b[39m | \u001b[39m0.9839   \u001b[39m | \u001b[39m45.01    \u001b[39m | \u001b[39m0.1485   \u001b[39m | \u001b[39m31.02    \u001b[39m | \u001b[39m125.7    \u001b[39m |\n",
      "| \u001b[39m18       \u001b[39m | \u001b[39m0.9839   \u001b[39m | \u001b[39m29.49    \u001b[39m | \u001b[39m0.4873   \u001b[39m | \u001b[39m52.49    \u001b[39m | \u001b[39m127.4    \u001b[39m |\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m0.9677   \u001b[39m | \u001b[39m29.41    \u001b[39m | \u001b[39m0.3213   \u001b[39m | \u001b[39m51.55    \u001b[39m | \u001b[39m126.2    \u001b[39m |\n",
      "| \u001b[39m20       \u001b[39m | \u001b[39m0.9677   \u001b[39m | \u001b[39m46.63    \u001b[39m | \u001b[39m0.2134   \u001b[39m | \u001b[39m30.97    \u001b[39m | \u001b[39m125.5    \u001b[39m |\n",
      "=========================================================================\n",
      "\n",
      " Best Hyperparameters: {'batch_size': 23.488894741236955, 'dropout_rate': 0.16239780813448107, 'epochs': 31.742508365045985, 'num_units': 115.15290999439378}\n",
      "\n",
      " Training Final Transformer Model...\n",
      "Epoch 1/31\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - accuracy: 0.4876 - loss: 1.2552 - val_accuracy: 0.8226 - val_loss: 0.4583\n",
      "Epoch 2/31\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8230 - loss: 0.5186 - val_accuracy: 0.8548 - val_loss: 0.4344\n",
      "Epoch 3/31\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8669 - loss: 0.3636 - val_accuracy: 0.8548 - val_loss: 0.3501\n",
      "Epoch 4/31\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8244 - loss: 0.4043 - val_accuracy: 0.8710 - val_loss: 0.3078\n",
      "Epoch 5/31\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8905 - loss: 0.3071 - val_accuracy: 0.9355 - val_loss: 0.2440\n",
      "Epoch 6/31\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8935 - loss: 0.2436 - val_accuracy: 0.9355 - val_loss: 0.1698\n",
      "Epoch 7/31\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9328 - loss: 0.1763 - val_accuracy: 0.9355 - val_loss: 0.1771\n",
      "Epoch 8/31\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9361 - loss: 0.1801 - val_accuracy: 0.9516 - val_loss: 0.1228\n",
      "Epoch 9/31\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9306 - loss: 0.1763 - val_accuracy: 0.9355 - val_loss: 0.1138\n",
      "Epoch 10/31\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9532 - loss: 0.1451 - val_accuracy: 0.9677 - val_loss: 0.0939\n",
      "Epoch 11/31\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9515 - loss: 0.1009 - val_accuracy: 0.9516 - val_loss: 0.1215\n",
      "Epoch 12/31\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9519 - loss: 0.1012 - val_accuracy: 0.9677 - val_loss: 0.1024\n",
      "Epoch 13/31\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9671 - loss: 0.0804 - val_accuracy: 0.9677 - val_loss: 0.0984\n",
      "Epoch 14/31\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9768 - loss: 0.0756 - val_accuracy: 0.9677 - val_loss: 0.0994\n",
      "Epoch 15/31\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9681 - loss: 0.0654 - val_accuracy: 0.9677 - val_loss: 0.1021\n",
      "Epoch 16/31\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9807 - loss: 0.0600 - val_accuracy: 0.9677 - val_loss: 0.1038\n",
      "Epoch 17/31\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9806 - loss: 0.0431 - val_accuracy: 0.9516 - val_loss: 0.1185\n",
      "Epoch 18/31\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9798 - loss: 0.0574 - val_accuracy: 0.9677 - val_loss: 0.0995\n",
      "Epoch 19/31\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9656 - loss: 0.0981 - val_accuracy: 0.8387 - val_loss: 0.2799\n",
      "Epoch 20/31\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9625 - loss: 0.0701 - val_accuracy: 0.9516 - val_loss: 0.1546\n",
      "Epoch 21/31\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9695 - loss: 0.0612 - val_accuracy: 0.9355 - val_loss: 0.1432\n",
      "Epoch 22/31\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9883 - loss: 0.0303 - val_accuracy: 0.9516 - val_loss: 0.1340\n",
      "Epoch 23/31\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9883 - loss: 0.0411 - val_accuracy: 0.9677 - val_loss: 0.1103\n",
      "Epoch 24/31\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9784 - loss: 0.0563 - val_accuracy: 0.8226 - val_loss: 0.4830\n",
      "Epoch 25/31\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9498 - loss: 0.1081 - val_accuracy: 0.9355 - val_loss: 0.1814\n",
      "Epoch 26/31\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9613 - loss: 0.1136 - val_accuracy: 0.8226 - val_loss: 0.4426\n",
      "Epoch 27/31\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9534 - loss: 0.1156 - val_accuracy: 0.9516 - val_loss: 0.1751\n",
      "Epoch 28/31\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9682 - loss: 0.0856 - val_accuracy: 0.9355 - val_loss: 0.1382\n",
      "Epoch 29/31\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9690 - loss: 0.0618 - val_accuracy: 0.9677 - val_loss: 0.1161\n",
      "Epoch 30/31\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9764 - loss: 0.0475 - val_accuracy: 0.9839 - val_loss: 0.0750\n",
      "Epoch 31/31\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9891 - loss: 0.0305 - val_accuracy: 0.9355 - val_loss: 0.1690\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Evaluation =====\n",
      "Accuracy: 0.8851\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6154    0.4706    0.5333        17\n",
      "           1     0.8478    0.8667    0.8571        45\n",
      "           2     0.9897    0.9505    0.9697       101\n",
      "           3     0.6111    1.0000    0.7586        11\n",
      "\n",
      "    accuracy                         0.8851       174\n",
      "   macro avg     0.7660    0.8219    0.7797       174\n",
      "weighted avg     0.8925    0.8851    0.8846       174\n",
      "\n",
      "[[ 8  7  0  2]\n",
      " [ 5 39  1  0]\n",
      " [ 0  0 96  5]\n",
      " [ 0  0  0 11]]\n",
      "\n",
      "===== Model Metrics =====\n",
      "Training Time: 8.55s\n",
      "Total Testing Time: 0.42s\n",
      "Inference Time per Sample: 0.002393s\n",
      "Model Size: 2.24 MB\n",
      "Trainable Params: 189,984\n",
      "RAM Usage: 2188.80 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import psutil\n",
    "import warnings\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from tensorflow.keras import layers, models\n",
    "import tensorflow as tf\n",
    "from bayes_opt import BayesianOptimization\n",
    "from scipy.fftpack import fft\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================\n",
    "# 1. Load and Preprocess Data\n",
    "# ============================================\n",
    "\n",
    "def load_and_preprocess_data(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df.columns = df.columns.str.replace('\"', '').str.strip()\n",
    "    if 'TIME' not in df.columns:\n",
    "        raise KeyError(\"Column 'TIME' missing.\")\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'], errors='coerce')\n",
    "    df.dropna(subset=['TIME'], inplace=True)\n",
    "    df.set_index('TIME', inplace=True)\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "    target_col = 'fenologia_h1'\n",
    "    if target_col not in df.columns:\n",
    "        target_col = 'target'\n",
    "    if target_col not in df.columns:\n",
    "        raise KeyError(\"Target column not found.\")\n",
    "\n",
    "    df[target_col].interpolate(method='linear', inplace=True)\n",
    "    df[target_col] = df[target_col].astype(int)\n",
    "    df[target_col] = df[target_col] - 1\n",
    "\n",
    "    for i in range(1, 4):\n",
    "        df[f'{target_col}_lag{i}'] = df[target_col].shift(i)\n",
    "    for window in [3, 6]:\n",
    "        df[f'{target_col}_roll_mean_{window}'] = df[target_col].rolling(window).mean()\n",
    "        df[f'{target_col}_roll_std_{window}'] = df[target_col].rolling(window).std()\n",
    "\n",
    "    # df['month'] = df.index.month\n",
    "    # df['weekofyear'] = df.index.isocalendar().week\n",
    "    # df['year'] = df.index.isocalendar().year\n",
    "    # df['sin_week'] = np.sin(2 * np.pi * df['weekofyear'] / 52)\n",
    "    # df['cos_week'] = np.cos(2 * np.pi * df['weekofyear'] / 52)\n",
    "    # df['EMA_3'] = df[target_col].ewm(span=3, adjust=False).mean()\n",
    "    # df['EMA_6'] = df[target_col].ewm(span=6, adjust=False).mean()\n",
    "    # df['correlation_target_month'] = df[target_col].rolling(window=6).corr(df['month'])\n",
    "    # df['correlation_target_week'] = df[target_col].rolling(window=6).corr(df['weekofyear'])\n",
    "\n",
    "    # fft_values = fft(df[target_col].dropna().values)\n",
    "    # fft_real = np.real(fft_values)[:len(df[target_col])]\n",
    "    # fft_imag = np.imag(fft_values)[:len(df[target_col])]\n",
    "    # df['fft_real'] = np.concatenate([fft_real, np.nan * np.ones(len(df) - len(fft_real))])\n",
    "    # df['fft_imag'] = np.concatenate([fft_imag, np.nan * np.ones(len(df) - len(fft_imag))])\n",
    "\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "    return df, target_col\n",
    "\n",
    "# ============================================\n",
    "# 2. Feature Scaling\n",
    "# ============================================\n",
    "def feature_scaling(df, target_col):\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col].astype(int)\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))  # 3D for Transformer\n",
    "    return X_scaled, y, scaler\n",
    "\n",
    "# ============================================\n",
    "# 3. Transformer Model\n",
    "# ============================================\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = models.Sequential([\n",
    "            layers.Dense(ff_dim, activation=\"relu\"),\n",
    "            layers.Dense(embed_dim),\n",
    "        ])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        attn_output = self.att(inputs, inputs, training=training)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1, training=training)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "def build_transformer_model(input_shape, num_classes=4, embed_dim=64, num_heads=2, ff_dim=128, dropout_rate=0.1):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = layers.Dense(embed_dim)(inputs)\n",
    "    x = TransformerBlock(embed_dim, num_heads, ff_dim, dropout_rate)(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    x = layers.Dense(ff_dim, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "# ============================================\n",
    "# 4. Objective Function\n",
    "# ============================================\n",
    "def objective_function(num_units, dropout_rate, epochs, batch_size, X_train, y_train):\n",
    "    model = build_transformer_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        num_classes=4,\n",
    "        embed_dim=int(num_units),\n",
    "        num_heads=2,\n",
    "        ff_dim=int(num_units * 2),\n",
    "        dropout_rate=dropout_rate\n",
    "    )\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=int(epochs),\n",
    "        batch_size=int(batch_size),\n",
    "        validation_split=0.2,\n",
    "        verbose=0\n",
    "    )\n",
    "    return np.max(history.history['val_accuracy'])\n",
    "\n",
    "# ============================================\n",
    "# 5. Optimize Hyperparameters\n",
    "# ============================================\n",
    "def optimize_hyperparameters(X_train, y_train):\n",
    "    pbounds = {\n",
    "        'num_units': (32, 128),\n",
    "        'dropout_rate': (0.1, 0.5),\n",
    "        'epochs': (30, 60),\n",
    "        'batch_size': (16, 64)\n",
    "    }\n",
    "\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=lambda num_units, dropout_rate, epochs, batch_size: objective_function(\n",
    "            num_units, dropout_rate, epochs, batch_size, X_train, y_train\n",
    "        ),\n",
    "        pbounds=pbounds,\n",
    "        random_state=42,\n",
    "        verbose=2\n",
    "    )\n",
    "    optimizer.maximize(init_points=5, n_iter=15)\n",
    "    return optimizer.max['params']\n",
    "\n",
    "# ============================================\n",
    "# 6. Train and Evaluate\n",
    "# ============================================\n",
    "def train_and_evaluate_model(X_train, X_test, y_train, y_test, best_params):\n",
    "    model = build_transformer_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        num_classes=4,\n",
    "        embed_dim=int(best_params['num_units']),\n",
    "        num_heads=2,\n",
    "        ff_dim=int(best_params['num_units'] * 2),\n",
    "        dropout_rate=float(best_params['dropout_rate'])\n",
    "    )\n",
    "\n",
    "    # Training time\n",
    "    start_train = time.time()\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=int(best_params['epochs']),\n",
    "        batch_size=int(best_params['batch_size']),\n",
    "        validation_split=0.2,\n",
    "        verbose=1\n",
    "    )\n",
    "    training_time = time.time() - start_train\n",
    "\n",
    "    # Testing time\n",
    "    start_test = time.time()\n",
    "    y_pred = model.predict(X_test).argmax(axis=1)\n",
    "    total_testing_time = time.time() - start_test\n",
    "    inference_time_per_sample = total_testing_time / len(X_test)\n",
    "\n",
    "    print(\"\\n===== Evaluation =====\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(classification_report(y_test, y_pred, digits=4))\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    model.save(\"temp_model.h5\")\n",
    "    model_size = os.path.getsize(\"temp_model.h5\") / (1024 ** 2)\n",
    "    os.remove(\"temp_model.h5\")\n",
    "\n",
    "    total_params = np.sum([np.prod(v.shape) for v in model.trainable_weights])\n",
    "    ram_usage = psutil.Process(os.getpid()).memory_info().rss / (1024 ** 2)\n",
    "\n",
    "    print(\"\\n===== Model Metrics =====\")\n",
    "    print(f\"Training Time: {training_time:.2f}s\")\n",
    "    print(f\"Total Testing Time: {total_testing_time:.2f}s\")\n",
    "    print(f\"Inference Time per Sample: {inference_time_per_sample:.6f}s\")\n",
    "    print(f\"Model Size: {model_size:.2f} MB\")\n",
    "    print(f\"Trainable Params: {total_params:,}\")\n",
    "    print(f\"RAM Usage: {ram_usage:.2f} MB\")\n",
    "\n",
    "    return model, history\n",
    "\n",
    "# ============================================\n",
    "# 7. Main Execution\n",
    "# ============================================\n",
    "def main():\n",
    "    train_path = r\"E:\\Abroad period research\\Phenology datasets\\PHENOLOGY_H1\\Cordoba_train.csv\"\n",
    "    test_path = r\"E:\\Abroad period research\\Phenology datasets\\PHENOLOGY_H1\\Cordoba_test.csv\"\n",
    "\n",
    "    df_train, target_col = load_and_preprocess_data(train_path)\n",
    "    X_train, y_train, _ = feature_scaling(df_train, target_col)\n",
    "\n",
    "    df_test, _ = load_and_preprocess_data(test_path)\n",
    "    X_test, y_test, _ = feature_scaling(df_test, target_col)\n",
    "\n",
    "    print(\"\\n Optimizing Hyperparameters...\")\n",
    "    best_params = optimize_hyperparameters(X_train, y_train)\n",
    "    print(\"\\n Best Hyperparameters:\", best_params)\n",
    "\n",
    "    print(\"\\n Training Final Transformer Model...\")\n",
    "    model, history = train_and_evaluate_model(X_train, X_test, y_train, y_test, best_params)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c760f6",
   "metadata": {},
   "source": [
    "Transformer for Jaen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a57be6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Optimizing Hyperparameters...\n",
      "|   iter    |  target   | batch_... | dropou... |  epochs   | num_units |\n",
      "-------------------------------------------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m33.98    \u001b[39m | \u001b[39m0.4803   \u001b[39m | \u001b[39m51.96    \u001b[39m | \u001b[39m89.47    \u001b[39m |\n",
      "| \u001b[39m2        \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m23.49    \u001b[39m | \u001b[39m0.1624   \u001b[39m | \u001b[39m31.74    \u001b[39m | \u001b[39m115.2    \u001b[39m |\n",
      "| \u001b[39m3        \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m44.85    \u001b[39m | \u001b[39m0.3832   \u001b[39m | \u001b[39m30.62    \u001b[39m | \u001b[39m125.1    \u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m0.98     \u001b[39m | \u001b[39m55.96    \u001b[39m | \u001b[39m0.1849   \u001b[39m | \u001b[39m35.45    \u001b[39m | \u001b[39m49.61    \u001b[39m |\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m30.6     \u001b[39m | \u001b[39m0.3099   \u001b[39m | \u001b[39m42.96    \u001b[39m | \u001b[39m59.96    \u001b[39m |\n",
      "| \u001b[39m6        \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m35.47    \u001b[39m | \u001b[39m0.1346   \u001b[39m | \u001b[39m30.11    \u001b[39m | \u001b[39m118.2    \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m16.22    \u001b[39m | \u001b[39m0.1457   \u001b[39m | \u001b[39m31.66    \u001b[39m | \u001b[39m78.0     \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m23.79    \u001b[39m | \u001b[39m0.3341   \u001b[39m | \u001b[39m30.08    \u001b[39m | \u001b[39m116.1    \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m16.5     \u001b[39m | \u001b[39m0.4436   \u001b[39m | \u001b[39m59.54    \u001b[39m | \u001b[39m54.2     \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m62.47    \u001b[39m | \u001b[39m0.3844   \u001b[39m | \u001b[39m59.61    \u001b[39m | \u001b[39m127.1    \u001b[39m |\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m18.27    \u001b[39m | \u001b[39m0.3674   \u001b[39m | \u001b[39m59.63    \u001b[39m | \u001b[39m128.0    \u001b[39m |\n",
      "| \u001b[39m12       \u001b[39m | \u001b[39m0.98     \u001b[39m | \u001b[39m16.51    \u001b[39m | \u001b[39m0.4038   \u001b[39m | \u001b[39m31.71    \u001b[39m | \u001b[39m42.02    \u001b[39m |\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m17.45    \u001b[39m | \u001b[39m0.1007   \u001b[39m | \u001b[39m59.5     \u001b[39m | \u001b[39m75.77    \u001b[39m |\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m0.98     \u001b[39m | \u001b[39m62.85    \u001b[39m | \u001b[39m0.4165   \u001b[39m | \u001b[39m30.59    \u001b[39m | \u001b[39m100.8    \u001b[39m |\n",
      "| \u001b[39m15       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m41.26    \u001b[39m | \u001b[39m0.1957   \u001b[39m | \u001b[39m59.68    \u001b[39m | \u001b[39m127.8    \u001b[39m |\n",
      "| \u001b[39m16       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m38.22    \u001b[39m | \u001b[39m0.283    \u001b[39m | \u001b[39m59.3     \u001b[39m | \u001b[39m66.36    \u001b[39m |\n",
      "| \u001b[39m17       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m17.42    \u001b[39m | \u001b[39m0.3338   \u001b[39m | \u001b[39m59.11    \u001b[39m | \u001b[39m105.4    \u001b[39m |\n",
      "| \u001b[39m18       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m26.36    \u001b[39m | \u001b[39m0.2062   \u001b[39m | \u001b[39m37.64    \u001b[39m | \u001b[39m127.8    \u001b[39m |\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m16.93    \u001b[39m | \u001b[39m0.3333   \u001b[39m | \u001b[39m42.35    \u001b[39m | \u001b[39m94.19    \u001b[39m |\n",
      "| \u001b[39m20       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m29.12    \u001b[39m | \u001b[39m0.1147   \u001b[39m | \u001b[39m45.19    \u001b[39m | \u001b[39m73.98    \u001b[39m |\n",
      "=========================================================================\n",
      "\n",
      " Best Hyperparameters: {'batch_size': 33.977925704673396, 'dropout_rate': 0.4802857225639665, 'epochs': 51.95981825434215, 'num_units': 89.47121448291551}\n",
      "\n",
      " Training Final Transformer Model...\n",
      "Epoch 1/51\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 64ms/step - accuracy: 0.3057 - loss: 1.9159 - val_accuracy: 0.6000 - val_loss: 1.0022\n",
      "Epoch 2/51\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5799 - loss: 1.0826 - val_accuracy: 0.6800 - val_loss: 0.8493\n",
      "Epoch 3/51\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5706 - loss: 1.0187 - val_accuracy: 0.6800 - val_loss: 0.6214\n",
      "Epoch 4/51\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7168 - loss: 0.7546 - val_accuracy: 0.7600 - val_loss: 0.4689\n",
      "Epoch 5/51\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7362 - loss: 0.5800 - val_accuracy: 0.8000 - val_loss: 0.3627\n",
      "Epoch 6/51\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7860 - loss: 0.5276 - val_accuracy: 0.8200 - val_loss: 0.2661\n",
      "Epoch 7/51\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7952 - loss: 0.4703 - val_accuracy: 0.8200 - val_loss: 0.2874\n",
      "Epoch 8/51\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8577 - loss: 0.3804 - val_accuracy: 0.8200 - val_loss: 0.2906\n",
      "Epoch 9/51\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8611 - loss: 0.3575 - val_accuracy: 0.8600 - val_loss: 0.2157\n",
      "Epoch 10/51\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9014 - loss: 0.2840 - val_accuracy: 0.8600 - val_loss: 0.1897\n",
      "Epoch 11/51\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8729 - loss: 0.3216 - val_accuracy: 0.8800 - val_loss: 0.1899\n",
      "Epoch 12/51\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9254 - loss: 0.2851 - val_accuracy: 0.9200 - val_loss: 0.1703\n",
      "Epoch 13/51\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8776 - loss: 0.2972 - val_accuracy: 0.9400 - val_loss: 0.1339\n",
      "Epoch 14/51\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9333 - loss: 0.1918 - val_accuracy: 0.9400 - val_loss: 0.1238\n",
      "Epoch 15/51\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9379 - loss: 0.2235 - val_accuracy: 0.9400 - val_loss: 0.1129\n",
      "Epoch 16/51\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8837 - loss: 0.2192 - val_accuracy: 0.9400 - val_loss: 0.1081\n",
      "Epoch 17/51\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9023 - loss: 0.1908 - val_accuracy: 0.9400 - val_loss: 0.0919\n",
      "Epoch 18/51\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9664 - loss: 0.1313 - val_accuracy: 0.9400 - val_loss: 0.0907\n",
      "Epoch 19/51\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9338 - loss: 0.1950 - val_accuracy: 0.9600 - val_loss: 0.0851\n",
      "Epoch 20/51\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9486 - loss: 0.1374 - val_accuracy: 0.9200 - val_loss: 0.1458\n",
      "Epoch 21/51\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9525 - loss: 0.1419 - val_accuracy: 0.9400 - val_loss: 0.1306\n",
      "Epoch 22/51\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9501 - loss: 0.1571 - val_accuracy: 0.9400 - val_loss: 0.1209\n",
      "Epoch 23/51\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9375 - loss: 0.1211 - val_accuracy: 0.9800 - val_loss: 0.0853\n",
      "Epoch 24/51\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9603 - loss: 0.0989 - val_accuracy: 0.9600 - val_loss: 0.0846\n",
      "Epoch 25/51\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9479 - loss: 0.1753 - val_accuracy: 0.9800 - val_loss: 0.0484\n",
      "Epoch 26/51\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9311 - loss: 0.2346 - val_accuracy: 0.9600 - val_loss: 0.0925\n",
      "Epoch 27/51\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9337 - loss: 0.1436 - val_accuracy: 0.9400 - val_loss: 0.0895\n",
      "Epoch 28/51\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9598 - loss: 0.0921 - val_accuracy: 0.9800 - val_loss: 0.0831\n",
      "Epoch 29/51\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9616 - loss: 0.0687 - val_accuracy: 0.9600 - val_loss: 0.0742\n",
      "Epoch 30/51\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9722 - loss: 0.0830 - val_accuracy: 0.9600 - val_loss: 0.0987\n",
      "Epoch 31/51\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9465 - loss: 0.1389 - val_accuracy: 0.9400 - val_loss: 0.1111\n",
      "Epoch 32/51\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9729 - loss: 0.0712 - val_accuracy: 0.9800 - val_loss: 0.0732\n",
      "Epoch 33/51\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9725 - loss: 0.0676 - val_accuracy: 0.9800 - val_loss: 0.0706\n",
      "Epoch 34/51\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9801 - loss: 0.0716 - val_accuracy: 0.9400 - val_loss: 0.0868\n",
      "Epoch 35/51\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9842 - loss: 0.0584 - val_accuracy: 0.9800 - val_loss: 0.0618\n",
      "Epoch 36/51\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9928 - loss: 0.0443 - val_accuracy: 0.9800 - val_loss: 0.0498\n",
      "Epoch 37/51\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9654 - loss: 0.0814 - val_accuracy: 0.9400 - val_loss: 0.0789\n",
      "Epoch 38/51\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9801 - loss: 0.0695 - val_accuracy: 0.9400 - val_loss: 0.0926\n",
      "Epoch 39/51\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9835 - loss: 0.0502 - val_accuracy: 0.9800 - val_loss: 0.0442\n",
      "Epoch 40/51\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9925 - loss: 0.0375 - val_accuracy: 0.9800 - val_loss: 0.0228\n",
      "Epoch 41/51\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9914 - loss: 0.0449 - val_accuracy: 1.0000 - val_loss: 0.0215\n",
      "Epoch 42/51\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9788 - loss: 0.0640 - val_accuracy: 1.0000 - val_loss: 0.0183\n",
      "Epoch 43/51\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9907 - loss: 0.0359 - val_accuracy: 1.0000 - val_loss: 0.0186\n",
      "Epoch 44/51\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9679 - loss: 0.0743 - val_accuracy: 0.9600 - val_loss: 0.0785\n",
      "Epoch 45/51\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9738 - loss: 0.0825 - val_accuracy: 0.9800 - val_loss: 0.0447\n",
      "Epoch 46/51\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9863 - loss: 0.0557 - val_accuracy: 0.9800 - val_loss: 0.0615\n",
      "Epoch 47/51\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9759 - loss: 0.0516 - val_accuracy: 1.0000 - val_loss: 0.0161\n",
      "Epoch 48/51\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9863 - loss: 0.0484 - val_accuracy: 1.0000 - val_loss: 0.0228\n",
      "Epoch 49/51\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9977 - loss: 0.0174 - val_accuracy: 1.0000 - val_loss: 0.0235\n",
      "Epoch 50/51\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9745 - loss: 0.0669 - val_accuracy: 1.0000 - val_loss: 0.0079\n",
      "Epoch 51/51\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9918 - loss: 0.0232 - val_accuracy: 1.0000 - val_loss: 0.0039\n",
      "WARNING:tensorflow:5 out of the last 15 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000002074F616C00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 15 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000002074F616C00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\n",
      "===== Evaluation =====\n",
      "Accuracy: 0.9185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9000    0.6923    0.7826        13\n",
      "           1     0.9302    0.8000    0.8602        50\n",
      "           2     0.9302    0.9917    0.9600       121\n",
      "           3     0.0000    0.0000    0.0000         0\n",
      "\n",
      "    accuracy                         0.9185       184\n",
      "   macro avg     0.6901    0.6210    0.6507       184\n",
      "weighted avg     0.9281    0.9185    0.9204       184\n",
      "\n",
      "[[  9   3   0   1]\n",
      " [  1  40   9   0]\n",
      " [  0   0 120   1]\n",
      " [  0   0   0   0]]\n",
      "\n",
      "===== Model Metrics =====\n",
      "Training Time: 9.11s\n",
      "Total Testing Time: 0.49s\n",
      "Inference Time per Sample: 0.002653s\n",
      "Model Size: 1.39 MB\n",
      "Trainable Params: 115,437\n",
      "RAM Usage: 2794.13 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import psutil\n",
    "import warnings\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from tensorflow.keras import layers, models\n",
    "import tensorflow as tf\n",
    "from bayes_opt import BayesianOptimization\n",
    "from scipy.fftpack import fft\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================\n",
    "# 1. Load and Preprocess Data\n",
    "# ============================================\n",
    "\n",
    "def load_and_preprocess_data(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df.columns = df.columns.str.replace('\"', '').str.strip()\n",
    "    if 'TIME' not in df.columns:\n",
    "        raise KeyError(\"Column 'TIME' missing.\")\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'], errors='coerce')\n",
    "    df.dropna(subset=['TIME'], inplace=True)\n",
    "    df.set_index('TIME', inplace=True)\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "    target_col = 'fenologia_h1'\n",
    "    if target_col not in df.columns:\n",
    "        target_col = 'target'\n",
    "    if target_col not in df.columns:\n",
    "        raise KeyError(\"Target column not found.\")\n",
    "\n",
    "    df[target_col].interpolate(method='linear', inplace=True)\n",
    "    df[target_col] = df[target_col].astype(int)\n",
    "    df[target_col] = df[target_col] - 1\n",
    "\n",
    "    for i in range(1, 4):\n",
    "        df[f'{target_col}_lag{i}'] = df[target_col].shift(i)\n",
    "    for window in [3, 6]:\n",
    "        df[f'{target_col}_roll_mean_{window}'] = df[target_col].rolling(window).mean()\n",
    "        df[f'{target_col}_roll_std_{window}'] = df[target_col].rolling(window).std()\n",
    "\n",
    "    df['month'] = df.index.month\n",
    "    df['weekofyear'] = df.index.isocalendar().week\n",
    "    df['year'] = df.index.isocalendar().year\n",
    "    df['sin_week'] = np.sin(2 * np.pi * df['weekofyear'] / 52)\n",
    "    df['cos_week'] = np.cos(2 * np.pi * df['weekofyear'] / 52)\n",
    "    df['EMA_3'] = df[target_col].ewm(span=3, adjust=False).mean()\n",
    "    df['EMA_6'] = df[target_col].ewm(span=6, adjust=False).mean()\n",
    "    df['correlation_target_month'] = df[target_col].rolling(window=6).corr(df['month'])\n",
    "    df['correlation_target_week'] = df[target_col].rolling(window=6).corr(df['weekofyear'])\n",
    "\n",
    "    # fft_values = fft(df[target_col].dropna().values)\n",
    "    # fft_real = np.real(fft_values)[:len(df[target_col])]\n",
    "    # fft_imag = np.imag(fft_values)[:len(df[target_col])]\n",
    "    # df['fft_real'] = np.concatenate([fft_real, np.nan * np.ones(len(df) - len(fft_real))])\n",
    "    # df['fft_imag'] = np.concatenate([fft_imag, np.nan * np.ones(len(df) - len(fft_imag))])\n",
    "\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "    return df, target_col\n",
    "\n",
    "# ============================================\n",
    "# 2. Feature Scaling\n",
    "# ============================================\n",
    "def feature_scaling(df, target_col):\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col].astype(int)\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))  # 3D for Transformer\n",
    "    return X_scaled, y, scaler\n",
    "\n",
    "# ============================================\n",
    "# 3. Transformer Model\n",
    "# ============================================\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = models.Sequential([\n",
    "            layers.Dense(ff_dim, activation=\"relu\"),\n",
    "            layers.Dense(embed_dim),\n",
    "        ])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        attn_output = self.att(inputs, inputs, training=training)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1, training=training)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "def build_transformer_model(input_shape, num_classes=4, embed_dim=64, num_heads=2, ff_dim=128, dropout_rate=0.1):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = layers.Dense(embed_dim)(inputs)\n",
    "    x = TransformerBlock(embed_dim, num_heads, ff_dim, dropout_rate)(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    x = layers.Dense(ff_dim, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "# ============================================\n",
    "# 4. Objective Function\n",
    "# ============================================\n",
    "def objective_function(num_units, dropout_rate, epochs, batch_size, X_train, y_train):\n",
    "    model = build_transformer_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        num_classes=4,\n",
    "        embed_dim=int(num_units),\n",
    "        num_heads=2,\n",
    "        ff_dim=int(num_units * 2),\n",
    "        dropout_rate=dropout_rate\n",
    "    )\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=int(epochs),\n",
    "        batch_size=int(batch_size),\n",
    "        validation_split=0.2,\n",
    "        verbose=0\n",
    "    )\n",
    "    return np.max(history.history['val_accuracy'])\n",
    "\n",
    "# ============================================\n",
    "# 5. Optimize Hyperparameters\n",
    "# ============================================\n",
    "def optimize_hyperparameters(X_train, y_train):\n",
    "    pbounds = {\n",
    "        'num_units': (32, 128),\n",
    "        'dropout_rate': (0.1, 0.5),\n",
    "        'epochs': (30, 60),\n",
    "        'batch_size': (16, 64)\n",
    "    }\n",
    "\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=lambda num_units, dropout_rate, epochs, batch_size: objective_function(\n",
    "            num_units, dropout_rate, epochs, batch_size, X_train, y_train\n",
    "        ),\n",
    "        pbounds=pbounds,\n",
    "        random_state=42,\n",
    "        verbose=2\n",
    "    )\n",
    "    optimizer.maximize(init_points=5, n_iter=15)\n",
    "    return optimizer.max['params']\n",
    "\n",
    "# ============================================\n",
    "# 6. Train and Evaluate\n",
    "# ============================================\n",
    "def train_and_evaluate_model(X_train, X_test, y_train, y_test, best_params):\n",
    "    model = build_transformer_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        num_classes=4,\n",
    "        embed_dim=int(best_params['num_units']),\n",
    "        num_heads=2,\n",
    "        ff_dim=int(best_params['num_units'] * 2),\n",
    "        dropout_rate=float(best_params['dropout_rate'])\n",
    "    )\n",
    "\n",
    "    # Training time\n",
    "    start_train = time.time()\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=int(best_params['epochs']),\n",
    "        batch_size=int(best_params['batch_size']),\n",
    "        validation_split=0.2,\n",
    "        verbose=1\n",
    "    )\n",
    "    training_time = time.time() - start_train\n",
    "\n",
    "    # Testing time\n",
    "    start_test = time.time()\n",
    "    y_pred = model.predict(X_test).argmax(axis=1)\n",
    "    total_testing_time = time.time() - start_test\n",
    "    inference_time_per_sample = total_testing_time / len(X_test)\n",
    "\n",
    "    print(\"\\n===== Evaluation =====\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(classification_report(y_test, y_pred, digits=4))\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    model.save(\"temp_model.h5\")\n",
    "    model_size = os.path.getsize(\"temp_model.h5\") / (1024 ** 2)\n",
    "    os.remove(\"temp_model.h5\")\n",
    "\n",
    "    total_params = np.sum([np.prod(v.shape) for v in model.trainable_weights])\n",
    "    ram_usage = psutil.Process(os.getpid()).memory_info().rss / (1024 ** 2)\n",
    "\n",
    "    print(\"\\n===== Model Metrics =====\")\n",
    "    print(f\"Training Time: {training_time:.2f}s\")\n",
    "    print(f\"Total Testing Time: {total_testing_time:.2f}s\")\n",
    "    print(f\"Inference Time per Sample: {inference_time_per_sample:.6f}s\")\n",
    "    print(f\"Model Size: {model_size:.2f} MB\")\n",
    "    print(f\"Trainable Params: {total_params:,}\")\n",
    "    print(f\"RAM Usage: {ram_usage:.2f} MB\")\n",
    "\n",
    "    return model, history\n",
    "\n",
    "# ============================================\n",
    "# 7. Main Execution\n",
    "# ============================================\n",
    "def main():\n",
    "    train_path = r\"E:\\Abroad period research\\Phenology datasets\\PHENOLOGY_H1\\Jaen_train.csv\"\n",
    "    test_path = r\"E:\\Abroad period research\\Phenology datasets\\PHENOLOGY_H1\\Jaen_test.csv\"\n",
    "\n",
    "    df_train, target_col = load_and_preprocess_data(train_path)\n",
    "    X_train, y_train, _ = feature_scaling(df_train, target_col)\n",
    "\n",
    "    df_test, _ = load_and_preprocess_data(test_path)\n",
    "    X_test, y_test, _ = feature_scaling(df_test, target_col)\n",
    "\n",
    "    print(\"\\n Optimizing Hyperparameters...\")\n",
    "    best_params = optimize_hyperparameters(X_train, y_train)\n",
    "    print(\"\\n Best Hyperparameters:\", best_params)\n",
    "\n",
    "    print(\"\\n Training Final Transformer Model...\")\n",
    "    model, history = train_and_evaluate_model(X_train, X_test, y_train, y_test, best_params)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ce78d1",
   "metadata": {},
   "source": [
    "Transformer for Sevilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce2de71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Optimizing Hyperparameters...\n",
      "|   iter    |  target   | batch_... | dropou... |  epochs   | num_units |\n",
      "-------------------------------------------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m33.98    \u001b[39m | \u001b[39m0.4803   \u001b[39m | \u001b[39m51.96    \u001b[39m | \u001b[39m89.47    \u001b[39m |\n",
      "| \u001b[39m2        \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m23.49    \u001b[39m | \u001b[39m0.1624   \u001b[39m | \u001b[39m31.74    \u001b[39m | \u001b[39m115.2    \u001b[39m |\n",
      "| \u001b[39m3        \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m44.85    \u001b[39m | \u001b[39m0.3832   \u001b[39m | \u001b[39m30.62    \u001b[39m | \u001b[39m125.1    \u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m55.96    \u001b[39m | \u001b[39m0.1849   \u001b[39m | \u001b[39m35.45    \u001b[39m | \u001b[39m49.61    \u001b[39m |\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m30.6     \u001b[39m | \u001b[39m0.3099   \u001b[39m | \u001b[39m42.96    \u001b[39m | \u001b[39m59.96    \u001b[39m |\n",
      "| \u001b[39m6        \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m18.1     \u001b[39m | \u001b[39m0.1947   \u001b[39m | \u001b[39m56.69    \u001b[39m | \u001b[39m32.88    \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m63.83    \u001b[39m | \u001b[39m0.1911   \u001b[39m | \u001b[39m59.46    \u001b[39m | \u001b[39m127.8    \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m63.87    \u001b[39m | \u001b[39m0.1809   \u001b[39m | \u001b[39m55.16    \u001b[39m | \u001b[39m32.23    \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m16.41    \u001b[39m | \u001b[39m0.4657   \u001b[39m | \u001b[39m59.32    \u001b[39m | \u001b[39m127.3    \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m17.67    \u001b[39m | \u001b[39m0.4729   \u001b[39m | \u001b[39m30.03    \u001b[39m | \u001b[39m32.79    \u001b[39m |\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m63.94    \u001b[39m | \u001b[39m0.4128   \u001b[39m | \u001b[39m30.35    \u001b[39m | \u001b[39m125.6    \u001b[39m |\n",
      "| \u001b[39m12       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m63.97    \u001b[39m | \u001b[39m0.1924   \u001b[39m | \u001b[39m57.2     \u001b[39m | \u001b[39m33.17    \u001b[39m |\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m16.94    \u001b[39m | \u001b[39m0.4823   \u001b[39m | \u001b[39m59.99    \u001b[39m | \u001b[39m125.4    \u001b[39m |\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m17.57    \u001b[39m | \u001b[39m0.4635   \u001b[39m | \u001b[39m32.5     \u001b[39m | \u001b[39m32.73    \u001b[39m |\n",
      "| \u001b[39m15       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m62.9     \u001b[39m | \u001b[39m0.1966   \u001b[39m | \u001b[39m31.93    \u001b[39m | \u001b[39m127.4    \u001b[39m |\n",
      "| \u001b[39m16       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m62.56    \u001b[39m | \u001b[39m0.4078   \u001b[39m | \u001b[39m30.15    \u001b[39m | \u001b[39m32.43    \u001b[39m |\n",
      "| \u001b[39m17       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m17.06    \u001b[39m | \u001b[39m0.4836   \u001b[39m | \u001b[39m59.22    \u001b[39m | \u001b[39m127.9    \u001b[39m |\n",
      "| \u001b[39m18       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m16.27    \u001b[39m | \u001b[39m0.1102   \u001b[39m | \u001b[39m59.58    \u001b[39m | \u001b[39m34.65    \u001b[39m |\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m62.79    \u001b[39m | \u001b[39m0.4373   \u001b[39m | \u001b[39m59.92    \u001b[39m | \u001b[39m127.9    \u001b[39m |\n",
      "| \u001b[39m20       \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m63.78    \u001b[39m | \u001b[39m0.1677   \u001b[39m | \u001b[39m58.89    \u001b[39m | \u001b[39m33.39    \u001b[39m |\n",
      "=========================================================================\n",
      "\n",
      " Best Hyperparameters: {'batch_size': 33.977925704673396, 'dropout_rate': 0.4802857225639665, 'epochs': 51.95981825434215, 'num_units': 89.47121448291551}\n",
      "\n",
      " Training Final Transformer Model...\n",
      "Epoch 1/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 49ms/step - accuracy: 0.2899 - loss: 1.7221 - val_accuracy: 0.6571 - val_loss: 0.6688\n",
      "Epoch 2/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.4925 - loss: 1.2707 - val_accuracy: 0.9000 - val_loss: 0.4059\n",
      "Epoch 3/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.5819 - loss: 0.9457 - val_accuracy: 0.9000 - val_loss: 0.2835\n",
      "Epoch 4/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.6802 - loss: 0.7795 - val_accuracy: 0.8857 - val_loss: 0.2261\n",
      "Epoch 5/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8118 - loss: 0.5327 - val_accuracy: 0.9429 - val_loss: 0.1617\n",
      "Epoch 6/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7520 - loss: 0.5141 - val_accuracy: 0.9714 - val_loss: 0.1177\n",
      "Epoch 7/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8230 - loss: 0.4247 - val_accuracy: 0.9714 - val_loss: 0.1164\n",
      "Epoch 8/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8793 - loss: 0.3294 - val_accuracy: 0.9714 - val_loss: 0.0651\n",
      "Epoch 9/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8772 - loss: 0.3103 - val_accuracy: 0.9714 - val_loss: 0.0703\n",
      "Epoch 10/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8736 - loss: 0.3284 - val_accuracy: 0.9857 - val_loss: 0.0479\n",
      "Epoch 11/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8905 - loss: 0.2434 - val_accuracy: 0.9571 - val_loss: 0.0786\n",
      "Epoch 12/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8720 - loss: 0.3688 - val_accuracy: 0.9857 - val_loss: 0.0399\n",
      "Epoch 13/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9102 - loss: 0.2289 - val_accuracy: 0.9857 - val_loss: 0.0355\n",
      "Epoch 14/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9280 - loss: 0.2490 - val_accuracy: 0.9857 - val_loss: 0.0285\n",
      "Epoch 15/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9144 - loss: 0.2907 - val_accuracy: 0.9857 - val_loss: 0.0408\n",
      "Epoch 16/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9191 - loss: 0.2095 - val_accuracy: 0.9857 - val_loss: 0.0302\n",
      "Epoch 17/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9315 - loss: 0.1819 - val_accuracy: 0.9857 - val_loss: 0.0299\n",
      "Epoch 18/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9290 - loss: 0.1724 - val_accuracy: 0.9857 - val_loss: 0.0446\n",
      "Epoch 19/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9182 - loss: 0.2402 - val_accuracy: 0.9857 - val_loss: 0.0256\n",
      "Epoch 20/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9448 - loss: 0.1476 - val_accuracy: 0.9857 - val_loss: 0.0308\n",
      "Epoch 21/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9350 - loss: 0.1654 - val_accuracy: 0.9857 - val_loss: 0.0295\n",
      "Epoch 22/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9494 - loss: 0.1578 - val_accuracy: 0.9857 - val_loss: 0.0242\n",
      "Epoch 23/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9164 - loss: 0.1963 - val_accuracy: 0.9857 - val_loss: 0.0281\n",
      "Epoch 24/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9565 - loss: 0.1310 - val_accuracy: 0.9857 - val_loss: 0.0311\n",
      "Epoch 25/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9513 - loss: 0.1707 - val_accuracy: 0.9857 - val_loss: 0.0205\n",
      "Epoch 26/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9642 - loss: 0.0995 - val_accuracy: 0.9857 - val_loss: 0.0188\n",
      "Epoch 27/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9574 - loss: 0.1596 - val_accuracy: 0.9857 - val_loss: 0.0335\n",
      "Epoch 28/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9468 - loss: 0.1072 - val_accuracy: 0.9857 - val_loss: 0.0284\n",
      "Epoch 29/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9513 - loss: 0.1089 - val_accuracy: 0.9857 - val_loss: 0.0169\n",
      "Epoch 30/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9554 - loss: 0.1419 - val_accuracy: 0.9857 - val_loss: 0.0355\n",
      "Epoch 31/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9525 - loss: 0.1413 - val_accuracy: 0.9857 - val_loss: 0.0314\n",
      "Epoch 32/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9600 - loss: 0.1287 - val_accuracy: 1.0000 - val_loss: 0.0063\n",
      "Epoch 33/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9613 - loss: 0.1030 - val_accuracy: 0.9857 - val_loss: 0.0308\n",
      "Epoch 34/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9711 - loss: 0.1100 - val_accuracy: 1.0000 - val_loss: 0.0014\n",
      "Epoch 35/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9778 - loss: 0.0564 - val_accuracy: 0.9857 - val_loss: 0.0120\n",
      "Epoch 36/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9846 - loss: 0.0783 - val_accuracy: 1.0000 - val_loss: 0.0021\n",
      "Epoch 37/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9693 - loss: 0.0587 - val_accuracy: 1.0000 - val_loss: 0.0043\n",
      "Epoch 38/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9930 - loss: 0.0470 - val_accuracy: 1.0000 - val_loss: 0.0014\n",
      "Epoch 39/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9781 - loss: 0.0447 - val_accuracy: 1.0000 - val_loss: 5.0380e-04\n",
      "Epoch 40/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9749 - loss: 0.0641 - val_accuracy: 1.0000 - val_loss: 4.0571e-04\n",
      "Epoch 41/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9799 - loss: 0.0750 - val_accuracy: 1.0000 - val_loss: 7.1845e-04\n",
      "Epoch 42/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9788 - loss: 0.0908 - val_accuracy: 0.9857 - val_loss: 0.0210\n",
      "Epoch 43/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9783 - loss: 0.0705 - val_accuracy: 1.0000 - val_loss: 0.0015\n",
      "Epoch 44/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9813 - loss: 0.0466 - val_accuracy: 1.0000 - val_loss: 6.3333e-04\n",
      "Epoch 45/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9650 - loss: 0.0707 - val_accuracy: 1.0000 - val_loss: 7.1077e-04\n",
      "Epoch 46/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9938 - loss: 0.0385 - val_accuracy: 1.0000 - val_loss: 0.0014\n",
      "Epoch 47/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9956 - loss: 0.0371 - val_accuracy: 1.0000 - val_loss: 1.6900e-04\n",
      "Epoch 48/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9883 - loss: 0.0519 - val_accuracy: 1.0000 - val_loss: 0.0017\n",
      "Epoch 49/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9886 - loss: 0.0306 - val_accuracy: 1.0000 - val_loss: 4.3204e-04\n",
      "Epoch 50/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9893 - loss: 0.0341 - val_accuracy: 1.0000 - val_loss: 2.3254e-04\n",
      "Epoch 51/51\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9746 - loss: 0.0539 - val_accuracy: 1.0000 - val_loss: 1.0649e-04\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000207533AD3A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 13 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000207533AD3A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Evaluation =====\n",
      "Accuracy: 0.9283\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7143    1.0000    0.8333        10\n",
      "           1     1.0000    0.8816    0.9371        76\n",
      "           2     0.9680    0.9453    0.9565       128\n",
      "           3     0.5294    1.0000    0.6923         9\n",
      "\n",
      "    accuracy                         0.9283       223\n",
      "   macro avg     0.8029    0.9567    0.8548       223\n",
      "weighted avg     0.9498    0.9283    0.9337       223\n",
      "\n",
      "[[ 10   0   0   0]\n",
      " [  4  67   4   1]\n",
      " [  0   0 121   7]\n",
      " [  0   0   0   9]]\n",
      "\n",
      "===== Model Metrics =====\n",
      "Training Time: 13.11s\n",
      "Total Testing Time: 0.62s\n",
      "Inference Time per Sample: 0.002788s\n",
      "Model Size: 1.39 MB\n",
      "Trainable Params: 115,437\n",
      "RAM Usage: 3301.42 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import psutil\n",
    "import warnings\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from tensorflow.keras import layers, models\n",
    "import tensorflow as tf\n",
    "from bayes_opt import BayesianOptimization\n",
    "from scipy.fftpack import fft\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================\n",
    "# 1. Load and Preprocess Data\n",
    "# ============================================\n",
    "\n",
    "def load_and_preprocess_data(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df.columns = df.columns.str.replace('\"', '').str.strip()\n",
    "    if 'TIME' not in df.columns:\n",
    "        raise KeyError(\"Column 'TIME' missing.\")\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'], errors='coerce')\n",
    "    df.dropna(subset=['TIME'], inplace=True)\n",
    "    df.set_index('TIME', inplace=True)\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "    target_col = 'fenologia_h1'\n",
    "    if target_col not in df.columns:\n",
    "        target_col = 'target'\n",
    "    if target_col not in df.columns:\n",
    "        raise KeyError(\"Target column not found.\")\n",
    "\n",
    "    df[target_col].interpolate(method='linear', inplace=True)\n",
    "    df[target_col] = df[target_col].astype(int)\n",
    "    df[target_col] = df[target_col] - 1\n",
    "\n",
    "    for i in range(1, 4):\n",
    "        df[f'{target_col}_lag{i}'] = df[target_col].shift(i)\n",
    "    for window in [3, 6]:\n",
    "        df[f'{target_col}_roll_mean_{window}'] = df[target_col].rolling(window).mean()\n",
    "        df[f'{target_col}_roll_std_{window}'] = df[target_col].rolling(window).std()\n",
    "\n",
    "    df['month'] = df.index.month\n",
    "    df['weekofyear'] = df.index.isocalendar().week\n",
    "    df['year'] = df.index.isocalendar().year\n",
    "    df['sin_week'] = np.sin(2 * np.pi * df['weekofyear'] / 52)\n",
    "    df['cos_week'] = np.cos(2 * np.pi * df['weekofyear'] / 52)\n",
    "    df['EMA_3'] = df[target_col].ewm(span=3, adjust=False).mean()\n",
    "    df['EMA_6'] = df[target_col].ewm(span=6, adjust=False).mean()\n",
    "    df['correlation_target_month'] = df[target_col].rolling(window=6).corr(df['month'])\n",
    "    df['correlation_target_week'] = df[target_col].rolling(window=6).corr(df['weekofyear'])\n",
    "\n",
    "    # fft_values = fft(df[target_col].dropna().values)\n",
    "    # fft_real = np.real(fft_values)[:len(df[target_col])]\n",
    "    # fft_imag = np.imag(fft_values)[:len(df[target_col])]\n",
    "    # df['fft_real'] = np.concatenate([fft_real, np.nan * np.ones(len(df) - len(fft_real))])\n",
    "    # df['fft_imag'] = np.concatenate([fft_imag, np.nan * np.ones(len(df) - len(fft_imag))])\n",
    "\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "    return df, target_col\n",
    "\n",
    "# ============================================\n",
    "# 2. Feature Scaling\n",
    "# ============================================\n",
    "def feature_scaling(df, target_col):\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col].astype(int)\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))  # 3D for Transformer\n",
    "    return X_scaled, y, scaler\n",
    "\n",
    "# ============================================\n",
    "# 3. Transformer Model\n",
    "# ============================================\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = models.Sequential([\n",
    "            layers.Dense(ff_dim, activation=\"relu\"),\n",
    "            layers.Dense(embed_dim),\n",
    "        ])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        attn_output = self.att(inputs, inputs, training=training)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1, training=training)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "def build_transformer_model(input_shape, num_classes=4, embed_dim=64, num_heads=2, ff_dim=128, dropout_rate=0.1):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = layers.Dense(embed_dim)(inputs)\n",
    "    x = TransformerBlock(embed_dim, num_heads, ff_dim, dropout_rate)(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    x = layers.Dense(ff_dim, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "# ============================================\n",
    "# 4. Objective Function\n",
    "# ============================================\n",
    "def objective_function(num_units, dropout_rate, epochs, batch_size, X_train, y_train):\n",
    "    model = build_transformer_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        num_classes=4,\n",
    "        embed_dim=int(num_units),\n",
    "        num_heads=2,\n",
    "        ff_dim=int(num_units * 2),\n",
    "        dropout_rate=dropout_rate\n",
    "    )\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=int(epochs),\n",
    "        batch_size=int(batch_size),\n",
    "        validation_split=0.2,\n",
    "        verbose=0\n",
    "    )\n",
    "    return np.max(history.history['val_accuracy'])\n",
    "\n",
    "# ============================================\n",
    "# 5. Optimize Hyperparameters\n",
    "# ============================================\n",
    "def optimize_hyperparameters(X_train, y_train):\n",
    "    pbounds = {\n",
    "        'num_units': (32, 128),\n",
    "        'dropout_rate': (0.1, 0.5),\n",
    "        'epochs': (30, 60),\n",
    "        'batch_size': (16, 64)\n",
    "    }\n",
    "\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=lambda num_units, dropout_rate, epochs, batch_size: objective_function(\n",
    "            num_units, dropout_rate, epochs, batch_size, X_train, y_train\n",
    "        ),\n",
    "        pbounds=pbounds,\n",
    "        random_state=42,\n",
    "        verbose=2\n",
    "    )\n",
    "    optimizer.maximize(init_points=5, n_iter=15)\n",
    "    return optimizer.max['params']\n",
    "\n",
    "# ============================================\n",
    "# 6. Train and Evaluate\n",
    "# ============================================\n",
    "def train_and_evaluate_model(X_train, X_test, y_train, y_test, best_params):\n",
    "    model = build_transformer_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        num_classes=4,\n",
    "        embed_dim=int(best_params['num_units']),\n",
    "        num_heads=2,\n",
    "        ff_dim=int(best_params['num_units'] * 2),\n",
    "        dropout_rate=float(best_params['dropout_rate'])\n",
    "    )\n",
    "\n",
    "    # Training time\n",
    "    start_train = time.time()\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=int(best_params['epochs']),\n",
    "        batch_size=int(best_params['batch_size']),\n",
    "        validation_split=0.2,\n",
    "        verbose=1\n",
    "    )\n",
    "    training_time = time.time() - start_train\n",
    "\n",
    "    # Testing time\n",
    "    start_test = time.time()\n",
    "    y_pred = model.predict(X_test).argmax(axis=1)\n",
    "    total_testing_time = time.time() - start_test\n",
    "    inference_time_per_sample = total_testing_time / len(X_test)\n",
    "\n",
    "    print(\"\\n===== Evaluation =====\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(classification_report(y_test, y_pred, digits=4))\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    model.save(\"temp_model.h5\")\n",
    "    model_size = os.path.getsize(\"temp_model.h5\") / (1024 ** 2)\n",
    "    os.remove(\"temp_model.h5\")\n",
    "\n",
    "    total_params = np.sum([np.prod(v.shape) for v in model.trainable_weights])\n",
    "    ram_usage = psutil.Process(os.getpid()).memory_info().rss / (1024 ** 2)\n",
    "\n",
    "    print(\"\\n===== Model Metrics =====\")\n",
    "    print(f\"Training Time: {training_time:.2f}s\")\n",
    "    print(f\"Total Testing Time: {total_testing_time:.2f}s\")\n",
    "    print(f\"Inference Time per Sample: {inference_time_per_sample:.6f}s\")\n",
    "    print(f\"Model Size: {model_size:.2f} MB\")\n",
    "    print(f\"Trainable Params: {total_params:,}\")\n",
    "    print(f\"RAM Usage: {ram_usage:.2f} MB\")\n",
    "\n",
    "    return model, history\n",
    "\n",
    "# ============================================\n",
    "# 7. Main Execution\n",
    "# ============================================\n",
    "def main():\n",
    "    train_path = r\"E:\\Abroad period research\\Phenology datasets\\PHENOLOGY_H1\\sevilla_train.csv\"\n",
    "    test_path = r\"E:\\Abroad period research\\Phenology datasets\\PHENOLOGY_H1\\sevilla_test.csv\"\n",
    "\n",
    "    df_train, target_col = load_and_preprocess_data(train_path)\n",
    "    X_train, y_train, _ = feature_scaling(df_train, target_col)\n",
    "\n",
    "    df_test, _ = load_and_preprocess_data(test_path)\n",
    "    X_test, y_test, _ = feature_scaling(df_test, target_col)\n",
    "\n",
    "    print(\"\\n Optimizing Hyperparameters...\")\n",
    "    best_params = optimize_hyperparameters(X_train, y_train)\n",
    "    print(\"\\n Best Hyperparameters:\", best_params)\n",
    "\n",
    "    print(\"\\n Training Final Transformer Model...\")\n",
    "    model, history = train_and_evaluate_model(X_train, X_test, y_train, y_test, best_params)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
