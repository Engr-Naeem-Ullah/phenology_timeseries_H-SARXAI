{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9daf1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c9c176a",
   "metadata": {},
   "source": [
    "Cadiz station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b952a023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimizing Hyperparameters...\n",
      "|   iter    |  target   | batch_... | dropou... |  epochs   | learni... |   x_dim   |   y_dim   |\n",
      "-------------------------------------------------------------------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.9538   \u001b[39m | \u001b[39m33.98    \u001b[39m | \u001b[39m0.4803   \u001b[39m | \u001b[39m86.6     \u001b[39m | \u001b[39m0.006187 \u001b[39m | \u001b[39m6.56     \u001b[39m | \u001b[39m6.56     \u001b[39m |\n",
      "| \u001b[39m2        \u001b[39m | \u001b[39m0.9385   \u001b[39m | \u001b[39m18.79    \u001b[39m | \u001b[39m0.4465   \u001b[39m | \u001b[39m80.06    \u001b[39m | \u001b[39m0.007227 \u001b[39m | \u001b[39m5.206    \u001b[39m | \u001b[39m14.7     \u001b[39m |\n",
      "| \u001b[39m3        \u001b[39m | \u001b[39m0.9231   \u001b[39m | \u001b[39m55.96    \u001b[39m | \u001b[39m0.1849   \u001b[39m | \u001b[39m59.09    \u001b[39m | \u001b[39m0.002242 \u001b[39m | \u001b[39m8.042    \u001b[39m | \u001b[39m10.25    \u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m0.9538   \u001b[39m | \u001b[39m36.73    \u001b[39m | \u001b[39m0.2165   \u001b[39m | \u001b[39m80.59    \u001b[39m | \u001b[39m0.001825 \u001b[39m | \u001b[39m7.921    \u001b[39m | \u001b[39m8.664    \u001b[39m |\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function TensorFlowTrainer.make_test_function.<locals>.one_step_on_iterator at 0x0000022C678D9940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m0.9385   \u001b[39m | \u001b[39m37.89    \u001b[39m | \u001b[39m0.4141   \u001b[39m | \u001b[39m59.98    \u001b[39m | \u001b[39m0.005385 \u001b[39m | \u001b[39m10.92    \u001b[39m | \u001b[39m5.465    \u001b[39m |\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function TensorFlowTrainer.make_test_function.<locals>.one_step_on_iterator at 0x0000022C68B17C40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "| \u001b[35m6        \u001b[39m | \u001b[35m0.9692   \u001b[39m | \u001b[35m45.8     \u001b[39m | \u001b[35m0.1183   \u001b[39m | \u001b[35m93.58    \u001b[39m | \u001b[35m0.009748 \u001b[39m | \u001b[35m14.69    \u001b[39m | \u001b[35m12.5     \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m0.9385   \u001b[39m | \u001b[39m47.03    \u001b[39m | \u001b[39m0.3115   \u001b[39m | \u001b[39m93.5     \u001b[39m | \u001b[39m0.009682 \u001b[39m | \u001b[39m14.99    \u001b[39m | \u001b[39m12.59    \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m0.9231   \u001b[39m | \u001b[39m26.1     \u001b[39m | \u001b[39m0.497    \u001b[39m | \u001b[39m60.7     \u001b[39m | \u001b[39m0.00716  \u001b[39m | \u001b[39m9.956    \u001b[39m | \u001b[39m7.877    \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m0.9538   \u001b[39m | \u001b[39m53.9     \u001b[39m | \u001b[39m0.3142   \u001b[39m | \u001b[39m94.74    \u001b[39m | \u001b[39m0.004845 \u001b[39m | \u001b[39m9.225    \u001b[39m | \u001b[39m11.54    \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m0.9077   \u001b[39m | \u001b[39m37.02    \u001b[39m | \u001b[39m0.1646   \u001b[39m | \u001b[39m57.01    \u001b[39m | \u001b[39m0.0005144\u001b[39m | \u001b[39m14.98    \u001b[39m | \u001b[39m10.33    \u001b[39m |\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m0.9077   \u001b[39m | \u001b[39m50.96    \u001b[39m | \u001b[39m0.2747   \u001b[39m | \u001b[39m71.09    \u001b[39m | \u001b[39m0.000789 \u001b[39m | \u001b[39m5.756    \u001b[39m | \u001b[39m11.66    \u001b[39m |\n",
      "| \u001b[39m12       \u001b[39m | \u001b[39m0.8769   \u001b[39m | \u001b[39m46.03    \u001b[39m | \u001b[39m0.3355   \u001b[39m | \u001b[39m94.1     \u001b[39m | \u001b[39m0.004511 \u001b[39m | \u001b[39m14.98    \u001b[39m | \u001b[39m12.34    \u001b[39m |\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m0.9692   \u001b[39m | \u001b[39m51.4     \u001b[39m | \u001b[39m0.4166   \u001b[39m | \u001b[39m78.82    \u001b[39m | \u001b[39m0.00421  \u001b[39m | \u001b[39m10.72    \u001b[39m | \u001b[39m5.745    \u001b[39m |\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m0.9538   \u001b[39m | \u001b[39m50.66    \u001b[39m | \u001b[39m0.1115   \u001b[39m | \u001b[39m88.68    \u001b[39m | \u001b[39m0.006832 \u001b[39m | \u001b[39m11.9     \u001b[39m | \u001b[39m10.36    \u001b[39m |\n",
      "| \u001b[39m15       \u001b[39m | \u001b[39m0.9385   \u001b[39m | \u001b[39m58.03    \u001b[39m | \u001b[39m0.2338   \u001b[39m | \u001b[39m66.8     \u001b[39m | \u001b[39m0.009438 \u001b[39m | \u001b[39m10.48    \u001b[39m | \u001b[39m7.241    \u001b[39m |\n",
      "=================================================================================================\n",
      "\n",
      "Best Parameters Found: {'batch_size': 45.8039448528389, 'dropout_rate': 0.11829681352500515, 'epochs': 93.57684030761882, 'learning_rate': 0.00974814520718527, 'x_dim': 14.688778552856915, 'y_dim': 12.496518317429247}\n",
      "\n",
      "Training Final Model...\n",
      "Epoch 1/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.3820 - loss: 1.3236\n",
      "Epoch 2/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7102 - loss: 1.0286 \n",
      "Epoch 3/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8885 - loss: 0.6642 \n",
      "Epoch 4/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9317 - loss: 0.3176 \n",
      "Epoch 5/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9323 - loss: 0.1850 \n",
      "Epoch 6/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9433 - loss: 0.1228 \n",
      "Epoch 7/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9708 - loss: 0.0919 \n",
      "Epoch 8/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9798 - loss: 0.0670 \n",
      "Epoch 9/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9776 - loss: 0.0522  \n",
      "Epoch 10/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9710 - loss: 0.0503 \n",
      "Epoch 11/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9798 - loss: 0.0537 \n",
      "Epoch 12/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9840 - loss: 0.0431 \n",
      "Epoch 13/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9770 - loss: 0.0406 \n",
      "Epoch 14/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9819 - loss: 0.0478 \n",
      "Epoch 15/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9902 - loss: 0.0451 \n",
      "Epoch 16/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9821 - loss: 0.0485 \n",
      "Epoch 17/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9723 - loss: 0.0424 \n",
      "Epoch 18/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9802 - loss: 0.0526 \n",
      "Epoch 19/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9851 - loss: 0.0375 \n",
      "Epoch 20/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9762 - loss: 0.0396 \n",
      "Epoch 21/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9741 - loss: 0.0526 \n",
      "Epoch 22/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9866 - loss: 0.0315 \n",
      "Epoch 23/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9858 - loss: 0.0316 \n",
      "Epoch 24/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9832 - loss: 0.0354 \n",
      "Epoch 25/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9798 - loss: 0.0444 \n",
      "Epoch 26/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9775 - loss: 0.0393 \n",
      "Epoch 27/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9727 - loss: 0.0465 \n",
      "Epoch 28/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9832 - loss: 0.0412 \n",
      "Epoch 29/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9902 - loss: 0.0317 \n",
      "Epoch 30/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9741 - loss: 0.0490 \n",
      "Epoch 31/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9762 - loss: 0.0403 \n",
      "Epoch 32/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9892 - loss: 0.0285 \n",
      "Epoch 33/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9832 - loss: 0.0355 \n",
      "Epoch 34/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9750 - loss: 0.0417 \n",
      "Epoch 35/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9892 - loss: 0.0352 \n",
      "Epoch 36/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9778 - loss: 0.0376 \n",
      "Epoch 37/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9780 - loss: 0.0399 \n",
      "Epoch 38/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9804 - loss: 0.0447 \n",
      "Epoch 39/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9866 - loss: 0.0370 \n",
      "Epoch 40/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9857 - loss: 0.0377 \n",
      "Epoch 41/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9708 - loss: 0.0470 \n",
      "Epoch 42/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9761 - loss: 0.0422 \n",
      "Epoch 43/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9871 - loss: 0.0362 \n",
      "Epoch 44/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9686 - loss: 0.0441 \n",
      "Epoch 45/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9813 - loss: 0.0403 \n",
      "Epoch 46/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9798 - loss: 0.0377 \n",
      "Epoch 47/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9747 - loss: 0.0373 \n",
      "Epoch 48/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9731 - loss: 0.0337 \n",
      "Epoch 49/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9753 - loss: 0.0424 \n",
      "Epoch 50/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9727 - loss: 0.0419 \n",
      "Epoch 51/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9787 - loss: 0.0370 \n",
      "Epoch 52/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9853 - loss: 0.0355 \n",
      "Epoch 53/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9746 - loss: 0.0382 \n",
      "Epoch 54/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9728 - loss: 0.0397 \n",
      "Epoch 55/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9849 - loss: 0.0356 \n",
      "Epoch 56/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9807 - loss: 0.0357 \n",
      "Epoch 57/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9841 - loss: 0.0350 \n",
      "Epoch 58/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9883 - loss: 0.0344 \n",
      "Epoch 59/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9768 - loss: 0.0350 \n",
      "Epoch 60/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9742 - loss: 0.0410 \n",
      "Epoch 61/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9795 - loss: 0.0368 \n",
      "Epoch 62/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9847 - loss: 0.0360 \n",
      "Epoch 63/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9823 - loss: 0.0398 \n",
      "Epoch 64/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9821 - loss: 0.0371 \n",
      "Epoch 65/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9873 - loss: 0.0335 \n",
      "Epoch 66/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9857 - loss: 0.0373 \n",
      "Epoch 67/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9772 - loss: 0.0360 \n",
      "Epoch 68/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9736 - loss: 0.0401 \n",
      "Epoch 69/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9851 - loss: 0.0323 \n",
      "Epoch 70/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9787 - loss: 0.0385 \n",
      "Epoch 71/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9818 - loss: 0.0392 \n",
      "Epoch 72/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9748 - loss: 0.0366 \n",
      "Epoch 73/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9817 - loss: 0.0318 \n",
      "Epoch 74/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9829 - loss: 0.0360 \n",
      "Epoch 75/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9772 - loss: 0.0383 \n",
      "Epoch 76/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9823 - loss: 0.0360 \n",
      "Epoch 77/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9781 - loss: 0.0360 \n",
      "Epoch 78/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9847 - loss: 0.0385 \n",
      "Epoch 79/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9770 - loss: 0.0356 \n",
      "Epoch 80/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9843 - loss: 0.0322 \n",
      "Epoch 81/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9708 - loss: 0.0443 \n",
      "Epoch 82/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9798 - loss: 0.0320 \n",
      "Epoch 83/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9772 - loss: 0.0366 \n",
      "Epoch 84/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9728 - loss: 0.0382 \n",
      "Epoch 85/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9815 - loss: 0.0381 \n",
      "Epoch 86/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9847 - loss: 0.0354 \n",
      "Epoch 87/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9731 - loss: 0.0369 \n",
      "Epoch 88/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9762 - loss: 0.0353 \n",
      "Epoch 89/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9857 - loss: 0.0337 \n",
      "Epoch 90/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9786 - loss: 0.0348 \n",
      "Epoch 91/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9877 - loss: 0.0323 \n",
      "Epoch 92/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9877 - loss: 0.0355 \n",
      "Epoch 93/93\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9813 - loss: 0.0414 \n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "\n",
      "===== Evaluation =====\n",
      "Accuracy: 0.9181\n",
      "Report:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.6250    0.7692        16\n",
      "           1     0.8193    0.9855    0.8947        69\n",
      "           2     0.9773    0.9416    0.9591       137\n",
      "           3     0.8571    0.6000    0.7059        10\n",
      "\n",
      "    accuracy                         0.9181       232\n",
      "   macro avg     0.9134    0.7880    0.8322       232\n",
      "weighted avg     0.9267    0.9181    0.9160       232\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 10   5   0   1]\n",
      " [  0  68   1   0]\n",
      " [  0   8 129   0]\n",
      " [  0   2   2   6]]\n",
      "\n",
      "===== Model Metrics =====\n",
      "Training Time: 6.14s\n",
      "Inference/sample: 0.000707s\n",
      "Testing Time: 0.16s\n",
      "Model Size: 0.07 MB\n",
      "Trainable Params: 13,028\n",
      "RAM Usage: 645.25 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import psutil\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from minisom import MiniSom\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from bayes_opt import BayesianOptimization\n",
    "from scipy.fftpack import fft\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "def load_and_preprocess_data(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df.columns = df.columns.str.replace('\"', '').str.strip()\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'], errors='coerce')\n",
    "    df.dropna(subset=['TIME'], inplace=True)\n",
    "    df.set_index('TIME', inplace=True)\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "    target_col = 'fenologia_h1'\n",
    "    df[target_col].interpolate(method='linear', inplace=True)\n",
    "    df[target_col] = df[target_col] - 1\n",
    "\n",
    "    for i in range(1, 4):\n",
    "        df[f'{target_col}_lag{i}'] = df[target_col].shift(i)\n",
    "    for window in [3, 6]:\n",
    "        df[f'{target_col}_roll_mean_{window}'] = df[target_col].rolling(window).mean()\n",
    "        df[f'{target_col}_roll_std_{window}'] = df[target_col].rolling(window).std()\n",
    "\n",
    "    # df['month'] = df.index.month\n",
    "    # df['weekofyear'] = df.index.isocalendar().week\n",
    "    # df['year'] = df.index.isocalendar().year\n",
    "    # df['sin_week'] = np.sin(2 * np.pi * df['weekofyear'] / 52)\n",
    "    # df['cos_week'] = np.cos(2 * np.pi * df['weekofyear'] / 52)\n",
    "    # df['EMA_3'] = df[target_col].ewm(span=3, adjust=False).mean()\n",
    "    # df['EMA_6'] = df[target_col].ewm(span=6, adjust=False).mean()\n",
    "    # df['correlation_target_month'] = df[target_col].rolling(6).corr(df['month'])\n",
    "    # df['correlation_target_week'] = df[target_col].rolling(6).corr(df['weekofyear'])\n",
    "\n",
    "    # fft_vals = fft(df[target_col].dropna().values)\n",
    "    # fft_real = np.real(fft_vals)[:len(df[target_col])]\n",
    "    # fft_imag = np.imag(fft_vals)[:len(df[target_col])]\n",
    "    # df['fft_real'] = np.pad(fft_real, (0, len(df) - len(fft_real)), constant_values=np.nan)\n",
    "    # df['fft_imag'] = np.pad(fft_imag, (0, len(df) - len(fft_imag)), constant_values=np.nan)\n",
    "\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "    return df, target_col\n",
    "\n",
    "def feature_scaling(df, target_col):\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    return X_scaled, y, scaler\n",
    "\n",
    "def transform_with_som(som, X, y_dim, x_dim):\n",
    "    winners = np.array([som.winner(x) for x in X])\n",
    "    indices = winners[:, 0] * y_dim + winners[:, 1]\n",
    "    return to_categorical(indices, num_classes=x_dim * y_dim)\n",
    "\n",
    "def train_and_evaluate_som_dnn(X_train, X_test, y_train, y_test, x_dim, y_dim, epochs, batch_size, dropout_rate, learning_rate):\n",
    "    som = MiniSom(x_dim, y_dim, X_train.shape[1], sigma=1.0, learning_rate=learning_rate)\n",
    "    som.random_weights_init(X_train)\n",
    "    som.train_random(X_train, 100)\n",
    "\n",
    "    X_train_enc = transform_with_som(som, X_train, y_dim, x_dim)\n",
    "    X_test_enc = transform_with_som(som, X_test, y_dim, x_dim)\n",
    "    y_train_cat = to_categorical(y_train)\n",
    "    y_test_cat = to_categorical(y_test)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_dim=X_train_enc.shape[1]))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(y_train_cat.shape[1], activation='softmax'))\n",
    "    model.compile(optimizer=Adam(learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    start_train = time.time()\n",
    "    model.fit(X_train_enc, y_train_cat, epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "    training_time = time.time() - start_train\n",
    "\n",
    "    start_infer = time.time()\n",
    "    y_pred = model.predict(X_test_enc).argmax(axis=1)\n",
    "    inference_time = (time.time() - start_infer) / len(X_test_enc)\n",
    "    testing_time = time.time() - start_infer\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(\"\\n===== Evaluation =====\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(\"Report:\\n\", classification_report(y_test, y_pred, digits=4))\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "    model.save(\"temp_model_som_dnn.h5\", include_optimizer=False)\n",
    "    model_size = os.path.getsize(\"temp_model_som_dnn.h5\") / (1024 ** 2)\n",
    "    os.remove(\"temp_model_som_dnn.h5\")\n",
    "\n",
    "    total_params = np.sum([np.prod(v.shape) for v in model.trainable_weights])\n",
    "    ram_usage = psutil.Process(os.getpid()).memory_info().rss / (1024 ** 2)\n",
    "\n",
    "    print(\"\\n===== Model Metrics =====\")\n",
    "    print(f\"Training Time: {training_time:.2f}s\")\n",
    "    print(f\"Inference/sample: {inference_time:.6f}s\")\n",
    "    print(f\"Testing Time: {testing_time:.2f}s\")\n",
    "    print(f\"Model Size: {model_size:.2f} MB\")\n",
    "    print(f\"Trainable Params: {total_params:,}\")\n",
    "    print(f\"RAM Usage: {ram_usage:.2f} MB\")\n",
    "\n",
    "def objective_function(epochs, batch_size, x_dim, y_dim, dropout_rate, learning_rate, X_train, y_train, X_val, y_val):\n",
    "    x_dim, y_dim = int(x_dim), int(y_dim)\n",
    "    batch_size, epochs = int(batch_size), int(epochs)\n",
    "    dropout_rate = float(dropout_rate)\n",
    "    learning_rate = float(learning_rate)\n",
    "\n",
    "    som = MiniSom(x_dim, y_dim, X_train.shape[1], sigma=1.0, learning_rate=learning_rate)\n",
    "    som.random_weights_init(X_train)\n",
    "    som.train_random(X_train, 100)\n",
    "\n",
    "    X_train_enc = transform_with_som(som, X_train, y_dim, x_dim)\n",
    "    X_val_enc = transform_with_som(som, X_val, y_dim, x_dim)\n",
    "    y_train_cat = to_categorical(y_train)\n",
    "    y_val_cat = to_categorical(y_val)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_dim=X_train_enc.shape[1]))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(y_train_cat.shape[1], activation='softmax'))\n",
    "    model.compile(optimizer=Adam(learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(X_train_enc, y_train_cat, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    _, acc = model.evaluate(X_val_enc, y_val_cat, verbose=0)\n",
    "    return acc\n",
    "\n",
    "def optimize_som_dnn_hyperparameters(X, y):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    pbounds = {\n",
    "        'epochs': (50, 100),\n",
    "        'batch_size': (16, 64),\n",
    "        'x_dim': (5, 15),\n",
    "        'y_dim': (5, 15),\n",
    "        'dropout_rate': (0.1, 0.5),\n",
    "        'learning_rate': (0.0005, 0.01)\n",
    "    }\n",
    "\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=lambda epochs, batch_size, x_dim, y_dim, dropout_rate, learning_rate:\n",
    "            objective_function(epochs, batch_size, x_dim, y_dim, dropout_rate, learning_rate,\n",
    "                               X_train, y_train, X_val, y_val),\n",
    "        pbounds=pbounds,\n",
    "        random_state=42,\n",
    "        verbose=2\n",
    "    )\n",
    "    optimizer.maximize(init_points=5, n_iter=10)\n",
    "    return optimizer.max['params']\n",
    "\n",
    "def main():\n",
    "    train_path =  r\"E:\\Abroad period research\\Phenology datasets\\PHENOLOGY_H1\\Cadiz_train.csv\"\n",
    "    test_path =  r\"E:\\Abroad period research\\Phenology datasets\\PHENOLOGY_H1\\Cadiz_test.csv\"\n",
    "\n",
    "    df_train, target_col = load_and_preprocess_data(train_path)\n",
    "    X_train, y_train, _ = feature_scaling(df_train, target_col)\n",
    "\n",
    "    df_test, _ = load_and_preprocess_data(test_path)\n",
    "    X_test, y_test, _ = feature_scaling(df_test, target_col)\n",
    "\n",
    "    print(\"\\nOptimizing Hyperparameters...\")\n",
    "    best_params = optimize_som_dnn_hyperparameters(X_train, y_train)\n",
    "    print(\"\\nBest Parameters Found:\", best_params)\n",
    "\n",
    "    print(\"\\nTraining Final Model...\")\n",
    "    train_and_evaluate_som_dnn(\n",
    "        X_train, X_test, y_train, y_test,\n",
    "        x_dim=int(best_params['x_dim']),\n",
    "        y_dim=int(best_params['y_dim']),\n",
    "        epochs=int(best_params['epochs']),\n",
    "        batch_size=int(best_params['batch_size']),\n",
    "        dropout_rate=float(best_params['dropout_rate']),\n",
    "        learning_rate=float(best_params['learning_rate'])\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4912b1a2",
   "metadata": {},
   "source": [
    "Cordoba station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23c683fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimizing Hyperparameters...\n",
      "|   iter    |  target   | batch_... | dropou... |  epochs   | learni... |   x_dim   |   y_dim   |\n",
      "-------------------------------------------------------------------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.8387   \u001b[39m | \u001b[39m33.98    \u001b[39m | \u001b[39m0.4803   \u001b[39m | \u001b[39m86.6     \u001b[39m | \u001b[39m0.006187 \u001b[39m | \u001b[39m6.56     \u001b[39m | \u001b[39m6.56     \u001b[39m |\n",
      "| \u001b[35m2        \u001b[39m | \u001b[35m0.9677   \u001b[39m | \u001b[35m18.79    \u001b[39m | \u001b[35m0.4465   \u001b[39m | \u001b[35m80.06    \u001b[39m | \u001b[35m0.007227 \u001b[39m | \u001b[35m5.206    \u001b[39m | \u001b[35m14.7     \u001b[39m |\n",
      "| \u001b[39m3        \u001b[39m | \u001b[39m0.9677   \u001b[39m | \u001b[39m55.96    \u001b[39m | \u001b[39m0.1849   \u001b[39m | \u001b[39m59.09    \u001b[39m | \u001b[39m0.002242 \u001b[39m | \u001b[39m8.042    \u001b[39m | \u001b[39m10.25    \u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m0.871    \u001b[39m | \u001b[39m36.73    \u001b[39m | \u001b[39m0.2165   \u001b[39m | \u001b[39m80.59    \u001b[39m | \u001b[39m0.001825 \u001b[39m | \u001b[39m7.921    \u001b[39m | \u001b[39m8.664    \u001b[39m |\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m0.8065   \u001b[39m | \u001b[39m37.89    \u001b[39m | \u001b[39m0.4141   \u001b[39m | \u001b[39m59.98    \u001b[39m | \u001b[39m0.005385 \u001b[39m | \u001b[39m10.92    \u001b[39m | \u001b[39m5.465    \u001b[39m |\n",
      "| \u001b[39m6        \u001b[39m | \u001b[39m0.9355   \u001b[39m | \u001b[39m62.94    \u001b[39m | \u001b[39m0.2892   \u001b[39m | \u001b[39m57.93    \u001b[39m | \u001b[39m0.009781 \u001b[39m | \u001b[39m7.901    \u001b[39m | \u001b[39m9.092    \u001b[39m |\n",
      "| \u001b[35m7        \u001b[39m | \u001b[35m0.9839   \u001b[39m | \u001b[35m16.2     \u001b[39m | \u001b[35m0.4119   \u001b[39m | \u001b[35m70.99    \u001b[39m | \u001b[35m0.00253  \u001b[39m | \u001b[35m6.507    \u001b[39m | \u001b[35m13.35    \u001b[39m |\n",
      "| \u001b[35m8        \u001b[39m | \u001b[35m1.0      \u001b[39m | \u001b[35m16.13    \u001b[39m | \u001b[35m0.4231   \u001b[39m | \u001b[35m77.43    \u001b[39m | \u001b[35m0.006686 \u001b[39m | \u001b[35m14.58    \u001b[39m | \u001b[35m14.46    \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m0.9194   \u001b[39m | \u001b[39m16.31    \u001b[39m | \u001b[39m0.2935   \u001b[39m | \u001b[39m66.96    \u001b[39m | \u001b[39m0.009189 \u001b[39m | \u001b[39m14.85    \u001b[39m | \u001b[39m5.552    \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m0.9677   \u001b[39m | \u001b[39m57.97    \u001b[39m | \u001b[39m0.3839   \u001b[39m | \u001b[39m73.26    \u001b[39m | \u001b[39m0.001062 \u001b[39m | \u001b[39m5.4      \u001b[39m | \u001b[39m14.79    \u001b[39m |\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m0.9516   \u001b[39m | \u001b[39m63.9     \u001b[39m | \u001b[39m0.2996   \u001b[39m | \u001b[39m88.11    \u001b[39m | \u001b[39m0.003932 \u001b[39m | \u001b[39m11.31    \u001b[39m | \u001b[39m14.21    \u001b[39m |\n",
      "| \u001b[39m12       \u001b[39m | \u001b[39m0.9355   \u001b[39m | \u001b[39m60.14    \u001b[39m | \u001b[39m0.3754   \u001b[39m | \u001b[39m76.18    \u001b[39m | \u001b[39m0.007289 \u001b[39m | \u001b[39m14.83    \u001b[39m | \u001b[39m5.067    \u001b[39m |\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m0.9516   \u001b[39m | \u001b[39m16.16    \u001b[39m | \u001b[39m0.3636   \u001b[39m | \u001b[39m89.93    \u001b[39m | \u001b[39m0.003957 \u001b[39m | \u001b[39m14.46    \u001b[39m | \u001b[39m12.87    \u001b[39m |\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m0.871    \u001b[39m | \u001b[39m61.85    \u001b[39m | \u001b[39m0.1025   \u001b[39m | \u001b[39m99.53    \u001b[39m | \u001b[39m0.00543  \u001b[39m | \u001b[39m5.286    \u001b[39m | \u001b[39m5.811    \u001b[39m |\n",
      "| \u001b[39m15       \u001b[39m | \u001b[39m0.9677   \u001b[39m | \u001b[39m54.92    \u001b[39m | \u001b[39m0.4465   \u001b[39m | \u001b[39m65.72    \u001b[39m | \u001b[39m0.006372 \u001b[39m | \u001b[39m14.08    \u001b[39m | \u001b[39m14.75    \u001b[39m |\n",
      "=================================================================================================\n",
      "\n",
      "Best Parameters Found: {'batch_size': 16.131489880829374, 'dropout_rate': 0.4230664883427049, 'epochs': 77.43288212799175, 'learning_rate': 0.006685715890172139, 'x_dim': 14.580024011210522, 'y_dim': 14.458819807751643}\n",
      "\n",
      "Training Final Model...\n",
      "Epoch 1/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4180 - loss: 1.2867\n",
      "Epoch 2/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5362 - loss: 1.0096 \n",
      "Epoch 3/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6886 - loss: 0.7211 \n",
      "Epoch 4/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8870 - loss: 0.3813 \n",
      "Epoch 5/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9297 - loss: 0.1949 \n",
      "Epoch 6/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9670 - loss: 0.1114 \n",
      "Epoch 7/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9761 - loss: 0.0775 \n",
      "Epoch 8/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9686 - loss: 0.0781 \n",
      "Epoch 9/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9742 - loss: 0.0649 \n",
      "Epoch 10/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9717 - loss: 0.0582 \n",
      "Epoch 11/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9760 - loss: 0.0560 \n",
      "Epoch 12/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9816 - loss: 0.0553 \n",
      "Epoch 13/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9773 - loss: 0.0442 \n",
      "Epoch 14/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9724 - loss: 0.0787   \n",
      "Epoch 15/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9790 - loss: 0.0358 \n",
      "Epoch 16/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9812 - loss: 0.0509 \n",
      "Epoch 17/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9741 - loss: 0.0463 \n",
      "Epoch 18/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9795 - loss: 0.0359 \n",
      "Epoch 19/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9749 - loss: 0.0726 \n",
      "Epoch 20/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9829 - loss: 0.0434     \n",
      "Epoch 21/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9836 - loss: 0.0406     \n",
      "Epoch 22/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9812 - loss: 0.0353     \n",
      "Epoch 23/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9790 - loss: 0.0455     \n",
      "Epoch 24/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9749 - loss: 0.0498 \n",
      "Epoch 25/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9790 - loss: 0.0401     \n",
      "Epoch 26/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9812 - loss: 0.0432 \n",
      "Epoch 27/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9708 - loss: 0.0458 \n",
      "Epoch 28/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9765 - loss: 0.0380     \n",
      "Epoch 29/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9749 - loss: 0.0516     \n",
      "Epoch 30/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9796 - loss: 0.0359     \n",
      "Epoch 31/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9795 - loss: 0.0403 \n",
      "Epoch 32/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9835 - loss: 0.0345 \n",
      "Epoch 33/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9790 - loss: 0.0470     \n",
      "Epoch 34/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9812 - loss: 0.0393     \n",
      "Epoch 35/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9749 - loss: 0.0444     \n",
      "Epoch 36/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9795 - loss: 0.0331 \n",
      "Epoch 37/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9801 - loss: 0.0333     \n",
      "Epoch 38/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9785 - loss: 0.0325     \n",
      "Epoch 39/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9801 - loss: 0.0442     \n",
      "Epoch 40/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9724 - loss: 0.0440     \n",
      "Epoch 41/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9812 - loss: 0.0442     \n",
      "Epoch 42/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9790 - loss: 0.0412 \n",
      "Epoch 43/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9836 - loss: 0.0333      \n",
      "Epoch 44/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9836 - loss: 0.0327     \n",
      "Epoch 45/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9790 - loss: 0.0312 \n",
      "Epoch 46/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9788 - loss: 0.0335   \n",
      "Epoch 47/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9724 - loss: 0.0401     \n",
      "Epoch 48/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9812 - loss: 0.0425     \n",
      "Epoch 49/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9836 - loss: 0.0426     \n",
      "Epoch 50/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9790 - loss: 0.0275 \n",
      "Epoch 51/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9790 - loss: 0.0459     \n",
      "Epoch 52/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9871 - loss: 0.0324 \n",
      "Epoch 53/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9749 - loss: 0.0521 \n",
      "Epoch 54/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9804 - loss: 0.0325     \n",
      "Epoch 55/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9765 - loss: 0.0347     \n",
      "Epoch 56/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9795 - loss: 0.0343     \n",
      "Epoch 57/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9724 - loss: 0.0540     \n",
      "Epoch 58/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9805 - loss: 0.0321     \n",
      "Epoch 59/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9796 - loss: 0.0330     \n",
      "Epoch 60/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9813 - loss: 0.0357     \n",
      "Epoch 61/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9812 - loss: 0.0351     \n",
      "Epoch 62/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9790 - loss: 0.0314     \n",
      "Epoch 63/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9812 - loss: 0.0352     \n",
      "Epoch 64/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9836 - loss: 0.0301     \n",
      "Epoch 65/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9790 - loss: 0.0379     \n",
      "Epoch 66/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9836 - loss: 0.0327     \n",
      "Epoch 67/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9900 - loss: 0.0270     \n",
      "Epoch 68/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9771 - loss: 0.0466     \n",
      "Epoch 69/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9829 - loss: 0.0350   \n",
      "Epoch 70/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9790 - loss: 0.0243     \n",
      "Epoch 71/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9807 - loss: 0.0322     \n",
      "Epoch 72/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9757 - loss: 0.0400     \n",
      "Epoch 73/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9829 - loss: 0.0264 \n",
      "Epoch 74/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9812 - loss: 0.0447     \n",
      "Epoch 75/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9836 - loss: 0.0344 \n",
      "Epoch 76/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9812 - loss: 0.0287     \n",
      "Epoch 77/77\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9788 - loss: 0.0421     \n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\n",
      "===== Evaluation =====\n",
      "Accuracy: 0.8736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8182    0.5294    0.6429        17\n",
      "           1     0.8250    0.7333    0.7765        45\n",
      "           2     0.8839    0.9802    0.9296       101\n",
      "           3     1.0000    1.0000    1.0000        11\n",
      "\n",
      "    accuracy                         0.8736       174\n",
      "   macro avg     0.8818    0.8107    0.8372       174\n",
      "weighted avg     0.8696    0.8736    0.8664       174\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 9  5  3  0]\n",
      " [ 2 33 10  0]\n",
      " [ 0  2 99  0]\n",
      " [ 0  0  0 11]]\n",
      "\n",
      "===== Model Metrics =====\n",
      "Training Time: 7.03s\n",
      "Inference/sample: 0.000949s\n",
      "Testing Time: 0.17s\n",
      "Model Size: 0.08 MB\n",
      "Trainable Params: 14,820\n",
      "RAM Usage: 782.83 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import psutil\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from minisom import MiniSom\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from bayes_opt import BayesianOptimization\n",
    "from scipy.fftpack import fft\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "def load_and_preprocess_data(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df.columns = df.columns.str.replace('\"', '').str.strip()\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'], errors='coerce')\n",
    "    df.dropna(subset=['TIME'], inplace=True)\n",
    "    df.set_index('TIME', inplace=True)\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "    target_col = 'fenologia_h1'\n",
    "    df[target_col].interpolate(method='linear', inplace=True)\n",
    "    df[target_col] = df[target_col] - 1\n",
    "\n",
    "    for i in range(1, 4):\n",
    "        df[f'{target_col}_lag{i}'] = df[target_col].shift(i)\n",
    "    for window in [3, 6]:\n",
    "        df[f'{target_col}_roll_mean_{window}'] = df[target_col].rolling(window).mean()\n",
    "        df[f'{target_col}_roll_std_{window}'] = df[target_col].rolling(window).std()\n",
    "\n",
    "    # df['month'] = df.index.month\n",
    "    # df['weekofyear'] = df.index.isocalendar().week\n",
    "    # df['year'] = df.index.isocalendar().year\n",
    "    # df['sin_week'] = np.sin(2 * np.pi * df['weekofyear'] / 52)\n",
    "    # df['cos_week'] = np.cos(2 * np.pi * df['weekofyear'] / 52)\n",
    "    # df['EMA_3'] = df[target_col].ewm(span=3, adjust=False).mean()\n",
    "    # df['EMA_6'] = df[target_col].ewm(span=6, adjust=False).mean()\n",
    "    # df['correlation_target_month'] = df[target_col].rolling(6).corr(df['month'])\n",
    "    # df['correlation_target_week'] = df[target_col].rolling(6).corr(df['weekofyear'])\n",
    "\n",
    "    # fft_vals = fft(df[target_col].dropna().values)\n",
    "    # fft_real = np.real(fft_vals)[:len(df[target_col])]\n",
    "    # fft_imag = np.imag(fft_vals)[:len(df[target_col])]\n",
    "    # df['fft_real'] = np.pad(fft_real, (0, len(df) - len(fft_real)), constant_values=np.nan)\n",
    "    # df['fft_imag'] = np.pad(fft_imag, (0, len(df) - len(fft_imag)), constant_values=np.nan)\n",
    "\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "    return df, target_col\n",
    "\n",
    "def feature_scaling(df, target_col):\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    return X_scaled, y, scaler\n",
    "\n",
    "def transform_with_som(som, X, y_dim, x_dim):\n",
    "    winners = np.array([som.winner(x) for x in X])\n",
    "    indices = winners[:, 0] * y_dim + winners[:, 1]\n",
    "    return to_categorical(indices, num_classes=x_dim * y_dim)\n",
    "\n",
    "def train_and_evaluate_som_dnn(X_train, X_test, y_train, y_test, x_dim, y_dim, epochs, batch_size, dropout_rate, learning_rate):\n",
    "    som = MiniSom(x_dim, y_dim, X_train.shape[1], sigma=1.0, learning_rate=learning_rate)\n",
    "    som.random_weights_init(X_train)\n",
    "    som.train_random(X_train, 100)\n",
    "\n",
    "    X_train_enc = transform_with_som(som, X_train, y_dim, x_dim)\n",
    "    X_test_enc = transform_with_som(som, X_test, y_dim, x_dim)\n",
    "    y_train_cat = to_categorical(y_train)\n",
    "    y_test_cat = to_categorical(y_test)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_dim=X_train_enc.shape[1]))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(y_train_cat.shape[1], activation='softmax'))\n",
    "    model.compile(optimizer=Adam(learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    start_train = time.time()\n",
    "    model.fit(X_train_enc, y_train_cat, epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "    training_time = time.time() - start_train\n",
    "\n",
    "    start_infer = time.time()\n",
    "    y_pred = model.predict(X_test_enc).argmax(axis=1)\n",
    "    inference_time = (time.time() - start_infer) / len(X_test_enc)\n",
    "    testing_time = time.time() - start_infer\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(\"\\n===== Evaluation =====\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(\"Report:\\n\", classification_report(y_test, y_pred, digits=4))\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "    model.save(\"temp_model_som_dnn.h5\", include_optimizer=False)\n",
    "    model_size = os.path.getsize(\"temp_model_som_dnn.h5\") / (1024 ** 2)\n",
    "    os.remove(\"temp_model_som_dnn.h5\")\n",
    "\n",
    "    total_params = np.sum([np.prod(v.shape) for v in model.trainable_weights])\n",
    "    ram_usage = psutil.Process(os.getpid()).memory_info().rss / (1024 ** 2)\n",
    "\n",
    "    print(\"\\n===== Model Metrics =====\")\n",
    "    print(f\"Training Time: {training_time:.2f}s\")\n",
    "    print(f\"Inference/sample: {inference_time:.6f}s\")\n",
    "    print(f\"Testing Time: {testing_time:.2f}s\")\n",
    "    print(f\"Model Size: {model_size:.2f} MB\")\n",
    "    print(f\"Trainable Params: {total_params:,}\")\n",
    "    print(f\"RAM Usage: {ram_usage:.2f} MB\")\n",
    "\n",
    "def objective_function(epochs, batch_size, x_dim, y_dim, dropout_rate, learning_rate, X_train, y_train, X_val, y_val):\n",
    "    x_dim, y_dim = int(x_dim), int(y_dim)\n",
    "    batch_size, epochs = int(batch_size), int(epochs)\n",
    "    dropout_rate = float(dropout_rate)\n",
    "    learning_rate = float(learning_rate)\n",
    "\n",
    "    som = MiniSom(x_dim, y_dim, X_train.shape[1], sigma=1.0, learning_rate=learning_rate)\n",
    "    som.random_weights_init(X_train)\n",
    "    som.train_random(X_train, 100)\n",
    "\n",
    "    X_train_enc = transform_with_som(som, X_train, y_dim, x_dim)\n",
    "    X_val_enc = transform_with_som(som, X_val, y_dim, x_dim)\n",
    "    y_train_cat = to_categorical(y_train)\n",
    "    y_val_cat = to_categorical(y_val)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_dim=X_train_enc.shape[1]))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(y_train_cat.shape[1], activation='softmax'))\n",
    "    model.compile(optimizer=Adam(learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(X_train_enc, y_train_cat, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    _, acc = model.evaluate(X_val_enc, y_val_cat, verbose=0)\n",
    "    return acc\n",
    "\n",
    "def optimize_som_dnn_hyperparameters(X, y):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    pbounds = {\n",
    "        'epochs': (50, 100),\n",
    "        'batch_size': (16, 64),\n",
    "        'x_dim': (5, 15),\n",
    "        'y_dim': (5, 15),\n",
    "        'dropout_rate': (0.1, 0.5),\n",
    "        'learning_rate': (0.0005, 0.01)\n",
    "    }\n",
    "\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=lambda epochs, batch_size, x_dim, y_dim, dropout_rate, learning_rate:\n",
    "            objective_function(epochs, batch_size, x_dim, y_dim, dropout_rate, learning_rate,\n",
    "                               X_train, y_train, X_val, y_val),\n",
    "        pbounds=pbounds,\n",
    "        random_state=42,\n",
    "        verbose=2\n",
    "    )\n",
    "    optimizer.maximize(init_points=5, n_iter=10)\n",
    "    return optimizer.max['params']\n",
    "\n",
    "def main():\n",
    "    train_path = r\"E:\\Abroad period research\\Phenology datasets\\PHENOLOGY_H1\\Cordoba_train.csv\"\n",
    "    test_path = r\"E:\\Abroad period research\\Phenology datasets\\PHENOLOGY_H1\\Cordoba_test.csv\"\n",
    "\n",
    "    df_train, target_col = load_and_preprocess_data(train_path)\n",
    "    X_train, y_train, _ = feature_scaling(df_train, target_col)\n",
    "\n",
    "    df_test, _ = load_and_preprocess_data(test_path)\n",
    "    X_test, y_test, _ = feature_scaling(df_test, target_col)\n",
    "\n",
    "    print(\"\\nOptimizing Hyperparameters...\")\n",
    "    best_params = optimize_som_dnn_hyperparameters(X_train, y_train)\n",
    "    print(\"\\nBest Parameters Found:\", best_params)\n",
    "\n",
    "    print(\"\\nTraining Final Model...\")\n",
    "    train_and_evaluate_som_dnn(\n",
    "        X_train, X_test, y_train, y_test,\n",
    "        x_dim=int(best_params['x_dim']),\n",
    "        y_dim=int(best_params['y_dim']),\n",
    "        epochs=int(best_params['epochs']),\n",
    "        batch_size=int(best_params['batch_size']),\n",
    "        dropout_rate=float(best_params['dropout_rate']),\n",
    "        learning_rate=float(best_params['learning_rate'])\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc00450",
   "metadata": {},
   "source": [
    "Jaen station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b1a0326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimizing Hyperparameters...\n",
      "|   iter    |  target   | batch_... | dropou... |  epochs   | learni... |   x_dim   |   y_dim   |\n",
      "-------------------------------------------------------------------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.84     \u001b[39m | \u001b[39m33.98    \u001b[39m | \u001b[39m0.4803   \u001b[39m | \u001b[39m86.6     \u001b[39m | \u001b[39m0.006187 \u001b[39m | \u001b[39m6.56     \u001b[39m | \u001b[39m6.56     \u001b[39m |\n",
      "| \u001b[35m2        \u001b[39m | \u001b[35m0.92     \u001b[39m | \u001b[35m18.79    \u001b[39m | \u001b[35m0.4465   \u001b[39m | \u001b[35m80.06    \u001b[39m | \u001b[35m0.007227 \u001b[39m | \u001b[35m5.206    \u001b[39m | \u001b[35m14.7     \u001b[39m |\n",
      "| \u001b[35m3        \u001b[39m | \u001b[35m0.94     \u001b[39m | \u001b[35m55.96    \u001b[39m | \u001b[35m0.1849   \u001b[39m | \u001b[35m59.09    \u001b[39m | \u001b[35m0.002242 \u001b[39m | \u001b[35m8.042    \u001b[39m | \u001b[35m10.25    \u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m0.94     \u001b[39m | \u001b[39m36.73    \u001b[39m | \u001b[39m0.2165   \u001b[39m | \u001b[39m80.59    \u001b[39m | \u001b[39m0.001825 \u001b[39m | \u001b[39m7.921    \u001b[39m | \u001b[39m8.664    \u001b[39m |\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m0.88     \u001b[39m | \u001b[39m37.89    \u001b[39m | \u001b[39m0.4141   \u001b[39m | \u001b[39m59.98    \u001b[39m | \u001b[39m0.005385 \u001b[39m | \u001b[39m10.92    \u001b[39m | \u001b[39m5.465    \u001b[39m |\n",
      "| \u001b[39m6        \u001b[39m | \u001b[39m0.88     \u001b[39m | \u001b[39m52.17    \u001b[39m | \u001b[39m0.443    \u001b[39m | \u001b[39m79.51    \u001b[39m | \u001b[39m0.00398  \u001b[39m | \u001b[39m9.41     \u001b[39m | \u001b[39m12.86    \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m0.94     \u001b[39m | \u001b[39m21.96    \u001b[39m | \u001b[39m0.3851   \u001b[39m | \u001b[39m87.03    \u001b[39m | \u001b[39m0.001986 \u001b[39m | \u001b[39m11.34    \u001b[39m | \u001b[39m13.85    \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m0.92     \u001b[39m | \u001b[39m26.1     \u001b[39m | \u001b[39m0.497    \u001b[39m | \u001b[39m60.7     \u001b[39m | \u001b[39m0.00716  \u001b[39m | \u001b[39m9.956    \u001b[39m | \u001b[39m7.877    \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m0.86     \u001b[39m | \u001b[39m53.9     \u001b[39m | \u001b[39m0.3142   \u001b[39m | \u001b[39m94.74    \u001b[39m | \u001b[39m0.004845 \u001b[39m | \u001b[39m9.225    \u001b[39m | \u001b[39m11.54    \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m0.88     \u001b[39m | \u001b[39m37.02    \u001b[39m | \u001b[39m0.1646   \u001b[39m | \u001b[39m57.01    \u001b[39m | \u001b[39m0.0005144\u001b[39m | \u001b[39m14.98    \u001b[39m | \u001b[39m10.33    \u001b[39m |\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m0.88     \u001b[39m | \u001b[39m50.96    \u001b[39m | \u001b[39m0.2747   \u001b[39m | \u001b[39m71.09    \u001b[39m | \u001b[39m0.000789 \u001b[39m | \u001b[39m5.756    \u001b[39m | \u001b[39m11.66    \u001b[39m |\n",
      "| \u001b[39m12       \u001b[39m | \u001b[39m0.92     \u001b[39m | \u001b[39m36.1     \u001b[39m | \u001b[39m0.3902   \u001b[39m | \u001b[39m76.05    \u001b[39m | \u001b[39m0.007546 \u001b[39m | \u001b[39m5.782    \u001b[39m | \u001b[39m5.327    \u001b[39m |\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m0.88     \u001b[39m | \u001b[39m36.31    \u001b[39m | \u001b[39m0.3295   \u001b[39m | \u001b[39m78.77    \u001b[39m | \u001b[39m0.003848 \u001b[39m | \u001b[39m6.44     \u001b[39m | \u001b[39m7.711    \u001b[39m |\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m0.88     \u001b[39m | \u001b[39m50.66    \u001b[39m | \u001b[39m0.1115   \u001b[39m | \u001b[39m88.68    \u001b[39m | \u001b[39m0.006832 \u001b[39m | \u001b[39m11.9     \u001b[39m | \u001b[39m10.36    \u001b[39m |\n",
      "| \u001b[39m15       \u001b[39m | \u001b[39m0.94     \u001b[39m | \u001b[39m58.03    \u001b[39m | \u001b[39m0.2338   \u001b[39m | \u001b[39m66.8     \u001b[39m | \u001b[39m0.009438 \u001b[39m | \u001b[39m10.48    \u001b[39m | \u001b[39m7.241    \u001b[39m |\n",
      "=================================================================================================\n",
      "\n",
      "Best Parameters Found: {'batch_size': 55.95724675842024, 'dropout_rate': 0.18493564427131048, 'epochs': 59.09124836035503, 'learning_rate': 0.0022423428436076213, 'x_dim': 8.042422429595376, 'y_dim': 10.247564316322379}\n",
      "\n",
      "Training Final Model...\n",
      "Epoch 1/59\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.2444 - loss: 1.3907\n",
      "Epoch 2/59\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6092 - loss: 1.3006 \n",
      "Epoch 3/59\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5827 - loss: 1.2134 \n",
      "Epoch 4/59\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5827 - loss: 1.1288 \n",
      "Epoch 5/59\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5827 - loss: 1.0426  \n",
      "Epoch 6/59\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5827 - loss: 0.9606 \n",
      "Epoch 7/59\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5827 - loss: 0.8798 \n",
      "Epoch 8/59\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5827 - loss: 0.8186 \n",
      "Epoch 9/59\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5986 - loss: 0.7436 \n",
      "Epoch 10/59\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7124 - loss: 0.6737 \n",
      "Epoch 11/59\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7806 - loss: 0.5964 \n",
      "Epoch 12/59\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8696 - loss: 0.5136 \n",
      "Epoch 13/59\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8856 - loss: 0.4370 \n",
      "Epoch 14/59\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8958 - loss: 0.3705 \n",
      "Epoch 15/59\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9031 - loss: 0.3073 \n",
      "Epoch 16/59\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9195 - loss: 0.2629 \n",
      "Epoch 17/59\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9289 - loss: 0.2255 \n",
      "Epoch 18/59\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9547 - loss: 0.1887 \n",
      "Epoch 19/59\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9505 - loss: 0.1790 \n",
      "Epoch 20/59\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9689 - loss: 0.1513 \n",
      "Epoch 21/59\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9658 - loss: 0.1326 \n",
      "Epoch 22/59\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9550 - loss: 0.1192 \n",
      "Epoch 23/59\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9637 - loss: 0.1070 \n",
      "Epoch 24/59\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9680 - loss: 0.0989 \n",
      "Epoch 25/59\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9571 - loss: 0.0997 \n",
      "Epoch 26/59\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9716 - loss: 0.0823 \n",
      "Epoch 27/59\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9643 - loss: 0.0886  \n",
      "Epoch 28/59\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9737 - loss: 0.0804 \n",
      "Epoch 29/59\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9778 - loss: 0.0636 \n",
      "Epoch 30/59\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9655 - loss: 0.0752 \n",
      "Epoch 31/59\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9590 - loss: 0.0845 \n",
      "Epoch 32/59\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9697 - loss: 0.0722 \n",
      "Epoch 33/59\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9500 - loss: 0.0854 \n",
      "Epoch 34/59\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9522 - loss: 0.0814 \n",
      "Epoch 35/59\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9622 - loss: 0.0649 \n",
      "Epoch 36/59\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9538 - loss: 0.0845 \n",
      "Epoch 37/59\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9666 - loss: 0.0687 \n",
      "Epoch 38/59\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9607 - loss: 0.0760 \n",
      "Epoch 39/59\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9485 - loss: 0.0788 \n",
      "Epoch 40/59\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9741 - loss: 0.0601 \n",
      "Epoch 41/59\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9466 - loss: 0.0742 \n",
      "Epoch 42/59\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9522 - loss: 0.0729 \n",
      "Epoch 43/59\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9666 - loss: 0.0606 \n",
      "Epoch 44/59\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9599 - loss: 0.0741 \n",
      "Epoch 45/59\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9701 - loss: 0.0628 \n",
      "Epoch 46/59\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9551 - loss: 0.0832 \n",
      "Epoch 47/59\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9558 - loss: 0.0682 \n",
      "Epoch 48/59\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9692 - loss: 0.0638 \n",
      "Epoch 49/59\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9821 - loss: 0.0560 \n",
      "Epoch 50/59\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9535 - loss: 0.0716 \n",
      "Epoch 51/59\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9547 - loss: 0.0856 \n",
      "Epoch 52/59\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9830 - loss: 0.0529 \n",
      "Epoch 53/59\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9615 - loss: 0.0661 \n",
      "Epoch 54/59\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9727 - loss: 0.0582\n",
      "Epoch 55/59\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9707 - loss: 0.0560  \n",
      "Epoch 56/59\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9701 - loss: 0.0621 \n",
      "Epoch 57/59\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9772 - loss: 0.0530 \n",
      "Epoch 58/59\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9628 - loss: 0.0631 \n",
      "Epoch 59/59\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9757 - loss: 0.0588 \n",
      "WARNING:tensorflow:5 out of the last 15 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000022C0B1B9260> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 15 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000022C0B1B9260> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Evaluation =====\n",
      "Accuracy: 0.7772\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7143    0.3846    0.5000        13\n",
      "           1     0.7949    0.6200    0.6966        50\n",
      "           2     0.8629    0.8843    0.8735       121\n",
      "           3     0.0000    0.0000    0.0000         0\n",
      "\n",
      "    accuracy                         0.7772       184\n",
      "   macro avg     0.5930    0.4722    0.5175       184\n",
      "weighted avg     0.8339    0.7772    0.7990       184\n",
      "\n",
      "Confusion Matrix:\n",
      " [[  5   8   0   0]\n",
      " [  2  31  17   0]\n",
      " [  0   0 107  14]\n",
      " [  0   0   0   0]]\n",
      "\n",
      "===== Model Metrics =====\n",
      "Training Time: 5.44s\n",
      "Inference/sample: 0.001111s\n",
      "Testing Time: 0.20s\n",
      "Model Size: 0.05 MB\n",
      "Trainable Params: 7,396\n",
      "RAM Usage: 930.57 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import psutil\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from minisom import MiniSom\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from bayes_opt import BayesianOptimization\n",
    "from scipy.fftpack import fft\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "def load_and_preprocess_data(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df.columns = df.columns.str.replace('\"', '').str.strip()\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'], errors='coerce')\n",
    "    df.dropna(subset=['TIME'], inplace=True)\n",
    "    df.set_index('TIME', inplace=True)\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "    target_col = 'fenologia_h1'\n",
    "    df[target_col].interpolate(method='linear', inplace=True)\n",
    "    df[target_col] = df[target_col] - 1\n",
    "\n",
    "    for i in range(1, 4):\n",
    "        df[f'{target_col}_lag{i}'] = df[target_col].shift(i)\n",
    "    for window in [3, 6]:\n",
    "        df[f'{target_col}_roll_mean_{window}'] = df[target_col].rolling(window).mean()\n",
    "        df[f'{target_col}_roll_std_{window}'] = df[target_col].rolling(window).std()\n",
    "\n",
    "    df['month'] = df.index.month\n",
    "    df['weekofyear'] = df.index.isocalendar().week\n",
    "    df['year'] = df.index.isocalendar().year\n",
    "    df['sin_week'] = np.sin(2 * np.pi * df['weekofyear'] / 52)\n",
    "    df['cos_week'] = np.cos(2 * np.pi * df['weekofyear'] / 52)\n",
    "    df['EMA_3'] = df[target_col].ewm(span=3, adjust=False).mean()\n",
    "    df['EMA_6'] = df[target_col].ewm(span=6, adjust=False).mean()\n",
    "    df['correlation_target_month'] = df[target_col].rolling(6).corr(df['month'])\n",
    "    df['correlation_target_week'] = df[target_col].rolling(6).corr(df['weekofyear'])\n",
    "\n",
    "    # fft_vals = fft(df[target_col].dropna().values)\n",
    "    # fft_real = np.real(fft_vals)[:len(df[target_col])]\n",
    "    # fft_imag = np.imag(fft_vals)[:len(df[target_col])]\n",
    "    # df['fft_real'] = np.pad(fft_real, (0, len(df) - len(fft_real)), constant_values=np.nan)\n",
    "    # df['fft_imag'] = np.pad(fft_imag, (0, len(df) - len(fft_imag)), constant_values=np.nan)\n",
    "\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "    return df, target_col\n",
    "\n",
    "def feature_scaling(df, target_col):\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    return X_scaled, y, scaler\n",
    "\n",
    "def transform_with_som(som, X, y_dim, x_dim):\n",
    "    winners = np.array([som.winner(x) for x in X])\n",
    "    indices = winners[:, 0] * y_dim + winners[:, 1]\n",
    "    return to_categorical(indices, num_classes=x_dim * y_dim)\n",
    "\n",
    "def train_and_evaluate_som_dnn(X_train, X_test, y_train, y_test, x_dim, y_dim, epochs, batch_size, dropout_rate, learning_rate):\n",
    "    som = MiniSom(x_dim, y_dim, X_train.shape[1], sigma=1.0, learning_rate=learning_rate)\n",
    "    som.random_weights_init(X_train)\n",
    "    som.train_random(X_train, 100)\n",
    "\n",
    "    X_train_enc = transform_with_som(som, X_train, y_dim, x_dim)\n",
    "    X_test_enc = transform_with_som(som, X_test, y_dim, x_dim)\n",
    "    y_train_cat = to_categorical(y_train)\n",
    "    y_test_cat = to_categorical(y_test)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_dim=X_train_enc.shape[1]))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(y_train_cat.shape[1], activation='softmax'))\n",
    "    model.compile(optimizer=Adam(learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    start_train = time.time()\n",
    "    model.fit(X_train_enc, y_train_cat, epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "    training_time = time.time() - start_train\n",
    "\n",
    "    start_infer = time.time()\n",
    "    y_pred = model.predict(X_test_enc).argmax(axis=1)\n",
    "    inference_time = (time.time() - start_infer) / len(X_test_enc)\n",
    "    testing_time = time.time() - start_infer\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(\"\\n===== Evaluation =====\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(\"Report:\\n\", classification_report(y_test, y_pred, digits=4))\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "    model.save(\"temp_model_som_dnn.h5\", include_optimizer=False)\n",
    "    model_size = os.path.getsize(\"temp_model_som_dnn.h5\") / (1024 ** 2)\n",
    "    os.remove(\"temp_model_som_dnn.h5\")\n",
    "\n",
    "    total_params = np.sum([np.prod(v.shape) for v in model.trainable_weights])\n",
    "    ram_usage = psutil.Process(os.getpid()).memory_info().rss / (1024 ** 2)\n",
    "\n",
    "    print(\"\\n===== Model Metrics =====\")\n",
    "    print(f\"Training Time: {training_time:.2f}s\")\n",
    "    print(f\"Inference/sample: {inference_time:.6f}s\")\n",
    "    print(f\"Testing Time: {testing_time:.2f}s\")\n",
    "    print(f\"Model Size: {model_size:.2f} MB\")\n",
    "    print(f\"Trainable Params: {total_params:,}\")\n",
    "    print(f\"RAM Usage: {ram_usage:.2f} MB\")\n",
    "\n",
    "def objective_function(epochs, batch_size, x_dim, y_dim, dropout_rate, learning_rate, X_train, y_train, X_val, y_val):\n",
    "    x_dim, y_dim = int(x_dim), int(y_dim)\n",
    "    batch_size, epochs = int(batch_size), int(epochs)\n",
    "    dropout_rate = float(dropout_rate)\n",
    "    learning_rate = float(learning_rate)\n",
    "\n",
    "    som = MiniSom(x_dim, y_dim, X_train.shape[1], sigma=1.0, learning_rate=learning_rate)\n",
    "    som.random_weights_init(X_train)\n",
    "    som.train_random(X_train, 100)\n",
    "\n",
    "    X_train_enc = transform_with_som(som, X_train, y_dim, x_dim)\n",
    "    X_val_enc = transform_with_som(som, X_val, y_dim, x_dim)\n",
    "    y_train_cat = to_categorical(y_train)\n",
    "    y_val_cat = to_categorical(y_val)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_dim=X_train_enc.shape[1]))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(y_train_cat.shape[1], activation='softmax'))\n",
    "    model.compile(optimizer=Adam(learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(X_train_enc, y_train_cat, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    _, acc = model.evaluate(X_val_enc, y_val_cat, verbose=0)\n",
    "    return acc\n",
    "\n",
    "def optimize_som_dnn_hyperparameters(X, y):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    pbounds = {\n",
    "        'epochs': (50, 100),\n",
    "        'batch_size': (16, 64),\n",
    "        'x_dim': (5, 15),\n",
    "        'y_dim': (5, 15),\n",
    "        'dropout_rate': (0.1, 0.5),\n",
    "        'learning_rate': (0.0005, 0.01)\n",
    "    }\n",
    "\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=lambda epochs, batch_size, x_dim, y_dim, dropout_rate, learning_rate:\n",
    "            objective_function(epochs, batch_size, x_dim, y_dim, dropout_rate, learning_rate,\n",
    "                               X_train, y_train, X_val, y_val),\n",
    "        pbounds=pbounds,\n",
    "        random_state=42,\n",
    "        verbose=2\n",
    "    )\n",
    "    optimizer.maximize(init_points=5, n_iter=10)\n",
    "    return optimizer.max['params']\n",
    "\n",
    "def main():\n",
    "    train_path = r\"E:\\Abroad period research\\Phenology datasets\\PHENOLOGY_H1\\Jaen_train.csv\"\n",
    "    test_path = r\"E:\\Abroad period research\\Phenology datasets\\PHENOLOGY_H1\\Jaen_test.csv\"\n",
    "\n",
    "    df_train, target_col = load_and_preprocess_data(train_path)\n",
    "    X_train, y_train, _ = feature_scaling(df_train, target_col)\n",
    "\n",
    "    df_test, _ = load_and_preprocess_data(test_path)\n",
    "    X_test, y_test, _ = feature_scaling(df_test, target_col)\n",
    "\n",
    "    print(\"\\nOptimizing Hyperparameters...\")\n",
    "    best_params = optimize_som_dnn_hyperparameters(X_train, y_train)\n",
    "    print(\"\\nBest Parameters Found:\", best_params)\n",
    "\n",
    "    print(\"\\nTraining Final Model...\")\n",
    "    train_and_evaluate_som_dnn(\n",
    "        X_train, X_test, y_train, y_test,\n",
    "        x_dim=int(best_params['x_dim']),\n",
    "        y_dim=int(best_params['y_dim']),\n",
    "        epochs=int(best_params['epochs']),\n",
    "        batch_size=int(best_params['batch_size']),\n",
    "        dropout_rate=float(best_params['dropout_rate']),\n",
    "        learning_rate=float(best_params['learning_rate'])\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243a0ba6",
   "metadata": {},
   "source": [
    "Sevilla station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b8fa8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimizing Hyperparameters...\n",
      "|   iter    |  target   | batch_... | dropou... |  epochs   | learni... |   x_dim   |   y_dim   |\n",
      "-------------------------------------------------------------------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.9286   \u001b[39m | \u001b[39m33.98    \u001b[39m | \u001b[39m0.4803   \u001b[39m | \u001b[39m86.6     \u001b[39m | \u001b[39m0.006187 \u001b[39m | \u001b[39m6.56     \u001b[39m | \u001b[39m6.56     \u001b[39m |\n",
      "| \u001b[35m2        \u001b[39m | \u001b[35m0.9714   \u001b[39m | \u001b[35m18.79    \u001b[39m | \u001b[35m0.4465   \u001b[39m | \u001b[35m80.06    \u001b[39m | \u001b[35m0.007227 \u001b[39m | \u001b[35m5.206    \u001b[39m | \u001b[35m14.7     \u001b[39m |\n",
      "| \u001b[35m3        \u001b[39m | \u001b[35m0.9857   \u001b[39m | \u001b[35m55.96    \u001b[39m | \u001b[35m0.1849   \u001b[39m | \u001b[35m59.09    \u001b[39m | \u001b[35m0.002242 \u001b[39m | \u001b[35m8.042    \u001b[39m | \u001b[35m10.25    \u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m0.9714   \u001b[39m | \u001b[39m36.73    \u001b[39m | \u001b[39m0.2165   \u001b[39m | \u001b[39m80.59    \u001b[39m | \u001b[39m0.001825 \u001b[39m | \u001b[39m7.921    \u001b[39m | \u001b[39m8.664    \u001b[39m |\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m0.9714   \u001b[39m | \u001b[39m37.89    \u001b[39m | \u001b[39m0.4141   \u001b[39m | \u001b[39m59.98    \u001b[39m | \u001b[39m0.005385 \u001b[39m | \u001b[39m10.92    \u001b[39m | \u001b[39m5.465    \u001b[39m |\n",
      "| \u001b[39m6        \u001b[39m | \u001b[39m0.9714   \u001b[39m | \u001b[39m52.17    \u001b[39m | \u001b[39m0.443    \u001b[39m | \u001b[39m79.51    \u001b[39m | \u001b[39m0.00398  \u001b[39m | \u001b[39m9.41     \u001b[39m | \u001b[39m12.86    \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m0.9286   \u001b[39m | \u001b[39m21.96    \u001b[39m | \u001b[39m0.3851   \u001b[39m | \u001b[39m87.03    \u001b[39m | \u001b[39m0.001986 \u001b[39m | \u001b[39m11.34    \u001b[39m | \u001b[39m13.85    \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m0.9714   \u001b[39m | \u001b[39m26.1     \u001b[39m | \u001b[39m0.497    \u001b[39m | \u001b[39m60.7     \u001b[39m | \u001b[39m0.00716  \u001b[39m | \u001b[39m9.956    \u001b[39m | \u001b[39m7.877    \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m0.9286   \u001b[39m | \u001b[39m53.9     \u001b[39m | \u001b[39m0.3142   \u001b[39m | \u001b[39m94.74    \u001b[39m | \u001b[39m0.004845 \u001b[39m | \u001b[39m9.225    \u001b[39m | \u001b[39m11.54    \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m0.9857   \u001b[39m | \u001b[39m37.02    \u001b[39m | \u001b[39m0.1646   \u001b[39m | \u001b[39m57.01    \u001b[39m | \u001b[39m0.0005144\u001b[39m | \u001b[39m14.98    \u001b[39m | \u001b[39m10.33    \u001b[39m |\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m0.9857   \u001b[39m | \u001b[39m50.96    \u001b[39m | \u001b[39m0.2747   \u001b[39m | \u001b[39m71.09    \u001b[39m | \u001b[39m0.000789 \u001b[39m | \u001b[39m5.756    \u001b[39m | \u001b[39m11.66    \u001b[39m |\n",
      "| \u001b[39m12       \u001b[39m | \u001b[39m0.9      \u001b[39m | \u001b[39m36.1     \u001b[39m | \u001b[39m0.3902   \u001b[39m | \u001b[39m76.05    \u001b[39m | \u001b[39m0.007546 \u001b[39m | \u001b[39m5.782    \u001b[39m | \u001b[39m5.327    \u001b[39m |\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m0.9571   \u001b[39m | \u001b[39m51.4     \u001b[39m | \u001b[39m0.4166   \u001b[39m | \u001b[39m78.82    \u001b[39m | \u001b[39m0.00421  \u001b[39m | \u001b[39m10.72    \u001b[39m | \u001b[39m5.745    \u001b[39m |\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m0.9571   \u001b[39m | \u001b[39m50.66    \u001b[39m | \u001b[39m0.1115   \u001b[39m | \u001b[39m88.68    \u001b[39m | \u001b[39m0.006832 \u001b[39m | \u001b[39m11.9     \u001b[39m | \u001b[39m10.36    \u001b[39m |\n",
      "| \u001b[39m15       \u001b[39m | \u001b[39m0.9      \u001b[39m | \u001b[39m58.03    \u001b[39m | \u001b[39m0.2338   \u001b[39m | \u001b[39m66.8     \u001b[39m | \u001b[39m0.009438 \u001b[39m | \u001b[39m10.48    \u001b[39m | \u001b[39m7.241    \u001b[39m |\n",
      "=================================================================================================\n",
      "\n",
      "Best Parameters Found: {'batch_size': 55.95724675842024, 'dropout_rate': 0.18493564427131048, 'epochs': 59.09124836035503, 'learning_rate': 0.0022423428436076213, 'x_dim': 8.042422429595376, 'y_dim': 10.247564316322379}\n",
      "\n",
      "Training Final Model...\n",
      "Epoch 1/59\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.2556 - loss: 1.3632\n",
      "Epoch 2/59\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3999 - loss: 1.2774 \n",
      "Epoch 3/59\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6041 - loss: 1.2059 \n",
      "Epoch 4/59\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5948 - loss: 1.1241 \n",
      "Epoch 5/59\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5549 - loss: 1.0405 \n",
      "Epoch 6/59\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5970 - loss: 0.9425 \n",
      "Epoch 7/59\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6830 - loss: 0.8343 \n",
      "Epoch 8/59\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8188 - loss: 0.7080 \n",
      "Epoch 9/59\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8925 - loss: 0.5659 \n",
      "Epoch 10/59\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9020 - loss: 0.4298 \n",
      "Epoch 11/59\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9382 - loss: 0.3226 \n",
      "Epoch 12/59\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9362 - loss: 0.2457 \n",
      "Epoch 13/59\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9367 - loss: 0.1970 \n",
      "Epoch 14/59\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9501 - loss: 0.1649 \n",
      "Epoch 15/59\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9550 - loss: 0.1577 \n",
      "Epoch 16/59\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9601 - loss: 0.1119 \n",
      "Epoch 17/59\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9595 - loss: 0.1316 \n",
      "Epoch 18/59\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9573 - loss: 0.1277 \n",
      "Epoch 19/59\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9649 - loss: 0.1042 \n",
      "Epoch 20/59\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9665 - loss: 0.0956 \n",
      "Epoch 21/59\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9433 - loss: 0.1066 \n",
      "Epoch 22/59\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9636 - loss: 0.0933 \n",
      "Epoch 23/59\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9529 - loss: 0.0996 \n",
      "Epoch 24/59\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9682 - loss: 0.0845 \n",
      "Epoch 25/59\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9568 - loss: 0.0903 \n",
      "Epoch 26/59\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9507 - loss: 0.0867 \n",
      "Epoch 27/59\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9573 - loss: 0.0873 \n",
      "Epoch 28/59\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9609 - loss: 0.0898 \n",
      "Epoch 29/59\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9510 - loss: 0.0921\n",
      "Epoch 30/59\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9620 - loss: 0.0929 \n",
      "Epoch 31/59\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9544 - loss: 0.0949 \n",
      "Epoch 32/59\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9675 - loss: 0.0905 \n",
      "Epoch 33/59\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9676 - loss: 0.0776 \n",
      "Epoch 34/59\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9556 - loss: 0.0940 \n",
      "Epoch 35/59\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9501 - loss: 0.0938 \n",
      "Epoch 36/59\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9339 - loss: 0.1027 \n",
      "Epoch 37/59\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9601 - loss: 0.0928 \n",
      "Epoch 38/59\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9545 - loss: 0.0814 \n",
      "Epoch 39/59\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9386 - loss: 0.1089 \n",
      "Epoch 40/59\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9668 - loss: 0.0760 \n",
      "Epoch 41/59\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9615 - loss: 0.0883 \n",
      "Epoch 42/59\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9569 - loss: 0.0870 \n",
      "Epoch 43/59\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9537 - loss: 0.0930 \n",
      "Epoch 44/59\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9713 - loss: 0.0786 \n",
      "Epoch 45/59\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9631 - loss: 0.0832 \n",
      "Epoch 46/59\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9481 - loss: 0.0784 \n",
      "Epoch 47/59\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9545 - loss: 0.0863 \n",
      "Epoch 48/59\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9629 - loss: 0.0731 \n",
      "Epoch 49/59\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9556 - loss: 0.0862 \n",
      "Epoch 50/59\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9594 - loss: 0.0893 \n",
      "Epoch 51/59\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9539 - loss: 0.0858 \n",
      "Epoch 52/59\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9464 - loss: 0.0821  \n",
      "Epoch 53/59\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9486 - loss: 0.0827 \n",
      "Epoch 54/59\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9440 - loss: 0.0792 \n",
      "Epoch 55/59\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9692 - loss: 0.0786 \n",
      "Epoch 56/59\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9601 - loss: 0.0807  \n",
      "Epoch 57/59\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9650 - loss: 0.0748 \n",
      "Epoch 58/59\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9547 - loss: 0.0891 \n",
      "Epoch 59/59\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9563 - loss: 0.0807 \n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000022C15F4C0E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 13 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000022C15F4C0E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Evaluation =====\n",
      "Accuracy: 0.8072\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.1111    0.1000    0.1053        10\n",
      "           1     0.8714    0.8026    0.8356        76\n",
      "           2     0.9322    0.8594    0.8943       128\n",
      "           3     0.3077    0.8889    0.4571         9\n",
      "\n",
      "    accuracy                         0.8072       223\n",
      "   macro avg     0.5556    0.6627    0.5731       223\n",
      "weighted avg     0.8495    0.8072    0.8213       223\n",
      "\n",
      "Confusion Matrix:\n",
      " [[  1   9   0   0]\n",
      " [  8  61   7   0]\n",
      " [  0   0 110  18]\n",
      " [  0   0   1   8]]\n",
      "\n",
      "===== Model Metrics =====\n",
      "Training Time: 5.40s\n",
      "Inference/sample: 0.001052s\n",
      "Testing Time: 0.23s\n",
      "Model Size: 0.05 MB\n",
      "Trainable Params: 7,396\n",
      "RAM Usage: 1075.55 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import psutil\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from minisom import MiniSom\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from bayes_opt import BayesianOptimization\n",
    "from scipy.fftpack import fft\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "def load_and_preprocess_data(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df.columns = df.columns.str.replace('\"', '').str.strip()\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'], errors='coerce')\n",
    "    df.dropna(subset=['TIME'], inplace=True)\n",
    "    df.set_index('TIME', inplace=True)\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "    target_col = 'fenologia_h1'\n",
    "    df[target_col].interpolate(method='linear', inplace=True)\n",
    "    df[target_col] = df[target_col] - 1\n",
    "\n",
    "    for i in range(1, 4):\n",
    "        df[f'{target_col}_lag{i}'] = df[target_col].shift(i)\n",
    "    for window in [3, 6]:\n",
    "        df[f'{target_col}_roll_mean_{window}'] = df[target_col].rolling(window).mean()\n",
    "        df[f'{target_col}_roll_std_{window}'] = df[target_col].rolling(window).std()\n",
    "\n",
    "    df['month'] = df.index.month\n",
    "    df['weekofyear'] = df.index.isocalendar().week\n",
    "    df['year'] = df.index.isocalendar().year\n",
    "    df['sin_week'] = np.sin(2 * np.pi * df['weekofyear'] / 52)\n",
    "    df['cos_week'] = np.cos(2 * np.pi * df['weekofyear'] / 52)\n",
    "    df['EMA_3'] = df[target_col].ewm(span=3, adjust=False).mean()\n",
    "    df['EMA_6'] = df[target_col].ewm(span=6, adjust=False).mean()\n",
    "    df['correlation_target_month'] = df[target_col].rolling(6).corr(df['month'])\n",
    "    df['correlation_target_week'] = df[target_col].rolling(6).corr(df['weekofyear'])\n",
    "\n",
    "    # fft_vals = fft(df[target_col].dropna().values)\n",
    "    # fft_real = np.real(fft_vals)[:len(df[target_col])]\n",
    "    # fft_imag = np.imag(fft_vals)[:len(df[target_col])]\n",
    "    # df['fft_real'] = np.pad(fft_real, (0, len(df) - len(fft_real)), constant_values=np.nan)\n",
    "    # df['fft_imag'] = np.pad(fft_imag, (0, len(df) - len(fft_imag)), constant_values=np.nan)\n",
    "\n",
    "    df.fillna(df.median(), inplace=True)\n",
    "    return df, target_col\n",
    "\n",
    "def feature_scaling(df, target_col):\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    return X_scaled, y, scaler\n",
    "\n",
    "def transform_with_som(som, X, y_dim, x_dim):\n",
    "    winners = np.array([som.winner(x) for x in X])\n",
    "    indices = winners[:, 0] * y_dim + winners[:, 1]\n",
    "    return to_categorical(indices, num_classes=x_dim * y_dim)\n",
    "\n",
    "def train_and_evaluate_som_dnn(X_train, X_test, y_train, y_test, x_dim, y_dim, epochs, batch_size, dropout_rate, learning_rate):\n",
    "    som = MiniSom(x_dim, y_dim, X_train.shape[1], sigma=1.0, learning_rate=learning_rate)\n",
    "    som.random_weights_init(X_train)\n",
    "    som.train_random(X_train, 100)\n",
    "\n",
    "    X_train_enc = transform_with_som(som, X_train, y_dim, x_dim)\n",
    "    X_test_enc = transform_with_som(som, X_test, y_dim, x_dim)\n",
    "    y_train_cat = to_categorical(y_train)\n",
    "    y_test_cat = to_categorical(y_test)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_dim=X_train_enc.shape[1]))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(y_train_cat.shape[1], activation='softmax'))\n",
    "    model.compile(optimizer=Adam(learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    start_train = time.time()\n",
    "    model.fit(X_train_enc, y_train_cat, epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "    training_time = time.time() - start_train\n",
    "\n",
    "    start_infer = time.time()\n",
    "    y_pred = model.predict(X_test_enc).argmax(axis=1)\n",
    "    inference_time = (time.time() - start_infer) / len(X_test_enc)\n",
    "    testing_time = time.time() - start_infer\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(\"\\n===== Evaluation =====\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(\"Report:\\n\", classification_report(y_test, y_pred, digits=4))\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "    model.save(\"temp_model_som_dnn.h5\", include_optimizer=False)\n",
    "    model_size = os.path.getsize(\"temp_model_som_dnn.h5\") / (1024 ** 2)\n",
    "    os.remove(\"temp_model_som_dnn.h5\")\n",
    "\n",
    "    total_params = np.sum([np.prod(v.shape) for v in model.trainable_weights])\n",
    "    ram_usage = psutil.Process(os.getpid()).memory_info().rss / (1024 ** 2)\n",
    "\n",
    "    print(\"\\n===== Model Metrics =====\")\n",
    "    print(f\"Training Time: {training_time:.2f}s\")\n",
    "    print(f\"Inference/sample: {inference_time:.6f}s\")\n",
    "    print(f\"Testing Time: {testing_time:.2f}s\")\n",
    "    print(f\"Model Size: {model_size:.2f} MB\")\n",
    "    print(f\"Trainable Params: {total_params:,}\")\n",
    "    print(f\"RAM Usage: {ram_usage:.2f} MB\")\n",
    "\n",
    "def objective_function(epochs, batch_size, x_dim, y_dim, dropout_rate, learning_rate, X_train, y_train, X_val, y_val):\n",
    "    x_dim, y_dim = int(x_dim), int(y_dim)\n",
    "    batch_size, epochs = int(batch_size), int(epochs)\n",
    "    dropout_rate = float(dropout_rate)\n",
    "    learning_rate = float(learning_rate)\n",
    "\n",
    "    som = MiniSom(x_dim, y_dim, X_train.shape[1], sigma=1.0, learning_rate=learning_rate)\n",
    "    som.random_weights_init(X_train)\n",
    "    som.train_random(X_train, 100)\n",
    "\n",
    "    X_train_enc = transform_with_som(som, X_train, y_dim, x_dim)\n",
    "    X_val_enc = transform_with_som(som, X_val, y_dim, x_dim)\n",
    "    y_train_cat = to_categorical(y_train)\n",
    "    y_val_cat = to_categorical(y_val)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_dim=X_train_enc.shape[1]))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(y_train_cat.shape[1], activation='softmax'))\n",
    "    model.compile(optimizer=Adam(learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(X_train_enc, y_train_cat, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    _, acc = model.evaluate(X_val_enc, y_val_cat, verbose=0)\n",
    "    return acc\n",
    "\n",
    "def optimize_som_dnn_hyperparameters(X, y):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    pbounds = {\n",
    "        'epochs': (50, 100),\n",
    "        'batch_size': (16, 64),\n",
    "        'x_dim': (5, 15),\n",
    "        'y_dim': (5, 15),\n",
    "        'dropout_rate': (0.1, 0.5),\n",
    "        'learning_rate': (0.0005, 0.01)\n",
    "    }\n",
    "\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=lambda epochs, batch_size, x_dim, y_dim, dropout_rate, learning_rate:\n",
    "            objective_function(epochs, batch_size, x_dim, y_dim, dropout_rate, learning_rate,\n",
    "                               X_train, y_train, X_val, y_val),\n",
    "        pbounds=pbounds,\n",
    "        random_state=42,\n",
    "        verbose=2\n",
    "    )\n",
    "    optimizer.maximize(init_points=5, n_iter=10)\n",
    "    return optimizer.max['params']\n",
    "\n",
    "def main():\n",
    "    train_path = r\"E:\\Abroad period research\\Phenology datasets\\PHENOLOGY_H1\\sevilla_train.csv\"\n",
    "    test_path = r\"E:\\Abroad period research\\Phenology datasets\\PHENOLOGY_H1\\sevilla_test.csv\"\n",
    "\n",
    "    df_train, target_col = load_and_preprocess_data(train_path)\n",
    "    X_train, y_train, _ = feature_scaling(df_train, target_col)\n",
    "\n",
    "    df_test, _ = load_and_preprocess_data(test_path)\n",
    "    X_test, y_test, _ = feature_scaling(df_test, target_col)\n",
    "\n",
    "    print(\"\\nOptimizing Hyperparameters...\")\n",
    "    best_params = optimize_som_dnn_hyperparameters(X_train, y_train)\n",
    "    print(\"\\nBest Parameters Found:\", best_params)\n",
    "\n",
    "    print(\"\\nTraining Final Model...\")\n",
    "    train_and_evaluate_som_dnn(\n",
    "        X_train, X_test, y_train, y_test,\n",
    "        x_dim=int(best_params['x_dim']),\n",
    "        y_dim=int(best_params['y_dim']),\n",
    "        epochs=int(best_params['epochs']),\n",
    "        batch_size=int(best_params['batch_size']),\n",
    "        dropout_rate=float(best_params['dropout_rate']),\n",
    "        learning_rate=float(best_params['learning_rate'])\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
