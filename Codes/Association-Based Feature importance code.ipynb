{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b01157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "\n",
      "Running Rule-Based Feature Attribution...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import KBinsDiscretizer, MinMaxScaler\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from scipy.fftpack import fft\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "import gc\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================== 1. Load + Preprocess ==============================\n",
    "def load_and_preprocess_data(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df.columns = df.columns.str.replace('\"', '').str.strip()\n",
    "\n",
    "    if 'TIME' not in df.columns:\n",
    "        raise KeyError(\"The column 'TIME' does not exist in the dataset.\")\n",
    "\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'], errors='coerce')\n",
    "    df.dropna(subset=['TIME'], inplace=True)\n",
    "    df.set_index('TIME', inplace=True)\n",
    "\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "    target_col = 'fenologia_h1'\n",
    "    if target_col not in df.columns:\n",
    "        raise KeyError(f\"The target column '{target_col}' does not exist.\")\n",
    "    df[target_col].interpolate(method='linear', inplace=True)\n",
    "    df[target_col] = df[target_col] - 1\n",
    "\n",
    "    for i in range(1, 4):\n",
    "        df[f'{target_col}_lag{i}'] = df[target_col].shift(i)\n",
    "    for window in [3, 6]:\n",
    "        df[f'{target_col}_roll_mean_{window}'] = df[target_col].rolling(window).mean()\n",
    "        df[f'{target_col}_roll_std_{window}'] = df[target_col].rolling(window).std()\n",
    "\n",
    "    df['month'] = df.index.month\n",
    "    df['weekofyear'] = df.index.isocalendar().week.astype(int)\n",
    "    df['year'] = df.index.isocalendar().year.astype(int)\n",
    "    df['sin_week'] = np.sin(2 * np.pi * df['weekofyear'] / 52)\n",
    "    df['cos_week'] = np.cos(2 * np.pi * df['weekofyear'] / 52)\n",
    "    df['EMA_3'] = df[target_col].ewm(span=3, adjust=False).mean()\n",
    "    df['EMA_6'] = df[target_col].ewm(span=6, adjust=False).mean()\n",
    "    df['correlation_target_month'] = df[target_col].rolling(6).corr(df['month'])\n",
    "    df['correlation_target_week'] = df[target_col].rolling(6).corr(df['weekofyear'])\n",
    "\n",
    "    fft_vals = fft(df[target_col].dropna().values)\n",
    "    fft_real = np.real(fft_vals)[:len(df)]\n",
    "    fft_imag = np.imag(fft_vals)[:len(df)]\n",
    "    df['fft_real'] = np.pad(fft_real, (0, max(0, len(df) - len(fft_real))), constant_values=np.nan)\n",
    "    df['fft_imag'] = np.pad(fft_imag, (0, max(0, len(df) - len(fft_imag))), constant_values=np.nan)\n",
    "\n",
    "    df.fillna(df.median(numeric_only=True), inplace=True)\n",
    "    return df, target_col\n",
    "\n",
    "# ============================== 2. Scaling ==============================\n",
    "def feature_scaling(df, target_col):\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    return X_scaled, y.reset_index(drop=True), scaler\n",
    "\n",
    "# ============================== 3. Symbolic Rule-Based Feature Importance ==============================\n",
    "def rule_based_feature_importance(X, y, n_bins=3, min_support=0.1, min_conf=0.6, top_k=15, max_features=50):\n",
    "    if X.shape[1] > max_features:\n",
    "        X = X[:, :max_features]\n",
    "\n",
    "    df = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])])\n",
    "    y = pd.Series(y, name='target')\n",
    "\n",
    "    # Discretize features\n",
    "    kb = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='quantile')\n",
    "    df_binned = pd.DataFrame(kb.fit_transform(df), columns=df.columns).astype(int)\n",
    "\n",
    "    # Convert to symbolic format\n",
    "    for col in df_binned.columns:\n",
    "        df_binned[col] = df_binned[col].apply(lambda x: f\"{col}_bin{x}\")\n",
    "    df_binned['target'] = y.apply(lambda x: f\"target_class_{int(x)}\")\n",
    "\n",
    "    # One-hot encode using memory-efficient sparse format\n",
    "    ohe = pd.get_dummies(df_binned, sparse=True)\n",
    "\n",
    "    # Mine frequent patterns\n",
    "    frequent_itemsets = apriori(ohe, min_support=min_support, use_colnames=True, low_memory=True)\n",
    "    rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=min_conf)\n",
    "\n",
    "    # Filter rules leading to class outcomes\n",
    "    rules = rules[rules['consequents'].apply(lambda x: any('target_class' in i for i in x))]\n",
    "\n",
    "    # Calculate feature importance via cumulative lift\n",
    "    importance_scores = {}\n",
    "    for _, row in rules.iterrows():\n",
    "        for antecedent in row['antecedents']:\n",
    "            if 'feature_' in antecedent:\n",
    "                importance_scores[antecedent] = importance_scores.get(antecedent, 0) + row['lift']\n",
    "\n",
    "    importance_df = pd.DataFrame(list(importance_scores.items()), columns=['Feature_Bin', 'Importance'])\n",
    "    importance_df.sort_values('Importance', ascending=False, inplace=True)\n",
    "\n",
    "    # Plot symbolic feature influence\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=importance_df.head(top_k), y='Feature_Bin', x='Importance', palette='magma')\n",
    "    plt.title('Symbolic Rule-Based Feature Importance')\n",
    "    plt.xlabel('Cumulative Lift Score')\n",
    "    plt.ylabel('Feature Bin')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    gc.collect()\n",
    "    return importance_df\n",
    "\n",
    "# ============================== 4. Main ==============================\n",
    "def main():\n",
    "    train_path = r\"E:\\Abroad period research\\Phenology datasets\\PHENOLOGY_H1\\Cordoba_train.csv\"\n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    df_train, target_col = load_and_preprocess_data(train_path)\n",
    "    X_train, y_train, _ = feature_scaling(df_train, target_col)\n",
    "\n",
    "    print(\"\\nRunning Rule-Based Feature Attribution...\")\n",
    "    results = rule_based_feature_importance(X_train, y_train)\n",
    "\n",
    "    print(\"\\nTop symbolic feature patterns:\")\n",
    "    print(results.head(10))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030eb18e",
   "metadata": {},
   "source": [
    "Preprocessing and feature engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e161a3a1",
   "metadata": {},
   "source": [
    "Temporal Convolutional Network (TCN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223f384f",
   "metadata": {},
   "source": [
    "Novel Explainability Using Deep SHAP for Temporal Models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
